Pretraining policy
Training dynamics:
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0657276 |
| loss/dynamics_train_loss   | -18.8     |
| timestep                   | 1         |
---------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.05315913 |
| loss/dynamics_train_loss   | -28.9      |
| timestep                   | 2          |
----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0479027 |
| loss/dynamics_train_loss   | -31.7     |
| timestep                   | 3         |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.044521593 |
| loss/dynamics_train_loss   | -33.5       |
| timestep                   | 4           |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.042336825 |
| loss/dynamics_train_loss   | -34.8       |
| timestep                   | 5           |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0401439 |
| loss/dynamics_train_loss   | -35.9     |
| timestep                   | 6         |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.038307812 |
| loss/dynamics_train_loss   | -36.7       |
| timestep                   | 7           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03688193 |
| loss/dynamics_train_loss   | -37.4      |
| timestep                   | 8          |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.035746176 |
| loss/dynamics_train_loss   | -38         |
| timestep                   | 9           |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03470379 |
| loss/dynamics_train_loss   | -38.6      |
| timestep                   | 10         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.033646565 |
| loss/dynamics_train_loss   | -39.1       |
| timestep                   | 11          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.033058133 |
| loss/dynamics_train_loss   | -39.5       |
| timestep                   | 12          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03227833 |
| loss/dynamics_train_loss   | -39.9      |
| timestep                   | 13         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.031603314 |
| loss/dynamics_train_loss   | -40.2       |
| timestep                   | 14          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.030971631 |
| loss/dynamics_train_loss   | -40.5       |
| timestep                   | 15          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.03058141 |
| loss/dynamics_train_loss   | -40.8      |
| timestep                   | 16         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.029937586 |
| loss/dynamics_train_loss   | -41.1       |
| timestep                   | 17          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.029453885 |
| loss/dynamics_train_loss   | -41.4       |
| timestep                   | 18          |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0289508 |
| loss/dynamics_train_loss   | -41.6     |
| timestep                   | 19        |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028436935 |
| loss/dynamics_train_loss   | -41.8       |
| timestep                   | 20          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.028158993 |
| loss/dynamics_train_loss   | -42         |
| timestep                   | 21          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02763896 |
| loss/dynamics_train_loss   | -42.2      |
| timestep                   | 22         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.027410042 |
| loss/dynamics_train_loss   | -42.4       |
| timestep                   | 23          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026976833 |
| loss/dynamics_train_loss   | -42.6       |
| timestep                   | 24          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026675895 |
| loss/dynamics_train_loss   | -42.8       |
| timestep                   | 25          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02634655 |
| loss/dynamics_train_loss   | -43        |
| timestep                   | 26         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.026136074 |
| loss/dynamics_train_loss   | -43.2       |
| timestep                   | 27          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025846997 |
| loss/dynamics_train_loss   | -43.3       |
| timestep                   | 28          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02556644 |
| loss/dynamics_train_loss   | -43.5      |
| timestep                   | 29         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025120389 |
| loss/dynamics_train_loss   | -43.6       |
| timestep                   | 30          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.025188813 |
| loss/dynamics_train_loss   | -43.8       |
| timestep                   | 31          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024669137 |
| loss/dynamics_train_loss   | -43.9       |
| timestep                   | 32          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02447257 |
| loss/dynamics_train_loss   | -44        |
| timestep                   | 33         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.024290813 |
| loss/dynamics_train_loss   | -44.2       |
| timestep                   | 34          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023979984 |
| loss/dynamics_train_loss   | -44.3       |
| timestep                   | 35          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023690145 |
| loss/dynamics_train_loss   | -44.4       |
| timestep                   | 36          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023803765 |
| loss/dynamics_train_loss   | -44.5       |
| timestep                   | 37          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.023303673 |
| loss/dynamics_train_loss   | -44.7       |
| timestep                   | 38          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022823878 |
| loss/dynamics_train_loss   | -44.8       |
| timestep                   | 39          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022749314 |
| loss/dynamics_train_loss   | -44.9       |
| timestep                   | 40          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022803372 |
| loss/dynamics_train_loss   | -45         |
| timestep                   | 41          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022336423 |
| loss/dynamics_train_loss   | -45.1       |
| timestep                   | 42          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022190655 |
| loss/dynamics_train_loss   | -45.2       |
| timestep                   | 43          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021948535 |
| loss/dynamics_train_loss   | -45.3       |
| timestep                   | 44          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.022050563 |
| loss/dynamics_train_loss   | -45.4       |
| timestep                   | 45          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021945933 |
| loss/dynamics_train_loss   | -45.5       |
| timestep                   | 46          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02145356 |
| loss/dynamics_train_loss   | -45.6      |
| timestep                   | 47         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021351349 |
| loss/dynamics_train_loss   | -45.7       |
| timestep                   | 48          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021330204 |
| loss/dynamics_train_loss   | -45.8       |
| timestep                   | 49          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021127503 |
| loss/dynamics_train_loss   | -45.9       |
| timestep                   | 50          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.021017333 |
| loss/dynamics_train_loss   | -46         |
| timestep                   | 51          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02091069 |
| loss/dynamics_train_loss   | -46        |
| timestep                   | 52         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020801486 |
| loss/dynamics_train_loss   | -46.1       |
| timestep                   | 53          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.02070362 |
| loss/dynamics_train_loss   | -46.2      |
| timestep                   | 54         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020405889 |
| loss/dynamics_train_loss   | -46.3       |
| timestep                   | 55          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020216677 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 56          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020370781 |
| loss/dynamics_train_loss   | -46.4       |
| timestep                   | 57          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019918974 |
| loss/dynamics_train_loss   | -46.5       |
| timestep                   | 58          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.020050377 |
| loss/dynamics_train_loss   | -46.6       |
| timestep                   | 59          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019825092 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 60          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019841544 |
| loss/dynamics_train_loss   | -46.7       |
| timestep                   | 61          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01981433 |
| loss/dynamics_train_loss   | -46.8      |
| timestep                   | 62         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019664938 |
| loss/dynamics_train_loss   | -46.9       |
| timestep                   | 63          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019466724 |
| loss/dynamics_train_loss   | -46.9       |
| timestep                   | 64          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.019401593 |
| loss/dynamics_train_loss   | -47         |
| timestep                   | 65          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018913934 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 66          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018969757 |
| loss/dynamics_train_loss   | -47.1       |
| timestep                   | 67          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018983286 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 68          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018824331 |
| loss/dynamics_train_loss   | -47.2       |
| timestep                   | 69          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018793715 |
| loss/dynamics_train_loss   | -47.3       |
| timestep                   | 70          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018711183 |
| loss/dynamics_train_loss   | -47.3       |
| timestep                   | 71          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018616656 |
| loss/dynamics_train_loss   | -47.4       |
| timestep                   | 72          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01835129 |
| loss/dynamics_train_loss   | -47.5      |
| timestep                   | 73         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018314281 |
| loss/dynamics_train_loss   | -47.5       |
| timestep                   | 74          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017970337 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 75          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018247938 |
| loss/dynamics_train_loss   | -47.6       |
| timestep                   | 76          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.018077504 |
| loss/dynamics_train_loss   | -47.7       |
| timestep                   | 77          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017853249 |
| loss/dynamics_train_loss   | -47.8       |
| timestep                   | 78          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017824514 |
| loss/dynamics_train_loss   | -47.8       |
| timestep                   | 79          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01777654 |
| loss/dynamics_train_loss   | -47.9      |
| timestep                   | 80         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017754238 |
| loss/dynamics_train_loss   | -47.9       |
| timestep                   | 81          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017580092 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 82          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017465103 |
| loss/dynamics_train_loss   | -48         |
| timestep                   | 83          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017427087 |
| loss/dynamics_train_loss   | -48.1       |
| timestep                   | 84          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017283501 |
| loss/dynamics_train_loss   | -48.1       |
| timestep                   | 85          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017215634 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 86          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017034262 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 87          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.017187426 |
| loss/dynamics_train_loss   | -48.2       |
| timestep                   | 88          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016913926 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 89          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016866673 |
| loss/dynamics_train_loss   | -48.3       |
| timestep                   | 90          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016592946 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 91          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016678024 |
| loss/dynamics_train_loss   | -48.4       |
| timestep                   | 92          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016551455 |
| loss/dynamics_train_loss   | -48.5       |
| timestep                   | 93          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01641818 |
| loss/dynamics_train_loss   | -48.5      |
| timestep                   | 94         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016325116 |
| loss/dynamics_train_loss   | -48.6       |
| timestep                   | 95          |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01648733 |
| loss/dynamics_train_loss   | -48.6      |
| timestep                   | 96         |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016046781 |
| loss/dynamics_train_loss   | -48.6       |
| timestep                   | 97          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016212884 |
| loss/dynamics_train_loss   | -48.7       |
| timestep                   | 98          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015970286 |
| loss/dynamics_train_loss   | -48.7       |
| timestep                   | 99          |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016090792 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 100         |
-----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015894 |
| loss/dynamics_train_loss   | -48.8    |
| timestep                   | 101      |
--------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.016006056 |
| loss/dynamics_train_loss   | -48.8       |
| timestep                   | 102         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01595867 |
| loss/dynamics_train_loss   | -48.9      |
| timestep                   | 103        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015558894 |
| loss/dynamics_train_loss   | -48.9       |
| timestep                   | 104         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01577151 |
| loss/dynamics_train_loss   | -49        |
| timestep                   | 105        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015619087 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 106         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015915517 |
| loss/dynamics_train_loss   | -49         |
| timestep                   | 107         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0158356 |
| loss/dynamics_train_loss   | -49.1     |
| timestep                   | 108       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015619054 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 109         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015734877 |
| loss/dynamics_train_loss   | -49.1       |
| timestep                   | 110         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015790274 |
| loss/dynamics_train_loss   | -49.2       |
| timestep                   | 111         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015464464 |
| loss/dynamics_train_loss   | -49.2       |
| timestep                   | 112         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015356926 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 113         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015368666 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 114         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015485411 |
| loss/dynamics_train_loss   | -49.3       |
| timestep                   | 115         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015296144 |
| loss/dynamics_train_loss   | -49.4       |
| timestep                   | 116         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01521408 |
| loss/dynamics_train_loss   | -49.4      |
| timestep                   | 117        |
----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0151963625 |
| loss/dynamics_train_loss   | -49.4        |
| timestep                   | 118          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015335563 |
| loss/dynamics_train_loss   | -49.4       |
| timestep                   | 119         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015150448 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 120         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015008514 |
| loss/dynamics_train_loss   | -49.5       |
| timestep                   | 121         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01515373 |
| loss/dynamics_train_loss   | -49.6      |
| timestep                   | 122        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015068467 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 123         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015048777 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 124         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014980286 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 125         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.015031433 |
| loss/dynamics_train_loss   | -49.6       |
| timestep                   | 126         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014939083 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 127         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014813617 |
| loss/dynamics_train_loss   | -49.7       |
| timestep                   | 128         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014840555 |
| loss/dynamics_train_loss   | -49.8       |
| timestep                   | 129         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014855896 |
| loss/dynamics_train_loss   | -49.8       |
| timestep                   | 130         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0147951115 |
| loss/dynamics_train_loss   | -49.8        |
| timestep                   | 131          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014804649 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 132         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014868608 |
| loss/dynamics_train_loss   | -49.9       |
| timestep                   | 133         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0147225605 |
| loss/dynamics_train_loss   | -49.9        |
| timestep                   | 134          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014748143 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 135         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014533733 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 136         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014575225 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 137         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014525885 |
| loss/dynamics_train_loss   | -50         |
| timestep                   | 138         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014513803 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 139         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014576073 |
| loss/dynamics_train_loss   | -50.1       |
| timestep                   | 140         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01453274 |
| loss/dynamics_train_loss   | -50.1      |
| timestep                   | 141        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014455333 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 142         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014468282 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 143         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014561978 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 144         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014404252 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 145         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014311388 |
| loss/dynamics_train_loss   | -50.2       |
| timestep                   | 146         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014514443 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 147         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014179985 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 148         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014333919 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 149         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014245364 |
| loss/dynamics_train_loss   | -50.3       |
| timestep                   | 150         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014206578 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 151         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01425239 |
| loss/dynamics_train_loss   | -50.4      |
| timestep                   | 152        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014033755 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 153         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014244524 |
| loss/dynamics_train_loss   | -50.4       |
| timestep                   | 154         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014050638 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 155         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014006972 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 156         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014013857 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 157         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014282932 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 158         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013929822 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 159         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.014165446 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 160         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013862245 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 161         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013947807 |
| loss/dynamics_train_loss   | -50.5       |
| timestep                   | 162         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013712816 |
| loss/dynamics_train_loss   | -50.6       |
| timestep                   | 163         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013944109 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 164         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013815904 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 165         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013701631 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 166         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013835885 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 167         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013695364 |
| loss/dynamics_train_loss   | -50.7       |
| timestep                   | 168         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013806662 |
| loss/dynamics_train_loss   | -50.8       |
| timestep                   | 169         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01367837 |
| loss/dynamics_train_loss   | -50.8      |
| timestep                   | 170        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01387273 |
| loss/dynamics_train_loss   | -50.8      |
| timestep                   | 171        |
----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01374511 |
| loss/dynamics_train_loss   | -50.8      |
| timestep                   | 172        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013589963 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 173         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013775101 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 174         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013375995 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 175         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0135422405 |
| loss/dynamics_train_loss   | -50.9        |
| timestep                   | 176          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013510066 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 177         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013207205 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 178         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013483184 |
| loss/dynamics_train_loss   | -50.9       |
| timestep                   | 179         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013437699 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 180         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013549442 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 181         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013401872 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 182         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0135633 |
| loss/dynamics_train_loss   | -51       |
| timestep                   | 183       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013545419 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 184         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013292143 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 185         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013323395 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 186         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013313127 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 187         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013161982 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 188         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013164863 |
| loss/dynamics_train_loss   | -51         |
| timestep                   | 189         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012991101 |
| loss/dynamics_train_loss   | -51.1       |
| timestep                   | 190         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013226071 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 191         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01303075 |
| loss/dynamics_train_loss   | -51.2      |
| timestep                   | 192        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012995047 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 193         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.013019006 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 194         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012795076 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 195         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012884101 |
| loss/dynamics_train_loss   | -51.2       |
| timestep                   | 196         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0127191 |
| loss/dynamics_train_loss   | -51.3     |
| timestep                   | 197       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012964842 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 198         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012913628 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 199         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012885285 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 200         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012884544 |
| loss/dynamics_train_loss   | -51.3       |
| timestep                   | 201         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012785914 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 202         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012708711 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 203         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0127248615 |
| loss/dynamics_train_loss   | -51.4        |
| timestep                   | 204          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012750508 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 205         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012464787 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 206         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012852369 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 207         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012579316 |
| loss/dynamics_train_loss   | -51.4       |
| timestep                   | 208         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012622839 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 209         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012371825 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 210         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012923638 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 211         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012571758 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 212         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012390034 |
| loss/dynamics_train_loss   | -51.5       |
| timestep                   | 213         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012450502 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 214         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012386933 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 215         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012440648 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 216         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0123163415 |
| loss/dynamics_train_loss   | -51.6        |
| timestep                   | 217          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012249157 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 218         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012232564 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 219         |
-----------------------------------------------------------------------------
--------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012379 |
| loss/dynamics_train_loss   | -51.7    |
| timestep                   | 220      |
--------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01231971 |
| loss/dynamics_train_loss   | -51.6      |
| timestep                   | 221        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012213233 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 222         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012390216 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 223         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01242733 |
| loss/dynamics_train_loss   | -51.7      |
| timestep                   | 224        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012285282 |
| loss/dynamics_train_loss   | -51.6       |
| timestep                   | 225         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012416028 |
| loss/dynamics_train_loss   | -51.7       |
| timestep                   | 226         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012225726 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 227         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012145608 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 228         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012126007 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 229         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012080625 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 230         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012262237 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 231         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012119452 |
| loss/dynamics_train_loss   | -51.8       |
| timestep                   | 232         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01214028 |
| loss/dynamics_train_loss   | -51.9      |
| timestep                   | 233        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012093299 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 234         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012044081 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 235         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011887522 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 236         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012058359 |
| loss/dynamics_train_loss   | -51.9       |
| timestep                   | 237         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01187619 |
| loss/dynamics_train_loss   | -51.9      |
| timestep                   | 238        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011995874 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 239         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012001665 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 240         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012002496 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 241         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012051703 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 242         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011746803 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 243         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011891209 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 244         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0118265515 |
| loss/dynamics_train_loss   | -52          |
| timestep                   | 245          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011958764 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 246         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.012037089 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 247         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011969501 |
| loss/dynamics_train_loss   | -52         |
| timestep                   | 248         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011645056 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 249         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011921139 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 250         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011947724 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 251         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011580701 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 252         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01186884 |
| loss/dynamics_train_loss   | -52.1      |
| timestep                   | 253        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011884771 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 254         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011801947 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 255         |
-----------------------------------------------------------------------------
---------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0117338 |
| loss/dynamics_train_loss   | -52.2     |
| timestep                   | 256       |
---------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011618787 |
| loss/dynamics_train_loss   | -52.1       |
| timestep                   | 257         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011774861 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 258         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011677457 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 259         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011561405 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 260         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011415133 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 261         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011556989 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 262         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011789242 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 263         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011643065 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 264         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011588598 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 265         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011617618 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 266         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0115697635 |
| loss/dynamics_train_loss   | -52.3        |
| timestep                   | 267          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011649325 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 268         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011587622 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 269         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011428901 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 270         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01144499 |
| loss/dynamics_train_loss   | -52.4      |
| timestep                   | 271        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011511288 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 272         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011599529 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 273         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011527343 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 274         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011641595 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 275         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011529575 |
| loss/dynamics_train_loss   | -52.3       |
| timestep                   | 276         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011486131 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 277         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01144599 |
| loss/dynamics_train_loss   | -52.4      |
| timestep                   | 278        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011538401 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 279         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011437471 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 280         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011381456 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 281         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011267434 |
| loss/dynamics_train_loss   | -52.4       |
| timestep                   | 282         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011507747 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 283         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011208504 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 284         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01120094 |
| loss/dynamics_train_loss   | -52.5      |
| timestep                   | 285        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011191968 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 286         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011292285 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 287         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011368717 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 288         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0115113165 |
| loss/dynamics_train_loss   | -52.6        |
| timestep                   | 289          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011314565 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 290         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011244323 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 291         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011143556 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 292         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011301143 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 293         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0112008955 |
| loss/dynamics_train_loss   | -52.6        |
| timestep                   | 294          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01115723 |
| loss/dynamics_train_loss   | -52.6      |
| timestep                   | 295        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011300409 |
| loss/dynamics_train_loss   | -52.5       |
| timestep                   | 296         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011111213 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 297         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011122406 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 298         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011222281 |
| loss/dynamics_train_loss   | -52.6       |
| timestep                   | 299         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011513379 |
| loss/dynamics_train_loss   | -52.2       |
| timestep                   | 300         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01109446 |
| loss/dynamics_train_loss   | -52.6      |
| timestep                   | 301        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011224622 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 302         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011197532 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 303         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010963554 |
| loss/dynamics_train_loss   | -52.7       |
| timestep                   | 304         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011076635 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 305         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011061088 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 306         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011031313 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 307         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01092435 |
| loss/dynamics_train_loss   | -52.8      |
| timestep                   | 308        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011128025 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 309         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011139448 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 310         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.011130079 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 311         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010942182 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 312         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010921429 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 313         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010751675 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 314         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010810846 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 315         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0108398525 |
| loss/dynamics_train_loss   | -52.8        |
| timestep                   | 316          |
------------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0109249335 |
| loss/dynamics_train_loss   | -52.9        |
| timestep                   | 317          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010896247 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 318         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010743078 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 319         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0107901115 |
| loss/dynamics_train_loss   | -52.9        |
| timestep                   | 320          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010608576 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 321         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0107701775 |
| loss/dynamics_train_loss   | -53          |
| timestep                   | 322          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010888974 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 323         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010771664 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 324         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010911898 |
| loss/dynamics_train_loss   | -52.9       |
| timestep                   | 325         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010673882 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 326         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010748011 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 327         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010583299 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 328         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01066684 |
| loss/dynamics_train_loss   | -53        |
| timestep                   | 329        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010719599 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 330         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010450325 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 331         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010732237 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 332         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010676815 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 333         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010691615 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 334         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010756309 |
| loss/dynamics_train_loss   | -53         |
| timestep                   | 335         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010814364 |
| loss/dynamics_train_loss   | -52.8       |
| timestep                   | 336         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010608522 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 337         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010462141 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 338         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010579506 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 339         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010448177 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 340         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010570386 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 341         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010477496 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 342         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010582557 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 343         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010489245 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 344         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010357761 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 345         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010572475 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 346         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010596395 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 347         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010449131 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 348         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010469211 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 349         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0104753785 |
| loss/dynamics_train_loss   | -53.2        |
| timestep                   | 350          |
------------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01053325 |
| loss/dynamics_train_loss   | -53.2      |
| timestep                   | 351        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010632899 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 352         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01048824 |
| loss/dynamics_train_loss   | -53.2      |
| timestep                   | 353        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010385742 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 354         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010473351 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 355         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010298284 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 356         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010257561 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 357         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010506081 |
| loss/dynamics_train_loss   | -53.1       |
| timestep                   | 358         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010315903 |
| loss/dynamics_train_loss   | -53.2       |
| timestep                   | 359         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010437181 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 360         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010165789 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 361         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010557336 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 362         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010277854 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 363         |
-----------------------------------------------------------------------------
------------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.0103318235 |
| loss/dynamics_train_loss   | -53.4        |
| timestep                   | 364          |
------------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010518705 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 365         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01034712 |
| loss/dynamics_train_loss   | -53.4      |
| timestep                   | 366        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010306304 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 367         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010408352 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 368         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010319006 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 369         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010227874 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 370         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010329376 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 371         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010404095 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 372         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010296738 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 373         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010245904 |
| loss/dynamics_train_loss   | -53.3       |
| timestep                   | 374         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010253783 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 375         |
-----------------------------------------------------------------------------
----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.01042932 |
| loss/dynamics_train_loss   | -53.4      |
| timestep                   | 376        |
----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010215839 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 377         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010255367 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 378         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010039582 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 379         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010196323 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 380         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010031009 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 381         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010121469 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 382         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010474793 |
| loss/dynamics_train_loss   | -53.4       |
| timestep                   | 383         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010455012 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 384         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010205159 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 385         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010357454 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 386         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010236886 |
| loss/dynamics_train_loss   | -53.5       |
| timestep                   | 387         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010195046 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 388         |
-----------------------------------------------------------------------------
-----------------------------------------------------------------------------
| loss/dynamics_holdout_loss | 0.010302471 |
| loss/dynamics_train_loss   | -53.6       |
| timestep                   | 389         |
-----------------------------------------------------------------------------
elites:[3, 1, 4, 2, 5] , holdout loss: 0.009826074354350567
num rollout transitions: 250000, reward mean: 4.7993
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.8430
num rollout transitions: 250000, reward mean: 4.8693
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.05e-09 |
| adv_dynamics_update/adv_log_prob   | 28.1     |
| adv_dynamics_update/adv_loss       | 1.57     |
| adv_dynamics_update/all_loss       | -52.3    |
| adv_dynamics_update/sl_loss        | -52.3    |
| alpha                              | 1        |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.2      |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | 11.4     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 26.4     |
| loss/critic2                       | 25.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 1000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9531
num rollout transitions: 250000, reward mean: 4.9777
num rollout transitions: 250000, reward mean: 4.9524
num rollout transitions: 250000, reward mean: 4.9330
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.02e-10 |
| adv_dynamics_update/adv_log_prob   | 26.6     |
| adv_dynamics_update/adv_loss       | 3.47     |
| adv_dynamics_update/all_loss       | -53.2    |
| adv_dynamics_update/sl_loss        | -53.2    |
| alpha                              | 0.953    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.1      |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -35      |
| loss/alpha                         | -0.441   |
| loss/critic1                       | 3.15     |
| loss/critic2                       | 3.26     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 2000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9066
num rollout transitions: 250000, reward mean: 4.9089
num rollout transitions: 250000, reward mean: 4.9169
num rollout transitions: 250000, reward mean: 4.9066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.66e-10 |
| adv_dynamics_update/adv_log_prob   | 28.8     |
| adv_dynamics_update/adv_loss       | 3.39     |
| adv_dynamics_update/all_loss       | -53.2    |
| adv_dynamics_update/sl_loss        | -53.2    |
| alpha                              | 0.871    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 2.05     |
| eval/normalized_episode_reward_std | 2.26     |
| loss/actor                         | -63.8    |
| loss/alpha                         | -1.25    |
| loss/critic1                       | 4.24     |
| loss/critic2                       | 4.42     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 3000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9034
num rollout transitions: 250000, reward mean: 4.9017
num rollout transitions: 250000, reward mean: 4.9010
num rollout transitions: 250000, reward mean: 4.8997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-09 |
| adv_dynamics_update/adv_log_prob   | 33.4      |
| adv_dynamics_update/adv_loss       | 2.42      |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.792     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 2.13      |
| eval/normalized_episode_reward_std | 2.26      |
| loss/actor                         | -87.4     |
| loss/alpha                         | -2.02     |
| loss/critic1                       | 6.49      |
| loss/critic2                       | 6.65      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 4000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8718
num rollout transitions: 250000, reward mean: 4.8801
num rollout transitions: 250000, reward mean: 4.8554
num rollout transitions: 250000, reward mean: 4.8493
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.57e-11 |
| adv_dynamics_update/adv_log_prob   | 35.4      |
| adv_dynamics_update/adv_loss       | 2.07      |
| adv_dynamics_update/all_loss       | -53.1     |
| adv_dynamics_update/sl_loss        | -53.1     |
| alpha                              | 0.721     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 4.11      |
| eval/normalized_episode_reward_std | 4.53      |
| loss/actor                         | -107      |
| loss/alpha                         | -2.66     |
| loss/critic1                       | 8.77      |
| loss/critic2                       | 9         |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.86      |
| timestep                           | 5000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8190
num rollout transitions: 250000, reward mean: 4.8142
num rollout transitions: 250000, reward mean: 4.8184
num rollout transitions: 250000, reward mean: 4.7899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 36.5     |
| adv_dynamics_update/adv_loss       | 1.96     |
| adv_dynamics_update/all_loss       | -53.1    |
| adv_dynamics_update/sl_loss        | -53.1    |
| alpha                              | 0.657    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 1.82     |
| eval/normalized_episode_reward_std | 3.47     |
| loss/actor                         | -123     |
| loss/alpha                         | -3.11    |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.81     |
| timestep                           | 6000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8230
num rollout transitions: 250000, reward mean: 4.8231
num rollout transitions: 250000, reward mean: 4.8095
num rollout transitions: 250000, reward mean: 4.8264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.47e-10 |
| adv_dynamics_update/adv_log_prob   | 38.8     |
| adv_dynamics_update/adv_loss       | 2.42     |
| adv_dynamics_update/all_loss       | -53.3    |
| adv_dynamics_update/sl_loss        | -53.3    |
| alpha                              | 0.6      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 7.2      |
| eval/normalized_episode_reward_std | 9.84     |
| loss/actor                         | -138     |
| loss/alpha                         | -3.39    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.82     |
| timestep                           | 7000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.7999
num rollout transitions: 250000, reward mean: 4.8147
num rollout transitions: 250000, reward mean: 4.8109
num rollout transitions: 250000, reward mean: 4.8306
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-09 |
| adv_dynamics_update/adv_log_prob   | 39.5      |
| adv_dynamics_update/adv_loss       | 1.84      |
| adv_dynamics_update/all_loss       | -53.2     |
| adv_dynamics_update/sl_loss        | -53.2     |
| alpha                              | 0.548     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 1.17      |
| eval/normalized_episode_reward_std | 4.74      |
| loss/actor                         | -150      |
| loss/alpha                         | -3.64     |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.81      |
| timestep                           | 8000      |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8610
num rollout transitions: 250000, reward mean: 4.8414
num rollout transitions: 250000, reward mean: 4.8422
num rollout transitions: 250000, reward mean: 4.8763
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-09 |
| adv_dynamics_update/adv_log_prob   | 40.3     |
| adv_dynamics_update/adv_loss       | 1.74     |
| adv_dynamics_update/all_loss       | -53.2    |
| adv_dynamics_update/sl_loss        | -53.2    |
| alpha                              | 0.5      |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 5.26     |
| eval/normalized_episode_reward_std | 8.22     |
| loss/actor                         | -160     |
| loss/alpha                         | -3.92    |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 9000     |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8438
num rollout transitions: 250000, reward mean: 4.8477
num rollout transitions: 250000, reward mean: 4.8824
num rollout transitions: 250000, reward mean: 4.8602
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.02e-10 |
| adv_dynamics_update/adv_log_prob   | 41.7     |
| adv_dynamics_update/adv_loss       | 0.478    |
| adv_dynamics_update/all_loss       | -53.3    |
| adv_dynamics_update/sl_loss        | -53.3    |
| alpha                              | 0.457    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 11.6     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -169     |
| loss/alpha                         | -4.1     |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 10000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8377
num rollout transitions: 250000, reward mean: 4.8541
num rollout transitions: 250000, reward mean: 4.8732
num rollout transitions: 250000, reward mean: 4.8590
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.64e-10 |
| adv_dynamics_update/adv_log_prob   | 42.5     |
| adv_dynamics_update/adv_loss       | 1.36     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.417    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 25.8     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -178     |
| loss/alpha                         | -4.18    |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.86     |
| timestep                           | 11000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8633
num rollout transitions: 250000, reward mean: 4.8883
num rollout transitions: 250000, reward mean: 4.8613
num rollout transitions: 250000, reward mean: 4.8651
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 1.16      |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.381     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 26.5      |
| eval/normalized_episode_reward_std | 18.2      |
| loss/actor                         | -186      |
| loss/alpha                         | -4.19     |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.87      |
| timestep                           | 12000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8823
num rollout transitions: 250000, reward mean: 4.8596
num rollout transitions: 250000, reward mean: 4.8507
num rollout transitions: 250000, reward mean: 4.8597
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.95e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | 0.929     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.348     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 27.7      |
| eval/normalized_episode_reward_std | 20.6      |
| loss/actor                         | -194      |
| loss/alpha                         | -4.13     |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.86      |
| timestep                           | 13000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8578
num rollout transitions: 250000, reward mean: 4.8643
num rollout transitions: 250000, reward mean: 4.8501
num rollout transitions: 250000, reward mean: 4.8621
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.95e-09 |
| adv_dynamics_update/adv_log_prob   | 42.5      |
| adv_dynamics_update/adv_loss       | 0.587     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.319     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 47.4      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -203      |
| loss/alpha                         | -3.8      |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.86      |
| timestep                           | 14000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8626
num rollout transitions: 250000, reward mean: 4.8813
num rollout transitions: 250000, reward mean: 4.8732
num rollout transitions: 250000, reward mean: 4.8879
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-09 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.744     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.293     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 34.6      |
| eval/normalized_episode_reward_std | 20.1      |
| loss/actor                         | -211      |
| loss/alpha                         | -3.51     |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 15000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9135
num rollout transitions: 250000, reward mean: 4.8917
num rollout transitions: 250000, reward mean: 4.8913
num rollout transitions: 250000, reward mean: 4.8774
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-09 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -53.1    |
| adv_dynamics_update/sl_loss        | -53.1    |
| alpha                              | 0.269    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 41.7     |
| eval/normalized_episode_reward_std | 18.1     |
| loss/actor                         | -220     |
| loss/alpha                         | -3.13    |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 16000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9010
num rollout transitions: 250000, reward mean: 4.8803
num rollout transitions: 250000, reward mean: 4.9004
num rollout transitions: 250000, reward mean: 4.9139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.39e-09 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 1.11      |
| adv_dynamics_update/all_loss       | -52.8     |
| adv_dynamics_update/sl_loss        | -52.8     |
| alpha                              | 0.249     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 38.5      |
| eval/normalized_episode_reward_std | 18.1      |
| loss/actor                         | -230      |
| loss/alpha                         | -2.65     |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 17000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8956
num rollout transitions: 250000, reward mean: 4.8717
num rollout transitions: 250000, reward mean: 4.8692
num rollout transitions: 250000, reward mean: 4.9007
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.65e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 1.2       |
| adv_dynamics_update/all_loss       | -53.2     |
| adv_dynamics_update/sl_loss        | -53.2     |
| alpha                              | 0.23      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 34.4      |
| eval/normalized_episode_reward_std | 21        |
| loss/actor                         | -239      |
| loss/alpha                         | -2.13     |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.88      |
| timestep                           | 18000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8979
num rollout transitions: 250000, reward mean: 4.8896
num rollout transitions: 250000, reward mean: 4.8801
num rollout transitions: 250000, reward mean: 4.8793
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.519     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.215     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 49.9      |
| eval/normalized_episode_reward_std | 13.8      |
| loss/actor                         | -249      |
| loss/alpha                         | -1.52     |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 19000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8803
num rollout transitions: 250000, reward mean: 4.9022
num rollout transitions: 250000, reward mean: 4.8921
num rollout transitions: 250000, reward mean: 4.9130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.458     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.202     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 45.2      |
| eval/normalized_episode_reward_std | 19.2      |
| loss/actor                         | -259      |
| loss/alpha                         | -1.02     |
| loss/critic1                       | 18.9      |
| loss/critic2                       | 19        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 20000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9072
num rollout transitions: 250000, reward mean: 4.8821
num rollout transitions: 250000, reward mean: 4.9032
num rollout transitions: 250000, reward mean: 4.8801
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.632     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.193     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 39.1      |
| eval/normalized_episode_reward_std | 21.2      |
| loss/actor                         | -268      |
| loss/alpha                         | -0.543    |
| loss/critic1                       | 20        |
| loss/critic2                       | 20        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 21000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8903
num rollout transitions: 250000, reward mean: 4.9007
num rollout transitions: 250000, reward mean: 4.8918
num rollout transitions: 250000, reward mean: 4.8914
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.659     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.188     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 50.7      |
| eval/normalized_episode_reward_std | 15.2      |
| loss/actor                         | -278      |
| loss/alpha                         | -0.196    |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 19.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 22000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8819
num rollout transitions: 250000, reward mean: 4.8828
num rollout transitions: 250000, reward mean: 4.8977
num rollout transitions: 250000, reward mean: 4.8968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-09 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 42.8      |
| eval/normalized_episode_reward_std | 18.4      |
| loss/actor                         | -287      |
| loss/alpha                         | -0.0286   |
| loss/critic1                       | 20.3      |
| loss/critic2                       | 20.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 23000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9034
num rollout transitions: 250000, reward mean: 4.8757
num rollout transitions: 250000, reward mean: 4.8912
num rollout transitions: 250000, reward mean: 4.9060
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.155    |
| adv_dynamics_update/all_loss       | -53.2     |
| adv_dynamics_update/sl_loss        | -53.2     |
| alpha                              | 0.186     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 41.9      |
| eval/normalized_episode_reward_std | 24.6      |
| loss/actor                         | -297      |
| loss/alpha                         | 0.0495    |
| loss/critic1                       | 20.5      |
| loss/critic2                       | 20.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 24000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8885
num rollout transitions: 250000, reward mean: 4.8917
num rollout transitions: 250000, reward mean: 4.8641
num rollout transitions: 250000, reward mean: 4.8750
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.3e-09 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -53.3    |
| adv_dynamics_update/sl_loss        | -53.3    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 52.8     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -305     |
| loss/alpha                         | 0.146    |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.88     |
| timestep                           | 25000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8979
num rollout transitions: 250000, reward mean: 4.9075
num rollout transitions: 250000, reward mean: 4.8972
num rollout transitions: 250000, reward mean: 4.8945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-09 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.18      |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.193     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 45.8      |
| eval/normalized_episode_reward_std | 20.4      |
| loss/actor                         | -313      |
| loss/alpha                         | 0.0472    |
| loss/critic1                       | 21.2      |
| loss/critic2                       | 20.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 26000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8977
num rollout transitions: 250000, reward mean: 4.9002
num rollout transitions: 250000, reward mean: 4.9159
num rollout transitions: 250000, reward mean: 4.9279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.953    |
| adv_dynamics_update/all_loss       | -53.1    |
| adv_dynamics_update/sl_loss        | -53.1    |
| alpha                              | 0.195    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 54.2     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -321     |
| loss/alpha                         | 0.0365   |
| loss/critic1                       | 20.4     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.91     |
| timestep                           | 27000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9147
num rollout transitions: 250000, reward mean: 4.8982
num rollout transitions: 250000, reward mean: 4.9122
num rollout transitions: 250000, reward mean: 4.9114
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.328     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52.3      |
| eval/normalized_episode_reward_std | 12.5      |
| loss/actor                         | -329      |
| loss/alpha                         | -0.0327   |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 20.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 28000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8847
num rollout transitions: 250000, reward mean: 4.8898
num rollout transitions: 250000, reward mean: 4.8891
num rollout transitions: 250000, reward mean: 4.9059
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.123     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.196     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 55.8      |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -336      |
| loss/alpha                         | 0.0737    |
| loss/critic1                       | 20.9      |
| loss/critic2                       | 20.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.89      |
| timestep                           | 29000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9070
num rollout transitions: 250000, reward mean: 4.9035
num rollout transitions: 250000, reward mean: 4.9179
num rollout transitions: 250000, reward mean: 4.9070
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.161     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.198     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 54        |
| eval/normalized_episode_reward_std | 9.93      |
| loss/actor                         | -343      |
| loss/alpha                         | 0.00752   |
| loss/critic1                       | 20.5      |
| loss/critic2                       | 20.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 30000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8850
num rollout transitions: 250000, reward mean: 4.8931
num rollout transitions: 250000, reward mean: 4.8948
num rollout transitions: 250000, reward mean: 4.8947
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.211   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.197    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 4.6      |
| loss/actor                         | -350     |
| loss/alpha                         | -0.00201 |
| loss/critic1                       | 20.5     |
| loss/critic2                       | 20.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.89     |
| timestep                           | 31000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9145
num rollout transitions: 250000, reward mean: 4.8965
num rollout transitions: 250000, reward mean: 4.9047
num rollout transitions: 250000, reward mean: 4.8813
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.95      |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.198     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.5      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -356      |
| loss/alpha                         | 0.00139   |
| loss/critic1                       | 20        |
| loss/critic2                       | 19.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 32000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9045
num rollout transitions: 250000, reward mean: 4.9205
num rollout transitions: 250000, reward mean: 4.9124
num rollout transitions: 250000, reward mean: 4.9287
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.96e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.888     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.196     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 53.5      |
| eval/normalized_episode_reward_std | 11.9      |
| loss/actor                         | -363      |
| loss/alpha                         | -0.0864   |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 33000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9085
num rollout transitions: 250000, reward mean: 4.9303
num rollout transitions: 250000, reward mean: 4.9287
num rollout transitions: 250000, reward mean: 4.9047
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-09 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0374   |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.191    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 56.7     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -369     |
| loss/alpha                         | -0.0711  |
| loss/critic1                       | 19.6     |
| loss/critic2                       | 19.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 34000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8950
num rollout transitions: 250000, reward mean: 4.9185
num rollout transitions: 250000, reward mean: 4.8860
num rollout transitions: 250000, reward mean: 4.9058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.326    |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.191     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52.5      |
| eval/normalized_episode_reward_std | 19.9      |
| loss/actor                         | -375      |
| loss/alpha                         | 0.0172    |
| loss/critic1                       | 19.7      |
| loss/critic2                       | 19.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.9       |
| timestep                           | 35000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9160
num rollout transitions: 250000, reward mean: 4.8982
num rollout transitions: 250000, reward mean: 4.9189
num rollout transitions: 250000, reward mean: 4.9154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.724     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.191     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56.9      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -381      |
| loss/alpha                         | 0.00868   |
| loss/critic1                       | 20.4      |
| loss/critic2                       | 20.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 36000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9131
num rollout transitions: 250000, reward mean: 4.9117
num rollout transitions: 250000, reward mean: 4.9244
num rollout transitions: 250000, reward mean: 4.9062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0777    |
| adv_dynamics_update/all_loss       | -53.1     |
| adv_dynamics_update/sl_loss        | -53.1     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.1      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -387      |
| loss/alpha                         | 0.0389    |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 19.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 37000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9096
num rollout transitions: 250000, reward mean: 4.9186
num rollout transitions: 250000, reward mean: 4.9251
num rollout transitions: 250000, reward mean: 4.9162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.699     |
| adv_dynamics_update/all_loss       | -53.2     |
| adv_dynamics_update/sl_loss        | -53.2     |
| alpha                              | 0.196     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 51.9      |
| eval/normalized_episode_reward_std | 17.7      |
| loss/actor                         | -392      |
| loss/alpha                         | 0.0327    |
| loss/critic1                       | 20        |
| loss/critic2                       | 19.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 38000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9012
num rollout transitions: 250000, reward mean: 4.9090
num rollout transitions: 250000, reward mean: 4.9098
num rollout transitions: 250000, reward mean: 4.9130
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 1.51      |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.5      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -398      |
| loss/alpha                         | -0.00925  |
| loss/critic1                       | 19.5      |
| loss/critic2                       | 19.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.91      |
| timestep                           | 39000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9112
num rollout transitions: 250000, reward mean: 4.9096
num rollout transitions: 250000, reward mean: 4.9252
num rollout transitions: 250000, reward mean: 4.9144
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.263     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.194     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.2      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -403      |
| loss/alpha                         | -0.0643   |
| loss/critic1                       | 19.8      |
| loss/critic2                       | 19.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 40000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9192
num rollout transitions: 250000, reward mean: 4.9243
num rollout transitions: 250000, reward mean: 4.9049
num rollout transitions: 250000, reward mean: 4.9285
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.353     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.189     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.4      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -408      |
| loss/alpha                         | -0.122    |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 41000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9235
num rollout transitions: 250000, reward mean: 4.9082
num rollout transitions: 250000, reward mean: 4.9270
num rollout transitions: 250000, reward mean: 4.9237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.828    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.188    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 51.4     |
| eval/normalized_episode_reward_std | 19.6     |
| loss/actor                         | -413     |
| loss/alpha                         | 0.064    |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 42000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.8996
num rollout transitions: 250000, reward mean: 4.9266
num rollout transitions: 250000, reward mean: 4.9266
num rollout transitions: 250000, reward mean: 4.9495
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.4e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.44     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.5     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -418     |
| loss/alpha                         | -0.0849  |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 43000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9455
num rollout transitions: 250000, reward mean: 4.9325
num rollout transitions: 250000, reward mean: 4.9186
num rollout transitions: 250000, reward mean: 4.9292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.106     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.187     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59.4      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -422      |
| loss/alpha                         | 0.0548    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 44000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9115
num rollout transitions: 250000, reward mean: 4.9153
num rollout transitions: 250000, reward mean: 4.9184
num rollout transitions: 250000, reward mean: 4.9185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 1.68     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.1     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -427     |
| loss/alpha                         | 0.0286   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 45000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9275
num rollout transitions: 250000, reward mean: 4.9182
num rollout transitions: 250000, reward mean: 4.9251
num rollout transitions: 250000, reward mean: 4.9307
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.317     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.19      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -431      |
| loss/alpha                         | -0.0318   |
| loss/critic1                       | 18.6      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 46000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9236
num rollout transitions: 250000, reward mean: 4.9236
num rollout transitions: 250000, reward mean: 4.9263
num rollout transitions: 250000, reward mean: 4.9148
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.33     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.189    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -435     |
| loss/alpha                         | 0.00527  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 47000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9387
num rollout transitions: 250000, reward mean: 4.9359
num rollout transitions: 250000, reward mean: 4.9205
num rollout transitions: 250000, reward mean: 4.9258
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.57e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 1.12      |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.188     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -439      |
| loss/alpha                         | -0.0696   |
| loss/critic1                       | 19        |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 48000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9160
num rollout transitions: 250000, reward mean: 4.9205
num rollout transitions: 250000, reward mean: 4.9533
num rollout transitions: 250000, reward mean: 4.9249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-09 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.368     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.185     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 56.7      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -443      |
| loss/alpha                         | -0.046    |
| loss/critic1                       | 19.4      |
| loss/critic2                       | 19.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 49000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9077
num rollout transitions: 250000, reward mean: 4.9383
num rollout transitions: 250000, reward mean: 4.9083
num rollout transitions: 250000, reward mean: 4.9473
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.37e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 2.04     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.183    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.6     |
| eval/normalized_episode_reward_std | 2.56     |
| loss/actor                         | -447     |
| loss/alpha                         | -0.0297  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 50000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9263
num rollout transitions: 250000, reward mean: 4.9191
num rollout transitions: 250000, reward mean: 4.9173
num rollout transitions: 250000, reward mean: 4.9343
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-09 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.168    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.2      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -451      |
| loss/alpha                         | 0.00592   |
| loss/critic1                       | 19.6      |
| loss/critic2                       | 19.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 51000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9307
num rollout transitions: 250000, reward mean: 4.9277
num rollout transitions: 250000, reward mean: 4.9140
num rollout transitions: 250000, reward mean: 4.9340
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-09 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.319    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 58.9     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -455     |
| loss/alpha                         | -0.0539  |
| loss/critic1                       | 19.2     |
| loss/critic2                       | 18.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 52000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9190
num rollout transitions: 250000, reward mean: 4.9039
num rollout transitions: 250000, reward mean: 4.9181
num rollout transitions: 250000, reward mean: 4.9223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.6     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -459     |
| loss/alpha                         | 0.0139   |
| loss/critic1                       | 18.3     |
| loss/critic2                       | 17.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 53000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9266
num rollout transitions: 250000, reward mean: 4.9229
num rollout transitions: 250000, reward mean: 4.9404
num rollout transitions: 250000, reward mean: 4.9064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-09 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.461    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.182    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.9     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -462     |
| loss/alpha                         | 0.0507   |
| loss/critic1                       | 18.9     |
| loss/critic2                       | 18.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.92     |
| timestep                           | 54000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9018
num rollout transitions: 250000, reward mean: 4.9301
num rollout transitions: 250000, reward mean: 4.9281
num rollout transitions: 250000, reward mean: 4.9384
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.8      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -466      |
| loss/alpha                         | -0.0683   |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.92      |
| timestep                           | 55000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9331
num rollout transitions: 250000, reward mean: 4.9303
num rollout transitions: 250000, reward mean: 4.9228
num rollout transitions: 250000, reward mean: 4.9383
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.21      |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.18      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.5      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -469      |
| loss/alpha                         | -0.0158   |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 56000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9427
num rollout transitions: 250000, reward mean: 4.9286
num rollout transitions: 250000, reward mean: 4.9326
num rollout transitions: 250000, reward mean: 4.9373
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-09 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -1.58     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.183     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59        |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -472      |
| loss/alpha                         | 0.109     |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 57000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9354
num rollout transitions: 250000, reward mean: 4.9273
num rollout transitions: 250000, reward mean: 4.9182
num rollout transitions: 250000, reward mean: 4.9308
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.86e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.33      |
| adv_dynamics_update/all_loss       | -53.2     |
| adv_dynamics_update/sl_loss        | -53.2     |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.8      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -475      |
| loss/alpha                         | -0.188    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 58000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9256
num rollout transitions: 250000, reward mean: 4.9099
num rollout transitions: 250000, reward mean: 4.9373
num rollout transitions: 250000, reward mean: 4.9372
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -1.03     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.178     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -477      |
| loss/alpha                         | 0.00184   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 59000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9462
num rollout transitions: 250000, reward mean: 4.9277
num rollout transitions: 250000, reward mean: 4.9235
num rollout transitions: 250000, reward mean: 4.9238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.214    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.18     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.5     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -480     |
| loss/alpha                         | 0.0958   |
| loss/critic1                       | 19       |
| loss/critic2                       | 18.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 60000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9298
num rollout transitions: 250000, reward mean: 4.9379
num rollout transitions: 250000, reward mean: 4.9382
num rollout transitions: 250000, reward mean: 4.9185
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 1.77      |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.182     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59.4      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -483      |
| loss/alpha                         | -0.0142   |
| loss/critic1                       | 19.1      |
| loss/critic2                       | 18.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 61000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9299
num rollout transitions: 250000, reward mean: 4.9331
num rollout transitions: 250000, reward mean: 4.9317
num rollout transitions: 250000, reward mean: 4.9241
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.584     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59.9      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -485      |
| loss/alpha                         | -0.0277   |
| loss/critic1                       | 18.8      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 62000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9453
num rollout transitions: 250000, reward mean: 4.9389
num rollout transitions: 250000, reward mean: 4.9286
num rollout transitions: 250000, reward mean: 4.9239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.53e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.719     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.181     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.4      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -488      |
| loss/alpha                         | 0.0302    |
| loss/critic1                       | 18.7      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 63000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9420
num rollout transitions: 250000, reward mean: 4.9555
num rollout transitions: 250000, reward mean: 4.9430
num rollout transitions: 250000, reward mean: 4.9309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.0402    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.179     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.6      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -491      |
| loss/alpha                         | -0.0886   |
| loss/critic1                       | 18.5      |
| loss/critic2                       | 18        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 64000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9269
num rollout transitions: 250000, reward mean: 4.9412
num rollout transitions: 250000, reward mean: 4.9378
num rollout transitions: 250000, reward mean: 4.9244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-09 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.298   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.177    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -493     |
| loss/alpha                         | -0.0371  |
| loss/critic1                       | 18.4     |
| loss/critic2                       | 18       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 65000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9264
num rollout transitions: 250000, reward mean: 4.9252
num rollout transitions: 250000, reward mean: 4.9212
num rollout transitions: 250000, reward mean: 4.9340
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.803     |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.176     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 59        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -496      |
| loss/alpha                         | 0.0264    |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.93      |
| timestep                           | 66000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9311
num rollout transitions: 250000, reward mean: 4.9334
num rollout transitions: 250000, reward mean: 4.9450
num rollout transitions: 250000, reward mean: 4.9406
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.2      |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.5     |
| eval/normalized_episode_reward_std | 2.48     |
| loss/actor                         | -498     |
| loss/alpha                         | -0.0888  |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 67000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9400
num rollout transitions: 250000, reward mean: 4.9247
num rollout transitions: 250000, reward mean: 4.9305
num rollout transitions: 250000, reward mean: 4.9200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.244    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.173    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.3     |
| eval/normalized_episode_reward_std | 2.47     |
| loss/actor                         | -501     |
| loss/alpha                         | -0.0322  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 68000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9360
num rollout transitions: 250000, reward mean: 4.9302
num rollout transitions: 250000, reward mean: 4.9295
num rollout transitions: 250000, reward mean: 4.9483
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.163    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.174    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.2     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -503     |
| loss/alpha                         | 0.0117   |
| loss/critic1                       | 18.2     |
| loss/critic2                       | 17.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 69000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9350
num rollout transitions: 250000, reward mean: 4.9352
num rollout transitions: 250000, reward mean: 4.9397
num rollout transitions: 250000, reward mean: 4.9405
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.44e-09 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.731    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -505     |
| loss/alpha                         | 0.0741   |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 70000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9471
num rollout transitions: 250000, reward mean: 4.9301
num rollout transitions: 250000, reward mean: 4.9444
num rollout transitions: 250000, reward mean: 4.9248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.9e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.0407   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.175    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61       |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -507     |
| loss/alpha                         | -0.0964  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 71000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9422
num rollout transitions: 250000, reward mean: 4.9375
num rollout transitions: 250000, reward mean: 4.9512
num rollout transitions: 250000, reward mean: 4.9373
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-09 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.396     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.172     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -509      |
| loss/alpha                         | -0.0303   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.94      |
| timestep                           | 72000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9431
num rollout transitions: 250000, reward mean: 4.9377
num rollout transitions: 250000, reward mean: 4.9388
num rollout transitions: 250000, reward mean: 4.9400
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.235   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -511     |
| loss/alpha                         | -0.0897  |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 73000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9418
num rollout transitions: 250000, reward mean: 4.9226
num rollout transitions: 250000, reward mean: 4.9301
num rollout transitions: 250000, reward mean: 4.9378
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-09 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.264    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.4     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -513     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.93     |
| timestep                           | 74000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9497
num rollout transitions: 250000, reward mean: 4.9389
num rollout transitions: 250000, reward mean: 4.9359
num rollout transitions: 250000, reward mean: 4.9616
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.133     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.4      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -515      |
| loss/alpha                         | 0.0108    |
| loss/critic1                       | 17.4      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 75000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9559
num rollout transitions: 250000, reward mean: 4.9522
num rollout transitions: 250000, reward mean: 4.9371
num rollout transitions: 250000, reward mean: 4.9418
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.38     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60       |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -517     |
| loss/alpha                         | 0.0412   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 76000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9336
num rollout transitions: 250000, reward mean: 4.9391
num rollout transitions: 250000, reward mean: 4.9391
num rollout transitions: 250000, reward mean: 4.9436
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.351   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.171    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.7     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -519     |
| loss/alpha                         | 0.0152   |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.94     |
| timestep                           | 77000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9491
num rollout transitions: 250000, reward mean: 4.9593
num rollout transitions: 250000, reward mean: 4.9365
num rollout transitions: 250000, reward mean: 4.9429
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.778    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -522     |
| loss/alpha                         | -0.0202  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 78000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9452
num rollout transitions: 250000, reward mean: 4.9522
num rollout transitions: 250000, reward mean: 4.9411
num rollout transitions: 250000, reward mean: 4.9443
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.123    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.17     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -523     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 79000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9594
num rollout transitions: 250000, reward mean: 4.9292
num rollout transitions: 250000, reward mean: 4.9402
num rollout transitions: 250000, reward mean: 4.9593
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.792     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.169     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.1      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -525      |
| loss/alpha                         | 0.0508    |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 80000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9570
num rollout transitions: 250000, reward mean: 4.9435
num rollout transitions: 250000, reward mean: 4.9482
num rollout transitions: 250000, reward mean: 4.9644
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.01      |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.171     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.2      |
| eval/normalized_episode_reward_std | 2.42      |
| loss/actor                         | -527      |
| loss/alpha                         | -0.0581   |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 81000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9460
num rollout transitions: 250000, reward mean: 4.9540
num rollout transitions: 250000, reward mean: 4.9436
num rollout transitions: 250000, reward mean: 4.9558
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.21     |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -529     |
| loss/alpha                         | -0.0826  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 82000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9413
num rollout transitions: 250000, reward mean: 4.9439
num rollout transitions: 250000, reward mean: 4.9380
num rollout transitions: 250000, reward mean: 4.9601
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.37      |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.7      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -531      |
| loss/alpha                         | -0.0384   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 83000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9538
num rollout transitions: 250000, reward mean: 4.9478
num rollout transitions: 250000, reward mean: 4.9480
num rollout transitions: 250000, reward mean: 4.9516
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0257    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.1      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -532      |
| loss/alpha                         | 0.0379    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 84000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9558
num rollout transitions: 250000, reward mean: 4.9610
num rollout transitions: 250000, reward mean: 4.9417
num rollout transitions: 250000, reward mean: 4.9652
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.354    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.5      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -534      |
| loss/alpha                         | -0.0611   |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 85000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9460
num rollout transitions: 250000, reward mean: 4.9514
num rollout transitions: 250000, reward mean: 4.9539
num rollout transitions: 250000, reward mean: 4.9508
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-09 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.391   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -536     |
| loss/alpha                         | 0.13     |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 86000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9533
num rollout transitions: 250000, reward mean: 4.9615
num rollout transitions: 250000, reward mean: 4.9462
num rollout transitions: 250000, reward mean: 4.9542
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-09 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.0961   |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.2      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -537      |
| loss/alpha                         | -0.0211   |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 87000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9688
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9642
num rollout transitions: 250000, reward mean: 4.9596
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.891     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -539      |
| loss/alpha                         | -0.0681   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 88000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9537
num rollout transitions: 250000, reward mean: 4.9484
num rollout transitions: 250000, reward mean: 4.9434
num rollout transitions: 250000, reward mean: 4.9435
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 2.06     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.4     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -541     |
| loss/alpha                         | 0.0236   |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 89000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9698
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9561
num rollout transitions: 250000, reward mean: 4.9535
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.153    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 2.42      |
| loss/actor                         | -542      |
| loss/alpha                         | -0.0446   |
| loss/critic1                       | 18.2      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 90000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9426
num rollout transitions: 250000, reward mean: 4.9477
num rollout transitions: 250000, reward mean: 4.9522
num rollout transitions: 250000, reward mean: 4.9505
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.45     |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -544     |
| loss/alpha                         | 0.00698  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 91000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9490
num rollout transitions: 250000, reward mean: 4.9421
num rollout transitions: 250000, reward mean: 4.9666
num rollout transitions: 250000, reward mean: 4.9511
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.2      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -546      |
| loss/alpha                         | -0.0522   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 92000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9442
num rollout transitions: 250000, reward mean: 4.9475
num rollout transitions: 250000, reward mean: 4.9553
num rollout transitions: 250000, reward mean: 4.9685
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5e-10   |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.178    |
| adv_dynamics_update/all_loss       | -53.4    |
| adv_dynamics_update/sl_loss        | -53.4    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -547     |
| loss/alpha                         | -0.00719 |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 93000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9634
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9599
num rollout transitions: 250000, reward mean: 4.9670
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.265    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -549     |
| loss/alpha                         | 0.00816  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 94000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9651
num rollout transitions: 250000, reward mean: 4.9603
num rollout transitions: 250000, reward mean: 4.9447
num rollout transitions: 250000, reward mean: 4.9540
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.17e-09 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0423    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -550      |
| loss/alpha                         | 0.0643    |
| loss/critic1                       | 18.1      |
| loss/critic2                       | 17.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 95000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9641
num rollout transitions: 250000, reward mean: 4.9590
num rollout transitions: 250000, reward mean: 4.9550
num rollout transitions: 250000, reward mean: 4.9510
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.424    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -551     |
| loss/alpha                         | 0.0292   |
| loss/critic1                       | 17.8     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 96000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9407
num rollout transitions: 250000, reward mean: 4.9600
num rollout transitions: 250000, reward mean: 4.9472
num rollout transitions: 250000, reward mean: 4.9495
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.759    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -552     |
| loss/alpha                         | 0.0135   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 97000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9659
num rollout transitions: 250000, reward mean: 4.9540
num rollout transitions: 250000, reward mean: 4.9652
num rollout transitions: 250000, reward mean: 4.9614
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.376   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -554     |
| loss/alpha                         | -0.0925  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 98000    |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9504
num rollout transitions: 250000, reward mean: 4.9572
num rollout transitions: 250000, reward mean: 4.9554
num rollout transitions: 250000, reward mean: 4.9601
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.221     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 2.54      |
| loss/actor                         | -555      |
| loss/alpha                         | 0.048     |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 99000     |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9748
num rollout transitions: 250000, reward mean: 4.9473
num rollout transitions: 250000, reward mean: 4.9599
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.329    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -556     |
| loss/alpha                         | -0.0564  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 100000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9731
num rollout transitions: 250000, reward mean: 4.9746
num rollout transitions: 250000, reward mean: 4.9628
num rollout transitions: 250000, reward mean: 4.9657
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -558     |
| loss/alpha                         | -0.0486  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 101000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9628
num rollout transitions: 250000, reward mean: 4.9665
num rollout transitions: 250000, reward mean: 4.9635
num rollout transitions: 250000, reward mean: 4.9608
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.172    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -559      |
| loss/alpha                         | 0.0278    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 102000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9527
num rollout transitions: 250000, reward mean: 4.9625
num rollout transitions: 250000, reward mean: 4.9552
num rollout transitions: 250000, reward mean: 4.9486
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.408   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65       |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -560     |
| loss/alpha                         | 0.0409   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.95     |
| timestep                           | 103000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9646
num rollout transitions: 250000, reward mean: 4.9777
num rollout transitions: 250000, reward mean: 4.9712
num rollout transitions: 250000, reward mean: 4.9826
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.296   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -562     |
| loss/alpha                         | -0.0491  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 104000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9786
num rollout transitions: 250000, reward mean: 4.9639
num rollout transitions: 250000, reward mean: 4.9439
num rollout transitions: 250000, reward mean: 4.9607
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.819    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -563     |
| loss/alpha                         | 0.0924   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 105000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9496
num rollout transitions: 250000, reward mean: 4.9588
num rollout transitions: 250000, reward mean: 4.9659
num rollout transitions: 250000, reward mean: 4.9599
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.285    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.168    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -564     |
| loss/alpha                         | 0.068    |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 106000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9620
num rollout transitions: 250000, reward mean: 4.9624
num rollout transitions: 250000, reward mean: 4.9578
num rollout transitions: 250000, reward mean: 4.9662
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.632     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.9      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -565      |
| loss/alpha                         | -0.0765   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 107000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9691
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9594
num rollout transitions: 250000, reward mean: 4.9575
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -53.3    |
| adv_dynamics_update/sl_loss        | -53.3    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -566     |
| loss/alpha                         | 0.0365   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 108000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9626
num rollout transitions: 250000, reward mean: 4.9750
num rollout transitions: 250000, reward mean: 4.9679
num rollout transitions: 250000, reward mean: 4.9756
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-09 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.232    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.2      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -568      |
| loss/alpha                         | -0.0756   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 109000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9590
num rollout transitions: 250000, reward mean: 4.9498
num rollout transitions: 250000, reward mean: 4.9595
num rollout transitions: 250000, reward mean: 4.9619
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64        |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -569      |
| loss/alpha                         | 0.0504    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 110000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9580
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9607
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.915    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -570     |
| loss/alpha                         | -0.0501  |
| loss/critic1                       | 17.5     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 111000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9545
num rollout transitions: 250000, reward mean: 4.9649
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9653
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.093   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -571     |
| loss/alpha                         | 0.0095   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.96     |
| timestep                           | 112000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9665
num rollout transitions: 250000, reward mean: 4.9470
num rollout transitions: 250000, reward mean: 4.9485
num rollout transitions: 250000, reward mean: 4.9543
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0475   |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.3      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -572      |
| loss/alpha                         | 0.0155    |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.95      |
| timestep                           | 113000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9611
num rollout transitions: 250000, reward mean: 4.9694
num rollout transitions: 250000, reward mean: 4.9634
num rollout transitions: 250000, reward mean: 4.9630
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.552     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -573      |
| loss/alpha                         | -0.0684   |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.96      |
| timestep                           | 114000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9669
num rollout transitions: 250000, reward mean: 4.9685
num rollout transitions: 250000, reward mean: 4.9641
num rollout transitions: 250000, reward mean: 4.9701
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.0841   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.1     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -574     |
| loss/alpha                         | -0.01    |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 115000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9760
num rollout transitions: 250000, reward mean: 4.9695
num rollout transitions: 250000, reward mean: 4.9683
num rollout transitions: 250000, reward mean: 4.9559
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.678     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.1      |
| eval/normalized_episode_reward_std | 2.61      |
| loss/actor                         | -575      |
| loss/alpha                         | -0.13     |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 116000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9777
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9712
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.675    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -577      |
| loss/alpha                         | 0.115     |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 117000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9639
num rollout transitions: 250000, reward mean: 4.9686
num rollout transitions: 250000, reward mean: 4.9652
num rollout transitions: 250000, reward mean: 4.9676
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.67e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.339    |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.5      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -578      |
| loss/alpha                         | -0.062    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 118000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9682
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9723
num rollout transitions: 250000, reward mean: 4.9574
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.371    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -579      |
| loss/alpha                         | 0.02      |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 119000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9588
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.281   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -580     |
| loss/alpha                         | 0.0782   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 120000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9536
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 4.9660
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.809    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -581     |
| loss/alpha                         | -0.0563  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 121000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9778
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9707
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.552    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -582     |
| loss/alpha                         | 0.0536   |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 122000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9667
num rollout transitions: 250000, reward mean: 4.9730
num rollout transitions: 250000, reward mean: 4.9597
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-09 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.0645   |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -583      |
| loss/alpha                         | -0.0236   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 123000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9646
num rollout transitions: 250000, reward mean: 4.9734
num rollout transitions: 250000, reward mean: 4.9747
num rollout transitions: 250000, reward mean: 4.9741
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.184   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -584     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 124000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 4.9756
num rollout transitions: 250000, reward mean: 4.9727
num rollout transitions: 250000, reward mean: 4.9680
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.0381   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -585     |
| loss/alpha                         | -0.0385  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 125000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9667
num rollout transitions: 250000, reward mean: 4.9596
num rollout transitions: 250000, reward mean: 4.9692
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.899     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.2      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -586      |
| loss/alpha                         | 0.0408    |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 126000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9740
num rollout transitions: 250000, reward mean: 4.9584
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.73e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.239     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -587      |
| loss/alpha                         | 0.0174    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 127000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9645
num rollout transitions: 250000, reward mean: 4.9768
num rollout transitions: 250000, reward mean: 4.9731
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.267    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -588      |
| loss/alpha                         | 0.00578   |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 128000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9692
num rollout transitions: 250000, reward mean: 4.9638
num rollout transitions: 250000, reward mean: 4.9617
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.382    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -589     |
| loss/alpha                         | 0.00718  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 129000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9679
num rollout transitions: 250000, reward mean: 4.9622
num rollout transitions: 250000, reward mean: 4.9681
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.524    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -590     |
| loss/alpha                         | -0.0223  |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 130000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9752
num rollout transitions: 250000, reward mean: 4.9783
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9669
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.602   |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -591     |
| loss/alpha                         | 0.0551   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 131000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9763
num rollout transitions: 250000, reward mean: 4.9776
num rollout transitions: 250000, reward mean: 4.9769
num rollout transitions: 250000, reward mean: 4.9715
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.205     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -591      |
| loss/alpha                         | -0.0537   |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 132000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9705
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9675
num rollout transitions: 250000, reward mean: 4.9793
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.0589  |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -592     |
| loss/alpha                         | -0.0498  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 133000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9788
num rollout transitions: 250000, reward mean: 4.9826
num rollout transitions: 250000, reward mean: 4.9862
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.69    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -593     |
| loss/alpha                         | 0.109    |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 134000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9567
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.1e-11  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.286   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -594     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 16.6     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 135000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9625
num rollout transitions: 250000, reward mean: 4.9724
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9835
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.427    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -595     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 136000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9661
num rollout transitions: 250000, reward mean: 4.9662
num rollout transitions: 250000, reward mean: 4.9754
num rollout transitions: 250000, reward mean: 4.9733
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.47e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.595   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -596     |
| loss/alpha                         | -0.0553  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 137000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9755
num rollout transitions: 250000, reward mean: 4.9830
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.364    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.4     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -596     |
| loss/alpha                         | 0.0357   |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 138000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9681
num rollout transitions: 250000, reward mean: 4.9601
num rollout transitions: 250000, reward mean: 4.9718
num rollout transitions: 250000, reward mean: 4.9738
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.215    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 7.18     |
| loss/actor                         | -597     |
| loss/alpha                         | -0.00761 |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 139000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9667
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9658
num rollout transitions: 250000, reward mean: 4.9837
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.268     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.2      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -598      |
| loss/alpha                         | 0.0487    |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 140000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9804
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9796
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.26e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.564     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67        |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -599      |
| loss/alpha                         | -0.044    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 141000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9765
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9864
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -1.61    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -599     |
| loss/alpha                         | -0.0636  |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 142000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9798
num rollout transitions: 250000, reward mean: 4.9591
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.55    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -600     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 17.3     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 143000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9749
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9694
num rollout transitions: 250000, reward mean: 4.9629
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.947    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -601     |
| loss/alpha                         | 0.0541   |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 144000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9711
num rollout transitions: 250000, reward mean: 4.9722
num rollout transitions: 250000, reward mean: 4.9659
num rollout transitions: 250000, reward mean: 4.9880
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.1e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.329    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.166    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -601     |
| loss/alpha                         | 0.18     |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 145000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9745
num rollout transitions: 250000, reward mean: 4.9722
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.42    |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.169    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -602     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 146000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9623
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9752
num rollout transitions: 250000, reward mean: 4.9802
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.91e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.689    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.168     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.9      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -603      |
| loss/alpha                         | -0.0606   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 147000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9654
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9766
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.142    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.1      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -604      |
| loss/alpha                         | -0.0114   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 148000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9714
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9759
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.22e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.0229   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -604     |
| loss/alpha                         | -0.1     |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 149000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9704
num rollout transitions: 250000, reward mean: 4.9626
num rollout transitions: 250000, reward mean: 4.9797
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.392     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.2      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -605      |
| loss/alpha                         | -0.041    |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 150000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9904
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.849    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -606      |
| loss/alpha                         | -0.0214   |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 151000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9699
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9719
num rollout transitions: 250000, reward mean: 4.9766
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.754    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.3      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -607      |
| loss/alpha                         | -0.0141   |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 152000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9868
num rollout transitions: 250000, reward mean: 4.9743
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.0231    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.4      |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -607      |
| loss/alpha                         | -0.0572   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 153000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9867
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -1.09     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.6      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -608      |
| loss/alpha                         | -0.0274   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 154000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9868
num rollout transitions: 250000, reward mean: 4.9919
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-09 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.286    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.5     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -609     |
| loss/alpha                         | 0.0639   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 155000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9829
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9847
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.598    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.9      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -609      |
| loss/alpha                         | 0.128     |
| loss/critic1                       | 17        |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 156000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
num rollout transitions: 250000, reward mean: 4.9662
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9756
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.116     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.1      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -610      |
| loss/alpha                         | 0.00398   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 157000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9946
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.0267  |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -611     |
| loss/alpha                         | -0.00151 |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 158000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9782
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9774
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.215    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -612      |
| loss/alpha                         | -0.016    |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 159000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9878
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.135    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -613     |
| loss/alpha                         | 0.0145   |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 160000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9793
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9785
num rollout transitions: 250000, reward mean: 4.9872
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.98e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.138     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -613      |
| loss/alpha                         | -0.0346   |
| loss/critic1                       | 17.7      |
| loss/critic2                       | 17.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 161000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9650
num rollout transitions: 250000, reward mean: 4.9772
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.618   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -614     |
| loss/alpha                         | -0.00732 |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 162000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9660
num rollout transitions: 250000, reward mean: 4.9703
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.696    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -615     |
| loss/alpha                         | 0.0911   |
| loss/critic1                       | 18       |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 163000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9673
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.782     |
| adv_dynamics_update/all_loss       | -53.5     |
| adv_dynamics_update/sl_loss        | -53.5     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -615      |
| loss/alpha                         | -0.0586   |
| loss/critic1                       | 17.1      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 164000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9707
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.00993   |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.3      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -616      |
| loss/alpha                         | -0.0971   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 165000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.598    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -616     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 166000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9850
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.305    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -617     |
| loss/alpha                         | 0.0932   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 167000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9814
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9871
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -1.04    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -617     |
| loss/alpha                         | 0.0717   |
| loss/critic1                       | 16.4     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 168000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9729
num rollout transitions: 250000, reward mean: 4.9751
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.842     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -618      |
| loss/alpha                         | 0.0237    |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 169000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9719
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9847
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.656    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.165     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -618      |
| loss/alpha                         | -0.0447   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 170000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9720
num rollout transitions: 250000, reward mean: 4.9943
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.81e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.152    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.4      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -619      |
| loss/alpha                         | -0.0873   |
| loss/critic1                       | 16.7      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 171000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9779
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.522    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -620      |
| loss/alpha                         | -0.0854   |
| loss/critic1                       | 17.2      |
| loss/critic2                       | 16.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 172000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9774
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.363    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -620     |
| loss/alpha                         | 0.00468  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 173000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9874
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.562    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -621     |
| loss/alpha                         | 0.0583   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 174000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9735
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9756
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.839    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -622     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 175000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9802
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 5.0039
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.75      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -622      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 18.3      |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 176000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9855
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.257    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 7.56     |
| loss/actor                         | -622     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 19.4     |
| loss/critic2                       | 19       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 177000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9826
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9848
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.0834  |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -623     |
| loss/alpha                         | -0.0926  |
| loss/critic1                       | 18.1     |
| loss/critic2                       | 17.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 178000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9677
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.291   |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.9     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -624     |
| loss/alpha                         | 0.0356   |
| loss/critic1                       | 17.7     |
| loss/critic2                       | 17.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 179000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9673
num rollout transitions: 250000, reward mean: 4.9702
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.904    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -624     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.97     |
| timestep                           | 180000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9800
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.12    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 2.51     |
| loss/actor                         | -624     |
| loss/alpha                         | 0.00168  |
| loss/critic1                       | 17.2     |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 181000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9940
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.912    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.6     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -624     |
| loss/alpha                         | -0.0482  |
| loss/critic1                       | 16.9     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 182000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9716
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9824
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.271    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -625     |
| loss/alpha                         | -0.00611 |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 183000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9906
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.69e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.0992   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -626     |
| loss/alpha                         | -0.00337 |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 184000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9872
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-09 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.194   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -626     |
| loss/alpha                         | 0.106    |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 185000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9794
num rollout transitions: 250000, reward mean: 4.9808
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.923    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -627      |
| loss/alpha                         | 0.0163    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 186000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9758
num rollout transitions: 250000, reward mean: 4.9823
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9785
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.22e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.397    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 9.44      |
| loss/actor                         | -627      |
| loss/alpha                         | -0.0311   |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 187000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9911
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0729  |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -627     |
| loss/alpha                         | 0.0169   |
| loss/critic1                       | 17       |
| loss/critic2                       | 16.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 188000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9879
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.709    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.3      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -628      |
| loss/alpha                         | 0.00321   |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 189000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9713
num rollout transitions: 250000, reward mean: 4.9796
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9920
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.521    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -628     |
| loss/alpha                         | -0.0364  |
| loss/critic1                       | 17.1     |
| loss/critic2                       | 16.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 190000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9858
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.822     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -629      |
| loss/alpha                         | 0.0381    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 191000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9905
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.59     |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 2.62     |
| loss/actor                         | -629     |
| loss/alpha                         | -0.0108  |
| loss/critic1                       | 17.6     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 192000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 4.9770
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9760
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | 0.986     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.164     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -630      |
| loss/alpha                         | 0.0527    |
| loss/critic1                       | 17.6      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 193000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9852
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9878
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.57e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.593    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67        |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -630      |
| loss/alpha                         | -0.1      |
| loss/critic1                       | 17.9      |
| loss/critic2                       | 17.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 194000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9841
num rollout transitions: 250000, reward mean: 4.9927
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 1.27      |
| adv_dynamics_update/all_loss       | -53.4     |
| adv_dynamics_update/sl_loss        | -53.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -630      |
| loss/alpha                         | -0.0868   |
| loss/critic1                       | 17.3      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 195000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 4.9885
num rollout transitions: 250000, reward mean: 4.9846
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.181   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -631     |
| loss/alpha                         | 0.00838  |
| loss/critic1                       | 17.9     |
| loss/critic2                       | 17.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 196000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9994
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.78e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.879     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -631      |
| loss/alpha                         | -0.0604   |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 197000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9915
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9783
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.334     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.9      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -631      |
| loss/alpha                         | 0.101     |
| loss/critic1                       | 18        |
| loss/critic2                       | 17.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 198000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9706
num rollout transitions: 250000, reward mean: 4.9849
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.7      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.6      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -631      |
| loss/alpha                         | -0.0186   |
| loss/critic1                       | 16.9      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 199000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9830
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-11  |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.47     |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -632     |
| loss/alpha                         | 0.00014  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 200000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9663
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9747
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -53.5    |
| adv_dynamics_update/sl_loss        | -53.5    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -632     |
| loss/alpha                         | -0.0812  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 201000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9813
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.1      |
| eval/normalized_episode_reward_std | 9.59      |
| loss/actor                         | -632      |
| loss/alpha                         | 0.00176   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 202000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9798
num rollout transitions: 250000, reward mean: 4.9896
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.0793    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -632      |
| loss/alpha                         | 0.00779   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 203000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 4.9890
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 1.05     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -633     |
| loss/alpha                         | 0.0997   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 204000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9887
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.55e-12 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -1.36    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -633     |
| loss/alpha                         | 0.00362  |
| loss/critic1                       | 16.7     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 205000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9842
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.196    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -633      |
| loss/alpha                         | 0.146     |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 206000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 4.9825
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.294    |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.4      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -633      |
| loss/alpha                         | 0.021     |
| loss/critic1                       | 16.4      |
| loss/critic2                       | 16.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 207000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9731
num rollout transitions: 250000, reward mean: 4.9736
num rollout transitions: 250000, reward mean: 4.9709
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.99e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.059    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.166     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -634      |
| loss/alpha                         | -0.0325   |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.97      |
| timestep                           | 208000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 4.9852
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.112    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.165    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -634     |
| loss/alpha                         | -0.0499  |
| loss/critic1                       | 17.4     |
| loss/critic2                       | 17       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 209000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9944
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.293    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0771  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 210000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9781
num rollout transitions: 250000, reward mean: 4.9763
num rollout transitions: 250000, reward mean: 4.9696
num rollout transitions: 250000, reward mean: 4.9870
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.93     |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -635     |
| loss/alpha                         | -0.0762  |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 211000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9816
num rollout transitions: 250000, reward mean: 4.9875
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.499    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -636      |
| loss/alpha                         | 0.1       |
| loss/critic1                       | 15.9      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 212000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9863
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9706
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.163    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -636     |
| loss/alpha                         | -0.00707 |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 213000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
num rollout transitions: 250000, reward mean: 4.9812
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9841
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.383     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -636      |
| loss/alpha                         | -0.0163   |
| loss/critic1                       | 18.4      |
| loss/critic2                       | 18.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 214000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.28e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.0215    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.6      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.00585  |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 215000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9834
num rollout transitions: 250000, reward mean: 4.9865
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9902
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-09 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.819    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -637     |
| loss/alpha                         | 0.0218   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 216000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9825
num rollout transitions: 250000, reward mean: 4.9755
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9781
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.84e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.551     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0431   |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 217000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9808
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9728
num rollout transitions: 250000, reward mean: 4.9792
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.278     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0216   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 218000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9831
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9872
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.273     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0666   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 219000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.6e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.099   |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.113    |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 220000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9914
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 5.0043
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.941     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.6      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -637      |
| loss/alpha                         | -0.0687   |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 221000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 4.9866
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -638     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 222000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9859
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.523    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -638     |
| loss/alpha                         | 0.0127   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 223000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0007
num rollout transitions: 250000, reward mean: 5.0014
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.86e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.0635   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 224000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9762
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9711
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.85e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.594    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.2     |
| eval/normalized_episode_reward_std | 2.54     |
| loss/actor                         | -639     |
| loss/alpha                         | 0.0849   |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 225000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9795
num rollout transitions: 250000, reward mean: 4.9883
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-09   |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -639     |
| loss/alpha                         | 0.00298  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 226000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9950
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0156   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -639     |
| loss/alpha                         | 0.0852   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 227000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9827
num rollout transitions: 250000, reward mean: 4.9762
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.125    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -639     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 15.9     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 228000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0000
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.238    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -640     |
| loss/alpha                         | -0.0618  |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 229000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.391    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.61     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0465   |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 230000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9828
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.3e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.301    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -640     |
| loss/alpha                         | 0.0351   |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 231000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9838
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9838
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 1.26     |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -641     |
| loss/alpha                         | 0.0398   |
| loss/critic1                       | 15.7     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 232000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9807
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.803    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -641     |
| loss/alpha                         | 0.0185   |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 233000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9797
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9805
num rollout transitions: 250000, reward mean: 4.9900
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.544   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.7     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -641     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 234000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9839
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9944
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -641     |
| loss/alpha                         | 0.0188   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 235000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 1.03      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.6      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -642      |
| loss/alpha                         | -0.0244   |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 236000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9651
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9812
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.735     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -642      |
| loss/alpha                         | 0.00527   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 237000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9712
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9972
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.4e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.148   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.162    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -642     |
| loss/alpha                         | 0.00869  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 238000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9847
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9869
num rollout transitions: 250000, reward mean: 4.9966
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.267    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.162     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -642      |
| loss/alpha                         | -0.0157   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 239000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.0651    |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.9      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -643      |
| loss/alpha                         | 0.0242    |
| loss/critic1                       | 15.6      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 240000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9806
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9886
num rollout transitions: 250000, reward mean: 4.9803
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -1.05     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.163     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -643      |
| loss/alpha                         | 0.0955    |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 241000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9845
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9884
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.51    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.164    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 242000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9896
num rollout transitions: 250000, reward mean: 4.9867
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.579   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.0374  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 243000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9815
num rollout transitions: 250000, reward mean: 4.9803
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.351   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.161    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -643     |
| loss/alpha                         | -0.0363  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 244000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9828
num rollout transitions: 250000, reward mean: 4.9801
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9844
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.926     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -644      |
| loss/alpha                         | -0.0944   |
| loss/critic1                       | 15.7      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 245000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9835
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9840
num rollout transitions: 250000, reward mean: 4.9930
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.54e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.081   |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.7     |
| eval/normalized_episode_reward_std | 2.53     |
| loss/actor                         | -644     |
| loss/alpha                         | -0.0198  |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 246000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9941
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.956     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -644      |
| loss/alpha                         | -0.00697  |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 247000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9798
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.08e-12 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.194     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.67      |
| loss/actor                         | -644      |
| loss/alpha                         | 0.0565    |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 248000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9794
num rollout transitions: 250000, reward mean: 4.9881
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9958
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -645      |
| loss/alpha                         | -0.048    |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 249000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9779
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.353   |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -644     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 250000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.884    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.5     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -645     |
| loss/alpha                         | -0.0526  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 251000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9843
num rollout transitions: 250000, reward mean: 4.9852
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.59      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -645      |
| loss/alpha                         | -0.0908   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 252000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9850
num rollout transitions: 250000, reward mean: 4.9660
num rollout transitions: 250000, reward mean: 4.9878
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.634     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68        |
| eval/normalized_episode_reward_std | 2.54      |
| loss/actor                         | -645      |
| loss/alpha                         | 0.0521    |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 253000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0033
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9967
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.521    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -646      |
| loss/alpha                         | -0.00725  |
| loss/critic1                       | 15.2      |
| loss/critic2                       | 15.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 254000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9761
num rollout transitions: 250000, reward mean: 4.9982
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.0432   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -646     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 255000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9664
num rollout transitions: 250000, reward mean: 4.9849
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.9e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.606    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -646     |
| loss/alpha                         | 0.0399   |
| loss/critic1                       | 16.1     |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 256000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9820
num rollout transitions: 250000, reward mean: 4.9988
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0617    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -646      |
| loss/alpha                         | 0.0346    |
| loss/critic1                       | 16.3      |
| loss/critic2                       | 16.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 257000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9873
num rollout transitions: 250000, reward mean: 4.9964
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9924
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.225   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.5     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -646     |
| loss/alpha                         | -0.0298  |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 258000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9698
num rollout transitions: 250000, reward mean: 5.0117
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5e-10   |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.236   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -646     |
| loss/alpha                         | 0.02     |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 259000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.687     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -647      |
| loss/alpha                         | 0.105     |
| loss/critic1                       | 15.1      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 260000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 4.9937
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.239    |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -647      |
| loss/alpha                         | 0.0329    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 261000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9804
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9969
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.49e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.528     |
| adv_dynamics_update/all_loss       | -53.8     |
| adv_dynamics_update/sl_loss        | -53.8     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -648      |
| loss/alpha                         | -0.00278  |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 262000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9897
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.1e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 1.87     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -648     |
| loss/alpha                         | -0.0249  |
| loss/critic1                       | 15.4     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 263000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.0725   |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -648      |
| loss/alpha                         | 0.0459    |
| loss/critic1                       | 16.8      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 264000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9914
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.66e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.476    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -648     |
| loss/alpha                         | -0.122   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 265000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9848
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9894
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.47     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -649     |
| loss/alpha                         | -0.0105  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 266000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9980
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.57e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.667    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.5      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -649      |
| loss/alpha                         | 0.0178    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 267000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9888
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.199   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -649     |
| loss/alpha                         | -0.0246  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 268000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9922
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.22e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0778    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -650      |
| loss/alpha                         | -0.0437   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 269000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 4.9893
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.151   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -650     |
| loss/alpha                         | -0.0556  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 270000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9983
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.369   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.6     |
| eval/normalized_episode_reward_std | 2.59     |
| loss/actor                         | -650     |
| loss/alpha                         | 0.124    |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 14.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 271000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9924
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6      |
| adv_dynamics_update/adv_loss       | 0.912     |
| adv_dynamics_update/all_loss       | -53.6     |
| adv_dynamics_update/sl_loss        | -53.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -651      |
| loss/alpha                         | 0.0217    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 272000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9933
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.57e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.624    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -651     |
| loss/alpha                         | -0.124   |
| loss/critic1                       | 15.5     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 273000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9874
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9906
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.75e-12 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.522     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -652      |
| loss/alpha                         | 0.04      |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 15.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 274000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9868
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9871
num rollout transitions: 250000, reward mean: 4.9976
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -651      |
| loss/alpha                         | 0.0661    |
| loss/critic1                       | 17.5      |
| loss/critic2                       | 17.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 275000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9759
num rollout transitions: 250000, reward mean: 4.9809
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9949
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.442    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 2.6       |
| loss/actor                         | -652      |
| loss/alpha                         | 0.0433    |
| loss/critic1                       | 16.1      |
| loss/critic2                       | 16        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 276000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9734
num rollout transitions: 250000, reward mean: 4.9859
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.566    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -652     |
| loss/alpha                         | -0.107   |
| loss/critic1                       | 16.2     |
| loss/critic2                       | 16       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 277000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9803
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9821
num rollout transitions: 250000, reward mean: 4.9860
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.735    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -652     |
| loss/alpha                         | 0.107    |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.98     |
| timestep                           | 278000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9923
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.258   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -652     |
| loss/alpha                         | -0.0745  |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 279000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9884
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9954
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.498    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -652     |
| loss/alpha                         | -0.0706  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 280000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9887
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.55     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -653      |
| loss/alpha                         | 0.0488    |
| loss/critic1                       | 14.6      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 281000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9994
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.27e-11 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.646     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -653      |
| loss/alpha                         | 0.04      |
| loss/critic1                       | 16        |
| loss/critic2                       | 15.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 282000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9867
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9881
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.552     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.6      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -653      |
| loss/alpha                         | -0.00719  |
| loss/critic1                       | 16.2      |
| loss/critic2                       | 16.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 283000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9794
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0148  |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -653     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 16.8     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 284000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9922
num rollout transitions: 250000, reward mean: 4.9882
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.371     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0183   |
| loss/critic1                       | 15.8      |
| loss/critic2                       | 15.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 285000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 4.9792
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.159     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -653      |
| loss/alpha                         | -0.0161   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 286000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0001
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.159     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -654      |
| loss/alpha                         | -0.0602   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 287000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9892
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.2       |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -654      |
| loss/alpha                         | -0.133    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 288000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9998
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.848    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -654      |
| loss/alpha                         | 0.0413    |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 289000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9906
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.552    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -654      |
| loss/alpha                         | 0.0529    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 290000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9751
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.671   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 291000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.416     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -655      |
| loss/alpha                         | 0.105     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 292000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.602    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.0293   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 293000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 4.9953
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.95     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -655      |
| loss/alpha                         | -0.0157   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 294000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9980
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.103   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.00707 |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 295000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9984
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.38e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.613   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -655     |
| loss/alpha                         | -0.0335  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 296000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.0912   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -656     |
| loss/alpha                         | -0.0235  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 297000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0129
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 1.51     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -656     |
| loss/alpha                         | 0.0156   |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 298000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0028
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.34     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -656     |
| loss/alpha                         | 0.0229   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 299000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9980
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 4.9915
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.68e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.332     |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -656      |
| loss/alpha                         | -0.0828   |
| loss/critic1                       | 15        |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 300000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9853
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.0856   |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.9      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -657      |
| loss/alpha                         | 0.0126    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 301000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 4.9832
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0018
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.378    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -657     |
| loss/alpha                         | 0.073    |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 302000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.56e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.685   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -657     |
| loss/alpha                         | 0.0221   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 303000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.646   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.0128  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 304000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 4.9991
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.661    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -657      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 305000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9765
num rollout transitions: 250000, reward mean: 4.9824
num rollout transitions: 250000, reward mean: 4.9798
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.56      |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.71      |
| loss/actor                         | -657      |
| loss/alpha                         | 0.00103   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 306000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9877
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9878
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.352    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -657     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 307000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0003
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.248    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0342   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 308000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.761    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -658     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 309000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.131    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.8     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -658     |
| loss/alpha                         | 0.0497   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 310000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0037
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.791   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.3     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 311000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9864
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9853
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0264    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -658      |
| loss/alpha                         | 0.0169    |
| loss/critic1                       | 15.3      |
| loss/critic2                       | 15.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 312000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0022
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.932     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0575    |
| loss/critic1                       | 15.4      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 313000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9988
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.0871   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0745   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 314000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9842
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9881
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.332    |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.16     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -660     |
| loss/alpha                         | 0.0572   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 315000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 4.9786
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.967     |
| adv_dynamics_update/all_loss       | -53.3     |
| adv_dynamics_update/sl_loss        | -53.3     |
| alpha                              | 0.16      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.047     |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.98      |
| timestep                           | 316000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 4.9817
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 4.9868
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.215   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68       |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.162   |
| loss/critic1                       | 15.8     |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 317000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9851
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -1.69     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0114    |
| loss/critic1                       | 16.6      |
| loss/critic2                       | 16.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 318000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9897
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9903
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.965    |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.133   |
| loss/critic1                       | 16.5     |
| loss/critic2                       | 16.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 319000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0005
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.5e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.781    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -658     |
| loss/alpha                         | -0.0711  |
| loss/critic1                       | 16       |
| loss/critic2                       | 15.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 320000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 4.9854
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 1.31      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0119   |
| loss/critic1                       | 15.5      |
| loss/critic2                       | 15.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 321000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9787
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9856
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.783    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -658     |
| loss/alpha                         | 0.0354   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 322000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9836
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 1.34     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -658     |
| loss/alpha                         | 0.0682   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 323000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9956
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.03e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.012    |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0221   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 324000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 5.0064
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.29e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.105     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -658      |
| loss/alpha                         | -0.0345   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 325000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9904
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.81    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -658     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 326000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 4.9907
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 4.9865
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.601     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0767    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 327000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9956
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.496   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0374   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 328000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0025
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 4.9895
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0409    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.8      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -659      |
| loss/alpha                         | 0.0231    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 329000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9887
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9899
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.134   |
| adv_dynamics_update/all_loss       | -53.7    |
| adv_dynamics_update/sl_loss        | -53.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0621  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 330000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9975
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.22e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.408    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -659      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 331000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9810
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 5.0019
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.874    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.0821   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 332000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9978
num rollout transitions: 250000, reward mean: 4.9928
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.617   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.3     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -659     |
| loss/alpha                         | 0.00125  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 333000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 4.9916
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.234   |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -659     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 334000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 5.0029
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.404    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -659      |
| loss/alpha                         | -0.007    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 335000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9843
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | -0.454    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.63      |
| loss/actor                         | -660      |
| loss/alpha                         | -0.0549   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 336000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9891
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9938
num rollout transitions: 250000, reward mean: 4.9800
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.588     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.09      |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 337000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.98e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.421   |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 2.66     |
| loss/actor                         | -660     |
| loss/alpha                         | -0.0334  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 338000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.877     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -660      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 339000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.588    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -661     |
| loss/alpha                         | 0.0927   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 340000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0066
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.441   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -661     |
| loss/alpha                         | -0.108   |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 341000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 5.0017
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.471     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 3.58      |
| loss/actor                         | -661      |
| loss/alpha                         | -0.03     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 342000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9819
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9938
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -1.83    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0798   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 343000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9931
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 4.9918
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.63      |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.0504    |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 344000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9858
num rollout transitions: 250000, reward mean: 4.9958
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.2e-09  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -53.9    |
| adv_dynamics_update/sl_loss        | -53.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -662     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 345000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9981
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.128    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -662      |
| loss/alpha                         | 0.031     |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 346000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0078
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.568    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0502  |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 347000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9899
num rollout transitions: 250000, reward mean: 4.9942
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.609    |
| adv_dynamics_update/all_loss       | -53.6    |
| adv_dynamics_update/sl_loss        | -53.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 348000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9959
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.0238   |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.00876  |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 349000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 5.0101
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.096    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0442   |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 350000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9790
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9960
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.223   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -662     |
| loss/alpha                         | -0.0791  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 351000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9901
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0535    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.106     |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 352000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9933
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.146     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.0262    |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 353000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 4.9920
num rollout transitions: 250000, reward mean: 4.9930
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.463    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -663     |
| loss/alpha                         | 0.0254   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 354000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9846
num rollout transitions: 250000, reward mean: 4.9941
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.08     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 355000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9789
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0069
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.522     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.72      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 356000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9950
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.429    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.0604   |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 357000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0100
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.106    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -663      |
| loss/alpha                         | 0.0259    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 358000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9913
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.171     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.1      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -664      |
| loss/alpha                         | 0.013     |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 359000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9807
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9739
num rollout transitions: 250000, reward mean: 4.9953
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -1.55     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0263    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 360000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 4.9955
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.714    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.9      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -663      |
| loss/alpha                         | -0.0187   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 361000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9857
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9968
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-11   |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.208   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.1     |
| eval/normalized_episode_reward_std | 25.7     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0678  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 362000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 4.9822
num rollout transitions: 250000, reward mean: 4.9872
num rollout transitions: 250000, reward mean: 4.9951
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.517   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -663     |
| loss/alpha                         | -0.0178  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 363000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9903
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.615    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -664     |
| loss/alpha                         | -0.027   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 364000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9944
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9980
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.123    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -664      |
| loss/alpha                         | -0.0733   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 365000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.763    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 7.68     |
| loss/actor                         | -664     |
| loss/alpha                         | 0.0413   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 366000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.552    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 24        |
| loss/actor                         | -664      |
| loss/alpha                         | 0.0265    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 367000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 4.9919
num rollout transitions: 250000, reward mean: 4.9985
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0819    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.5      |
| eval/normalized_episode_reward_std | 28.6      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.00951  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 368000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0050
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.98     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0679   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 369000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0002
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.481    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -665      |
| loss/alpha                         | -0.107    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 370000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9853
num rollout transitions: 250000, reward mean: 4.9975
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9963
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.98e-13 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.798    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -665     |
| loss/alpha                         | 0.0739   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 371000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0097
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 2.05      |
| adv_dynamics_update/all_loss       | -53.9     |
| adv_dynamics_update/sl_loss        | -53.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -665      |
| loss/alpha                         | -0.0456   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 372000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9854
num rollout transitions: 250000, reward mean: 4.9921
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.457   |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.123   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 373000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 4.9905
num rollout transitions: 250000, reward mean: 4.9932
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.225    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.4     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 374000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9870
num rollout transitions: 250000, reward mean: 4.9776
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 4.9841
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.691     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0781    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 375000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.966     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -666      |
| loss/alpha                         | 0.0355    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 376000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9928
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9973
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.66e-12 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.00276 |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 377000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9917
num rollout transitions: 250000, reward mean: 4.9933
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.25      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -667      |
| loss/alpha                         | 0.0371    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 378000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9985
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.975    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -666     |
| loss/alpha                         | -0.0273  |
| loss/critic1                       | 14       |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 379000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 4.9926
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 42.4      |
| adv_dynamics_update/adv_loss       | 0.849     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 4.26      |
| loss/actor                         | -667      |
| loss/alpha                         | 0.00775   |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 380000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 5.0003
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.99e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.255   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0251   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 381000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9962
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.259    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0595   |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 382000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 4.9978
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.341    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0188  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 383000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9833
num rollout transitions: 250000, reward mean: 4.9855
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0051
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.1       |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -667      |
| loss/alpha                         | -0.0183   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 384000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 4.9869
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.41     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0671  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 385000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 4.9951
num rollout transitions: 250000, reward mean: 4.9925
num rollout transitions: 250000, reward mean: 5.0039
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.42     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -667     |
| loss/alpha                         | 0.00492  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 386000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9955
num rollout transitions: 250000, reward mean: 4.9913
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.323   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0234   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 387000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0004
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-09 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.29     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -667     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 388000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.149    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -667     |
| loss/alpha                         | -0.0765  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 389000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 4.9916
num rollout transitions: 250000, reward mean: 4.9945
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.712     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.0153    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 390000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 4.9990
num rollout transitions: 250000, reward mean: 4.9879
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.168     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -668      |
| loss/alpha                         | 0.11      |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 391000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9923
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-11  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.33    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.0359   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 392000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9895
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.405   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.119   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 393000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 4.9939
num rollout transitions: 250000, reward mean: 4.9912
num rollout transitions: 250000, reward mean: 4.9929
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.293   |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -668     |
| loss/alpha                         | 0.039    |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 394000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0080
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 1.56     |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.94     |
| loss/actor                         | -668     |
| loss/alpha                         | -0.00439 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 395000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9909
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9974
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.294     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 15.7      |
| loss/actor                         | -668      |
| loss/alpha                         | -0.00776  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 396000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9908
num rollout transitions: 250000, reward mean: 5.0027
num rollout transitions: 250000, reward mean: 4.9962
num rollout transitions: 250000, reward mean: 5.0019
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.00292   |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -668      |
| loss/alpha                         | -0.0168   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 397000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.24e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.516    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0139  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 398000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.2e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.299    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -669     |
| loss/alpha                         | 0.0375   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 399000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 4.9992
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.512    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0529  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 400000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9984
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.295   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0546  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 401000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9951
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.98e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 1.29      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.4      |
| eval/normalized_episode_reward_std | 22.4      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.077     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 402000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 4.9950
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.103     |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.3      |
| eval/normalized_episode_reward_std | 17.8      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.0966    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 403000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0075
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0501    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 10.5      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0361   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 404000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9967
num rollout transitions: 250000, reward mean: 4.9991
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.526   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -669     |
| loss/alpha                         | -0.0401  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 405000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9935
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 4.9901
num rollout transitions: 250000, reward mean: 4.9975
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0362    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -669      |
| loss/alpha                         | -0.0109   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 406000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9849
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0032
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.54e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 407000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 4.9862
num rollout transitions: 250000, reward mean: 4.9813
num rollout transitions: 250000, reward mean: 4.9956
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 42.2      |
| adv_dynamics_update/adv_loss       | 0.825     |
| adv_dynamics_update/all_loss       | -53.7     |
| adv_dynamics_update/sl_loss        | -53.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.105    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 408000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 4.9943
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.33     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 409000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9889
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0012
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.92e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.488     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -669      |
| loss/alpha                         | 0.0225    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 410000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0027
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.281     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 8.11      |
| loss/actor                         | -670      |
| loss/alpha                         | 0.00147   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 411000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0028
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.567    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 412000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0040
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.99e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.594   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -670     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 413000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.53e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.164    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -670     |
| loss/alpha                         | 0.0578   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 414000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0057
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.367    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -670      |
| loss/alpha                         | -0.0611   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 415000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0030
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-12 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.538    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.6     |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0565  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 416000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9918
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9934
num rollout transitions: 250000, reward mean: 4.9978
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.586     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 9.56      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0533    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 417000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9931
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.12     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.4      |
| eval/normalized_episode_reward_std | 17        |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0914    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 418000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9851
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 4.9965
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -1.4      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -671      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 419000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 4.9959
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.616   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 13.5     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0132   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 420000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.77e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.303   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.3     |
| eval/normalized_episode_reward_std | 28.5     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0573  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 421000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9954
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9963
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.113     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 60.2      |
| eval/normalized_episode_reward_std | 19.7      |
| loss/actor                         | -671      |
| loss/alpha                         | 0.0156    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 422000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 4.9983
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 1.28     |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -671     |
| loss/alpha                         | -0.0345  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 423000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0001
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.4      |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -671     |
| loss/alpha                         | 0.0863   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 424000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0020
num rollout transitions: 250000, reward mean: 4.9937
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.54e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.655   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.035    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 425000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.418    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.1     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0277  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 426000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9927
num rollout transitions: 250000, reward mean: 4.9875
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.265    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 427000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9946
num rollout transitions: 250000, reward mean: 4.9963
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.865   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 22.1     |
| loss/actor                         | -672     |
| loss/alpha                         | 0.0328   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 428000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 4.9979
num rollout transitions: 250000, reward mean: 4.9980
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.34      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 13.7      |
| loss/actor                         | -672      |
| loss/alpha                         | -0.0369   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 429000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0002
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.02e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.17     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 5.83     |
| loss/actor                         | -672     |
| loss/alpha                         | -0.0487  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 430000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 4.9930
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0014
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.105     |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 4.21      |
| loss/actor                         | -672      |
| loss/alpha                         | 0.0669    |
| loss/critic1                       | 14.9      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 431000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0008
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0044
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.64e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.638   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.7     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0245   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 432000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 4.9956
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.157     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.3      |
| eval/normalized_episode_reward_std | 18        |
| loss/actor                         | -673      |
| loss/alpha                         | -0.029    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 433000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9966
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.5      |
| eval/normalized_episode_reward_std | 23.4      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.0179   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 434000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0059
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.4e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.612   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 39.4     |
| eval/normalized_episode_reward_std | 30.8     |
| loss/actor                         | -673     |
| loss/alpha                         | -0.0963  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 435000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 4.9964
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.139   |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -673     |
| loss/alpha                         | 0.0978   |
| loss/critic1                       | 14.8     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 436000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 4.9882
num rollout transitions: 250000, reward mean: 5.0018
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.23      |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -673      |
| loss/alpha                         | -0.0483   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 437000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0056
num rollout transitions: 250000, reward mean: 4.9969
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 4.9816
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.149   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.041   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 438000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 4.9968
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.41e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.105    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.4     |
| eval/normalized_episode_reward_std | 25.8     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0133   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 439000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.05    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 440000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 4.9976
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.65     |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -674     |
| loss/alpha                         | -0.00589 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 441000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 4.9912
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.486     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -674      |
| loss/alpha                         | -0.0065   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 442000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 4.9890
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0089
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.423    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -674     |
| loss/alpha                         | 0.00901  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 443000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0075
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.204     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 5.8       |
| loss/actor                         | -674      |
| loss/alpha                         | -0.00971  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 444000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9947
num rollout transitions: 250000, reward mean: 5.0003
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 4.9989
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.389    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -675     |
| loss/alpha                         | 0.0786   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 445000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0102
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.779     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 57.7      |
| eval/normalized_episode_reward_std | 28.3      |
| loss/actor                         | -675      |
| loss/alpha                         | 0.0779    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 446000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0040
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 4.9973
num rollout transitions: 250000, reward mean: 4.9977
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.0161   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.2     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -675     |
| loss/alpha                         | 0.046    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 447000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9860
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9968
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.756     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.9      |
| eval/normalized_episode_reward_std | 6.35      |
| loss/actor                         | -675      |
| loss/alpha                         | -0.0912   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 448000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0083
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 1.16      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 19.2      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0193    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 449000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0000
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0093
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.223    |
| adv_dynamics_update/all_loss       | -54.1     |
| adv_dynamics_update/sl_loss        | -54.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 8.46      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0582    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 450000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9985
num rollout transitions: 250000, reward mean: 5.0045
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.528    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 20.3     |
| loss/actor                         | -676     |
| loss/alpha                         | 0.048    |
| loss/critic1                       | 14.7     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 451000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0042
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.651   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0334  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 452000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9910
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 5.0031
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.691   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 14       |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0827  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 453000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 4.9991
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 4.9995
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.368     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.9      |
| eval/normalized_episode_reward_std | 22.1      |
| loss/actor                         | -676      |
| loss/alpha                         | 0.0201    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 454000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0242
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.542    |
| adv_dynamics_update/all_loss       | -53.8    |
| adv_dynamics_update/sl_loss        | -53.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -676     |
| loss/alpha                         | -0.0759  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 455000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0034
num rollout transitions: 250000, reward mean: 5.0125
num rollout transitions: 250000, reward mean: 5.0035
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.493    |
| adv_dynamics_update/all_loss       | -54       |
| adv_dynamics_update/sl_loss        | -54       |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.8      |
| eval/normalized_episode_reward_std | 21.7      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.00537  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 456000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.624     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.9      |
| eval/normalized_episode_reward_std | 21.7      |
| loss/actor                         | -677      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 457000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.73e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.538    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.5     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -677     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 458000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 4.9924
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.21     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -678     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 459000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 4.9990
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.518     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 54        |
| eval/normalized_episode_reward_std | 21.6      |
| loss/actor                         | -678      |
| loss/alpha                         | -0.0376   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 460000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0055
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.253     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -678      |
| loss/alpha                         | 0.0536    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 461000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9941
num rollout transitions: 250000, reward mean: 5.0063
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.84e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.709    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0962    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 462000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9866
num rollout transitions: 250000, reward mean: 4.9806
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 4.9982
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.0725    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0391   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 463000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0041
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9989
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0634    |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 10.4      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.015    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 464000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0047
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.274     |
| adv_dynamics_update/all_loss       | -54.2     |
| adv_dynamics_update/sl_loss        | -54.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.6      |
| eval/normalized_episode_reward_std | 26.2      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.015    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 465000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 4.9943
num rollout transitions: 250000, reward mean: 5.0092
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.36e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.173    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.5      |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0049   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 466000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9936
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 4.9935
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.455   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0393  |
| loss/critic1                       | 15.3     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 467000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0022
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0061
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.04     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 468000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0089
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.99e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.136     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.00547   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 469000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.96e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.453   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 30.5     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0713  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 470000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0095
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 4.9999
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -1.73     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0243   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 471000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 4.9949
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.98    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -679     |
| loss/alpha                         | 0.049    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 472000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 4.9875
num rollout transitions: 250000, reward mean: 5.0021
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.161     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 12.5      |
| loss/actor                         | -679      |
| loss/alpha                         | -0.0196   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 473000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 4.9988
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.855   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.0665  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 474000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 4.9948
num rollout transitions: 250000, reward mean: 5.0032
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0784    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -679      |
| loss/alpha                         | 0.0414    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 475000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0038
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.104    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.4      |
| eval/normalized_episode_reward_std | 22        |
| loss/actor                         | -679      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 476000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0013
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.375    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -679     |
| loss/alpha                         | -0.00569 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 477000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0098
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.666    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.072    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 478000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0064
num rollout transitions: 250000, reward mean: 4.9981
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 4.9961
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.9e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -680     |
| loss/alpha                         | -0.0636  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 479000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 4.9957
num rollout transitions: 250000, reward mean: 4.9975
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.426   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -680     |
| loss/alpha                         | 0.0483   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 480000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 4.9974
num rollout transitions: 250000, reward mean: 5.0064
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.759    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 481000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.0207  |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0704  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 482000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0053
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 4.9861
num rollout transitions: 250000, reward mean: 4.9888
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.374    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 19.8      |
| loss/actor                         | -681      |
| loss/alpha                         | -0.0163   |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 483000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 4.9926
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -681     |
| loss/alpha                         | 0.00758  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 484000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9963
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0041
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.36e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -1.02    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -681     |
| loss/alpha                         | -0.0514  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 485000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.0134    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.00913  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 486000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9945
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 4.9977
num rollout transitions: 250000, reward mean: 4.9975
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.12      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.9      |
| eval/normalized_episode_reward_std | 15.8      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.12      |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 487000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 5.0005
num rollout transitions: 250000, reward mean: 5.0057
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.7e-10  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.108    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 15.5     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0797  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 488000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9949
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.142    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52.5      |
| eval/normalized_episode_reward_std | 29.1      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0212    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 489000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 4.9945
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.303    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 12.5     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0805   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 490000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 4.9965
num rollout transitions: 250000, reward mean: 4.9898
num rollout transitions: 250000, reward mean: 4.9954
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.279    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.4     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0584   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 491000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0013
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.132    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0429  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 492000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0049
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.5     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 27.3     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 493000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 4.9953
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.781    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 4.56     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0918  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 494000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0062
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-10  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.541   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0361  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 495000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0023
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0044
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.87e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.897    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0486    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 496000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9953
num rollout transitions: 250000, reward mean: 4.9904
num rollout transitions: 250000, reward mean: 4.9837
num rollout transitions: 250000, reward mean: 4.9848
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.94e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.218    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 8.23      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0725    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 4.99      |
| timestep                           | 497000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.787   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 4.85     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0275   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 498000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0118
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 6.74      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0789    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 499000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9902
num rollout transitions: 250000, reward mean: 4.9799
num rollout transitions: 250000, reward mean: 4.9876
num rollout transitions: 250000, reward mean: 4.9881
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | 0.369    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0064  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 4.99     |
| timestep                           | 500000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 4.9982
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0062
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.451    |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0479   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 501000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.241   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -682     |
| loss/alpha                         | 0.0216   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 502000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0052
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.416   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 503000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.893    |
| adv_dynamics_update/all_loss       | -54      |
| adv_dynamics_update/sl_loss        | -54      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 504000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.697    |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -682     |
| loss/alpha                         | 0.00291  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 505000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 4.9957
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.2e-10  |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | 1.64     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -682     |
| loss/alpha                         | -0.0457  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 506000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0061
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.548     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0462    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 507000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.976     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -682      |
| loss/alpha                         | 0.0502    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 508000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 4.9999
num rollout transitions: 250000, reward mean: 4.9997
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.88      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.5      |
| eval/normalized_episode_reward_std | 22.5      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0378   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 509000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.685    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -682      |
| loss/alpha                         | -0.0619   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 510000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-10    |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 0.426    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0254   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 511000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.446    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0443   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 512000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0026
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0011
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.7e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.623    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.1     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0904  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 513000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9932
num rollout transitions: 250000, reward mean: 4.9900
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0122
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.0162   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 32.8     |
| eval/normalized_episode_reward_std | 26.4     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0572   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 514000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0075
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.29e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.111    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 49.4      |
| eval/normalized_episode_reward_std | 29.3      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.000311 |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 515000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9959
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.64e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -1.73    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 21.9     |
| loss/actor                         | -683     |
| loss/alpha                         | 0.0993   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 516000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0157
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.08e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -1.29     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.224    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 517000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.89e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.368    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 20.2      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0021   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 518000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9940
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0007
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.154    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.2     |
| eval/normalized_episode_reward_std | 27       |
| loss/actor                         | -684     |
| loss/alpha                         | 0.132    |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 519000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9942
num rollout transitions: 250000, reward mean: 4.9987
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.564     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 63.8      |
| eval/normalized_episode_reward_std | 21.8      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0474   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 520000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0075
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.42e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.699    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.8     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0314  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 521000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0049
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.213   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.8     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0998   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 522000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.5e-11  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.852   |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -683     |
| loss/alpha                         | -0.00792 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 523000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 4.9992
num rollout transitions: 250000, reward mean: 4.9944
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.278    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.8     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0284  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 524000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0058
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-13 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.314    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 13.2      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.082    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 525000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.78e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58.9      |
| eval/normalized_episode_reward_std | 28.5      |
| loss/actor                         | -683      |
| loss/alpha                         | -0.0536   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 526000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.42e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.47      |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 18.4      |
| loss/actor                         | -683      |
| loss/alpha                         | 0.0931    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 527000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0095
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.339    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 10       |
| loss/actor                         | -683     |
| loss/alpha                         | -0.0461  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 528000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9972
num rollout transitions: 250000, reward mean: 4.9915
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.0378  |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 24.2     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0338   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 529000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0001
num rollout transitions: 250000, reward mean: 5.0105
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.17     |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -684     |
| loss/alpha                         | -0.0336  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 530000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0208
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | -0.571    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 52.2      |
| eval/normalized_episode_reward_std | 27.5      |
| loss/actor                         | -684      |
| loss/alpha                         | -0.0587   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 531000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 4.9960
num rollout transitions: 250000, reward mean: 5.0162
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.303     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.8      |
| eval/normalized_episode_reward_std | 27.9      |
| loss/actor                         | -684      |
| loss/alpha                         | 0.0757    |
| loss/critic1                       | 12        |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 532000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.133    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 55.6     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -684     |
| loss/alpha                         | 0.0522   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 533000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 4.9970
num rollout transitions: 250000, reward mean: 4.9893
num rollout transitions: 250000, reward mean: 5.0109
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.14e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.704     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.6      |
| eval/normalized_episode_reward_std | 24.3      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0228   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 534000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0158
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.724    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -685     |
| loss/alpha                         | -0.0576  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 535000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0137
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.61      |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66        |
| eval/normalized_episode_reward_std | 24.6      |
| loss/actor                         | -685      |
| loss/alpha                         | -0.0164   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 536000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0313
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.198     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -685      |
| loss/alpha                         | 0.0954    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 537000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.443   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 538000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0020
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.681    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.7     |
| eval/normalized_episode_reward_std | 22.9     |
| loss/actor                         | -686     |
| loss/alpha                         | -0.0483  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 539000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.712     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.1      |
| eval/normalized_episode_reward_std | 21.8      |
| loss/actor                         | -686      |
| loss/alpha                         | 0.0221    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 540000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.706   |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 3.96     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.0976   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 541000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0250
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.76e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.89     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -686      |
| loss/alpha                         | -0.00948  |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 542000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.4     |
| eval/normalized_episode_reward_std | 19.4     |
| loss/actor                         | -686     |
| loss/alpha                         | 0.103    |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 543000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0146
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.451    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 5.43      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0367   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 544000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0171
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.444    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.83      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0355   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 545000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9998
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.651    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -687     |
| loss/alpha                         | 0.0393   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 546000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 1.31     |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0166  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 547000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0414    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.4      |
| eval/normalized_episode_reward_std | 18.2      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0557   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 548000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0086
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.579    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62       |
| eval/normalized_episode_reward_std | 18       |
| loss/actor                         | -688     |
| loss/alpha                         | 0.0331   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 549000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9929
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0114
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.18e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.695     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 3.94      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0538    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 550000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0105
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0164   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 551000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.22e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.637     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 10.9      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0684   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 552000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0067
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.0921    |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 4.12      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0256   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 553000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.277    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.3     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0315  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 554000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.0354  |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.8     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0118  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 555000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0083
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.123   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.1     |
| eval/normalized_episode_reward_std | 29.4     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0469   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 556000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0069
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0165
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.0641   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0467   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 557000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.97e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.767    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.4     |
| eval/normalized_episode_reward_std | 17.3     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0774   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 558000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0054
num rollout transitions: 250000, reward mean: 5.0055
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.9e-10  |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.876   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.9     |
| eval/normalized_episode_reward_std | 22.2     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.058   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 559000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 560000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 4.9956
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.198    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 4.9      |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0285   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 561000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0030
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.299   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.6     |
| eval/normalized_episode_reward_std | 21       |
| loss/actor                         | -689     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 562000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.61e-11 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | -0.462    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0143   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 563000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0087
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.247     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0371    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 564000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0046
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.599     |
| adv_dynamics_update/all_loss       | -54.3     |
| adv_dynamics_update/sl_loss        | -54.3     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.5      |
| eval/normalized_episode_reward_std | 15.8      |
| loss/actor                         | -688      |
| loss/alpha                         | -0.0722   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 565000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0186
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.462    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.4      |
| eval/normalized_episode_reward_std | 22.5      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0328   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 566000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0070
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.842   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.3     |
| eval/normalized_episode_reward_std | 23.8     |
| loss/actor                         | -687     |
| loss/alpha                         | -0.0498  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 567000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0019
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0066
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.384    |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -687      |
| loss/alpha                         | -0.0388   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 568000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0144
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.23     |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.6     |
| eval/normalized_episode_reward_std | 27.5     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0144  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 569000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 1.36     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63       |
| eval/normalized_episode_reward_std | 23.5     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 570000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0002
num rollout transitions: 250000, reward mean: 4.9987
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.398     |
| adv_dynamics_update/all_loss       | -54.4     |
| adv_dynamics_update/sl_loss        | -54.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.7      |
| eval/normalized_episode_reward_std | 13        |
| loss/actor                         | -688      |
| loss/alpha                         | -0.00813  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 571000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9997
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0133
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.765    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 12.3     |
| loss/actor                         | -688     |
| loss/alpha                         | 0.115    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 572000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9986
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.602     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 15        |
| loss/actor                         | -688      |
| loss/alpha                         | 0.102     |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 573000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9995
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0096
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.57e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.388    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 6.34     |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0473  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 574000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.909    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 61.7     |
| eval/normalized_episode_reward_std | 26       |
| loss/actor                         | -688     |
| loss/alpha                         | 0.045    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 575000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.226    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.2     |
| eval/normalized_episode_reward_std | 23       |
| loss/actor                         | -688     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 576000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.139     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.2      |
| eval/normalized_episode_reward_std | 6.58      |
| loss/actor                         | -688      |
| loss/alpha                         | 0.0137    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 577000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0017
num rollout transitions: 250000, reward mean: 4.9993
num rollout transitions: 250000, reward mean: 4.9933
num rollout transitions: 250000, reward mean: 5.0085
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.75e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.509    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 63.9     |
| eval/normalized_episode_reward_std | 21.2     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 578000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.8e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | -0.0869  |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 50.3     |
| eval/normalized_episode_reward_std | 31.8     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.00454  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 579000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0042
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.0278  |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.00813 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 580000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0309
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.4e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.114    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 53.1     |
| eval/normalized_episode_reward_std | 29.5     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0582   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 581000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0063
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | -0.0802  |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 57.9     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0284   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 582000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.66      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.48      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0756   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 583000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -1.07    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -689     |
| loss/alpha                         | 0.0516   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 584000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.59e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.879   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.89     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.023   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 585000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0074
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0515    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65        |
| eval/normalized_episode_reward_std | 23.7      |
| loss/actor                         | -689      |
| loss/alpha                         | -0.0509   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 586000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -1.86    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.5     |
| eval/normalized_episode_reward_std | 16.4     |
| loss/actor                         | -689     |
| loss/alpha                         | -0.0414  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 587000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0031
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.42    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 13       |
| loss/actor                         | -690     |
| loss/alpha                         | -0.0169  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 588000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -1.18    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0295   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 589000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0042
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.19e-12 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0954    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -690      |
| loss/alpha                         | 0.00816   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 590000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.711    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66       |
| eval/normalized_episode_reward_std | 26.2     |
| loss/actor                         | -690     |
| loss/alpha                         | 0.0165   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 591000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0106
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.47e-11 |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | -0.604   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0461   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 592000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0132
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0428   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 593000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0011
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.0247   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0245   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 594000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0085
num rollout transitions: 250000, reward mean: 5.0004
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.604   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0613  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 595000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 4.9952
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0043
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -1.21    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0576   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5        |
| timestep                           | 596000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0349
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.447    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0296   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 597000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.8e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.467   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.5     |
| eval/normalized_episode_reward_std | 27.8     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 598000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -1.1     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 62.9     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.00277  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 599000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0144
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.23e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6      |
| adv_dynamics_update/adv_loss       | 0.728     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.2      |
| eval/normalized_episode_reward_std | 19        |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0692   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 600000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0062
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0159
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.00222   |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.00741  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 601000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.34e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.609     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 24        |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0101    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 602000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9958
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.295    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -691     |
| loss/alpha                         | 0.0311   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 603000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.318     |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.3      |
| eval/normalized_episode_reward_std | 19.1      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0262    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 604000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.462     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 5.99      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0488    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 605000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0113
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.15    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.1     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0395  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 606000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.687   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 12       |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0586  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 607000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-12 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.686     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 8.48      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0103    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 608000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.154     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 10.2      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.0506   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 609000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0039
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.995    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0309    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 610000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0015
num rollout transitions: 250000, reward mean: 5.0070
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.701     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 19.1      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.032     |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5         |
| timestep                           | 611000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0118
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.0688   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0714  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 612000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0088
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-11 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.368   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.5     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0818  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 613000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0045
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0080
num rollout transitions: 250000, reward mean: 5.0065
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.894     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.6      |
| eval/normalized_episode_reward_std | 10.6      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0464    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 614000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.234    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 15.7      |
| loss/actor                         | -691      |
| loss/alpha                         | 0.0707    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 615000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.29e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.553     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.00121  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 616000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0230
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.59e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6      |
| adv_dynamics_update/adv_loss       | -0.467    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -691      |
| loss/alpha                         | -0.066    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 617000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.6e-11  |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.885    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -691     |
| loss/alpha                         | -0.0865  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 618000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0116
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.388     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.11      |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 619000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0060
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.172    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0502    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 620000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0054
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -1.16     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 14.1      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 621000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.98e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.557   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 5.51     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.00288 |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 622000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0006
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.578     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 18        |
| loss/actor                         | -692      |
| loss/alpha                         | -0.00669  |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 623000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0137
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -1.02    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 624000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 4.9989
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.316     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -692      |
| loss/alpha                         | -0.0116   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 625000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0074
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.5       |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -692      |
| loss/alpha                         | 0.0255    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 626000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0102
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 42.3     |
| adv_dynamics_update/adv_loss       | -0.13    |
| adv_dynamics_update/all_loss       | -54.4    |
| adv_dynamics_update/sl_loss        | -54.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -692     |
| loss/alpha                         | -0.0537  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 627000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0223
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.48e-11 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.0332   |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.1      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -693      |
| loss/alpha                         | 0.103     |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 628000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 1.09     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -693     |
| loss/alpha                         | 0.0252   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 629000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.17e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.0335  |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0191  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 630000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0338
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.656    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 59.6     |
| eval/normalized_episode_reward_std | 24.8     |
| loss/actor                         | -693     |
| loss/alpha                         | -0.0733  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 631000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.2       |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 21.6      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0145   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 632000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0189
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.23      |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 61.9      |
| eval/normalized_episode_reward_std | 23        |
| loss/actor                         | -694      |
| loss/alpha                         | 0.0577    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 633000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0010
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 42.5     |
| adv_dynamics_update/adv_loss       | 0.141    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0149  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 634000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.5e-10  |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.243    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.00938  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 635000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0078
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.493     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0313    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 636000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.6e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.291    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 4.2      |
| loss/actor                         | -695     |
| loss/alpha                         | -0.057   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 637000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.496    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.7     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.028   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 638000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9996
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0038
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-11  |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.4     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.41     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 639000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0024
num rollout transitions: 250000, reward mean: 5.0187
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.254     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -695      |
| loss/alpha                         | 0.0594    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 640000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.402    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.3      |
| eval/normalized_episode_reward_std | 13.2      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0643   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 641000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0102
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.0848    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -694      |
| loss/alpha                         | -0.0306   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 642000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0156
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.868    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -694     |
| loss/alpha                         | 0.0699   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 643000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0198
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.396     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -695      |
| loss/alpha                         | -0.0143   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 644000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.223    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0291  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 645000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0327
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.558    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.00446  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 646000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0232
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.626    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -695     |
| loss/alpha                         | -0.0303  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 647000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.744    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.00255  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 648000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.267   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 9.11     |
| loss/actor                         | -695     |
| loss/alpha                         | 0.0442   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 649000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-11 |
| adv_dynamics_update/adv_log_prob   | 42.2     |
| adv_dynamics_update/adv_loss       | 0.149    |
| adv_dynamics_update/all_loss       | -54.2    |
| adv_dynamics_update/sl_loss        | -54.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0106   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 650000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0065
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0200
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.19e-12 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.17      |
| adv_dynamics_update/all_loss       | -54.5     |
| adv_dynamics_update/sl_loss        | -54.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.00674  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 651000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0153
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | -0.595    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -696      |
| loss/alpha                         | 0.00681   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 652000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0043
num rollout transitions: 250000, reward mean: 5.0066
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.215   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.4     |
| eval/normalized_episode_reward_std | 23.3     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.12     |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 653000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0115
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.1e-10  |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -1.35    |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 9.05     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0484  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 654000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0016
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.15e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.113    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.125   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 655000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0072
num rollout transitions: 250000, reward mean: 5.0036
num rollout transitions: 250000, reward mean: 5.0068
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0102    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.00835  |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 656000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0215
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.371    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.00455   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 657000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 4.9971
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.79    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0589   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 658000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.842    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.53     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0286  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 659000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.87e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.114   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0322   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 660000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0149
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 21.1     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0354   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 661000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.15e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | 0.578    |
| adv_dynamics_update/all_loss       | -54.3    |
| adv_dynamics_update/sl_loss        | -54.3    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 4.17     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0896  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 662000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.56e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.0898  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.053   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 663000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0170
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.756   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.00329 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 664000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0200
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -1.1      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 3.65      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0375    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 665000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.53e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.138    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.0477    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 666000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0082
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.68     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.4     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.00478  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 667000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 4.9961
num rollout transitions: 250000, reward mean: 5.0071
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.0299  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 15.9     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0274   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 668000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.381   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 5.51     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.0458  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 669000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.75     |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -696     |
| loss/alpha                         | -0.088   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 670000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0038
num rollout transitions: 250000, reward mean: 5.0081
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.00104   |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -696      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 671000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0050
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.89e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.413     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -696      |
| loss/alpha                         | -0.0289   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 672000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-11 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.0629   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.063    |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 673000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0018
num rollout transitions: 250000, reward mean: 5.0120
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.7e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.55    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.3     |
| eval/normalized_episode_reward_std | 16.2     |
| loss/actor                         | -696     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 674000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 1.83      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 13.3      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0532   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 675000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0097
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.544     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.44      |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0176    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 676000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0012
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0235
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.51e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | -1.09     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.8      |
| eval/normalized_episode_reward_std | 15.2      |
| loss/actor                         | -697      |
| loss/alpha                         | -0.0474   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 677000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0184
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.113    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.1      |
| eval/normalized_episode_reward_std | 23        |
| loss/actor                         | -697      |
| loss/alpha                         | 0.0727    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 678000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0100
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 4.9994
num rollout transitions: 250000, reward mean: 5.0035
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.016   |
| adv_dynamics_update/all_loss       | -54.5    |
| adv_dynamics_update/sl_loss        | -54.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -697     |
| loss/alpha                         | -0.0262  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 679000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.77e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.404    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.84     |
| loss/actor                         | -697     |
| loss/alpha                         | 0.03     |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 680000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0114
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.4e-10  |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.283    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -698     |
| loss/alpha                         | -0.0429  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 681000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0326
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.49e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.356    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.53      |
| loss/actor                         | -698      |
| loss/alpha                         | -0.0135   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 682000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.103    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -698      |
| loss/alpha                         | -0.0353   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 683000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.796   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -698     |
| loss/alpha                         | -0.016   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 684000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0176
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.42     |
| adv_dynamics_update/all_loss       | -54.1    |
| adv_dynamics_update/sl_loss        | -54.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 9.72     |
| loss/actor                         | -698     |
| loss/alpha                         | -0.0862  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 685000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0097
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.77     |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -698     |
| loss/alpha                         | 0.115    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 686000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0111
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.277   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -698     |
| loss/alpha                         | 0.0763   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 687000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0092
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.481     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -698      |
| loss/alpha                         | -0.00624  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 688000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.733     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.00658  |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 689000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0241
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.284    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 6.54     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.036    |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 690000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0112
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.12e-12 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.868    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0807  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 691000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0360
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 42.3      |
| adv_dynamics_update/adv_loss       | 0.619     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -699      |
| loss/alpha                         | 0.00852   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 692000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0230
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.45e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.0624    |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.6      |
| eval/normalized_episode_reward_std | 24.7      |
| loss/actor                         | -699      |
| loss/alpha                         | 0.0686    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 693000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0131
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.995    |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.6     |
| eval/normalized_episode_reward_std | 8.16     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.00919 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 694000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0410
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.03e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.455     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 6.94      |
| loss/actor                         | -699      |
| loss/alpha                         | -0.0237   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 695000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.0194  |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 8.93     |
| loss/actor                         | -699     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 696000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.422   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0403  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 697000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0081
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.344    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -699     |
| loss/alpha                         | -0.0672  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 698000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0112
num rollout transitions: 250000, reward mean: 5.0083
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.8e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.284    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 16.3     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0173  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 699000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0443
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.393   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.3     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -700     |
| loss/alpha                         | -0.0205  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 700000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.326     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.93      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.0485    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 701000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-11 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | 0.741     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 16.5      |
| loss/actor                         | -700      |
| loss/alpha                         | 0.008     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 702000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0179
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.457     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 9.03      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0239   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 703000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0161
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.464    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0971   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 704000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | -0.346   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.00396  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 705000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.38     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0181   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 706000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.562     |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.5      |
| eval/normalized_episode_reward_std | 21.3      |
| loss/actor                         | -701      |
| loss/alpha                         | -0.0113   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 707000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.78e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.408   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -701     |
| loss/alpha                         | 0.0771   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 708000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0084
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.432    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.066   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 709000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0376
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.23e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0853    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 58        |
| eval/normalized_episode_reward_std | 29.9      |
| loss/actor                         | -702      |
| loss/alpha                         | 0.127     |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 710000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0224
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 0.266    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -702     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 711000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.597    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.00737 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 712000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.503     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0103    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 713000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0006
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0101
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.98e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.0162    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.3      |
| eval/normalized_episode_reward_std | 23.7      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0441   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 714000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0197
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.38e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.00961   |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 8.27      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0276    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 715000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.41e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.156   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0964  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 716000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.239     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -704      |
| loss/alpha                         | 0.0424    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 717000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.0434   |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -704      |
| loss/alpha                         | 0.0503    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 718000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0288
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.156     |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -704      |
| loss/alpha                         | -0.0144   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 719000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0057
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 4.9999
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1e-11    |
| adv_dynamics_update/adv_log_prob   | 42.5     |
| adv_dynamics_update/adv_loss       | -0.0818  |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.0387   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 720000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0134
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.89e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.477    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0973  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 721000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.529   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.00761  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 722000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0110
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0193
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.52e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.79     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0452    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 723000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.179   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -703     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 724000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | -0.632    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0252   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 725000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0311
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.7e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.148    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 9.8      |
| loss/actor                         | -703     |
| loss/alpha                         | 0.113    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 726000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0147
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.87e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.401     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0204    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 727000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.836    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.142   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 728000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.7e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.162    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0519  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 729000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-09 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.678    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.00262 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 730000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.482   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 5.58     |
| loss/actor                         | -702     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 731000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0022
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.906   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -703     |
| loss/alpha                         | -0.0306  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 732000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.1e-11  |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.808    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -703     |
| loss/alpha                         | 0.0542   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 733000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0095
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.67e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.257     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.8      |
| eval/normalized_episode_reward_std | 24.6      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0263   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 734000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.26      |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0881   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 735000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0058
num rollout transitions: 250000, reward mean: 5.0113
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.539     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.0726    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 736000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.0638   |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0233   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 737000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.24e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.717    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70        |
| eval/normalized_episode_reward_std | 24.5      |
| loss/actor                         | -703      |
| loss/alpha                         | 0.115     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 738000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0162    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 10.2      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.0857   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 739000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0201
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.54e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.571     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -703      |
| loss/alpha                         | -0.00224  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 740000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.801    |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.00857  |
| loss/critic1                       | 14.3     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 741000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0126
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.514   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 7.27     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.022    |
| loss/critic1                       | 14.5     |
| loss/critic2                       | 14.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 742000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.647     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -704      |
| loss/alpha                         | -0.042    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 743000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.239    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.00934  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 744000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.42e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.663    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.0898   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 745000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.11    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -704     |
| loss/alpha                         | 0.0628   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 746000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0193
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.792    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -704      |
| loss/alpha                         | -0.045    |
| loss/critic1                       | 14        |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 747000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.523   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 17.4     |
| loss/actor                         | -704     |
| loss/alpha                         | -0.00495 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 748000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.28e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.291   |
| adv_dynamics_update/all_loss       | -54.7    |
| adv_dynamics_update/sl_loss        | -54.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.00655 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 749000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0155
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 1.02     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -705     |
| loss/alpha                         | -0.0196  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 750000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0235
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.0811  |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -705     |
| loss/alpha                         | 0.0324   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 751000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0336
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.228    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0627    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 752000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0321
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.237     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -705      |
| loss/alpha                         | 0.00889   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 753000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.987     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0658   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 754000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0232
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.326     |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 21.2      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00984   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 755000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0104
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.206   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0448  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 756000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.274     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0219    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 757000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0376
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.03e-11 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 5.99     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0448  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 758000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0094
num rollout transitions: 250000, reward mean: 5.0134
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.374     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 9.78      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0392    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 759000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.53e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.086   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0736  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 760000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0235
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.415    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 13.3      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.00188  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 761000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 0.737    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 7.89     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.119    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 762000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0099
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0271
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.441    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0105   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 763000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0310
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.954    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0138    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 764000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0169
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.141     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.5      |
| eval/normalized_episode_reward_std | 27.1      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0749   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 765000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.0328   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.6     |
| eval/normalized_episode_reward_std | 19.3     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0455   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 766000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0348
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.75e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.447    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0569   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 767000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0358
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.246    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 4.11     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0337   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 768000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.0686   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0156  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 769000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0329
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 7.19      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00979   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 770000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 42.3     |
| adv_dynamics_update/adv_loss       | -0.593   |
| adv_dynamics_update/all_loss       | -54.6    |
| adv_dynamics_update/sl_loss        | -54.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0534   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 771000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.142     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0597   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 772000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0291
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.512    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0351   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 773000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0142
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.144   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0332   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 774000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.052   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 775000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0119
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.981     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -706      |
| loss/alpha                         | 0.00938   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 776000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.0771   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 4.8      |
| loss/actor                         | -706     |
| loss/alpha                         | -0.0414  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 777000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.23e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 1.16     |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0418   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 778000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.456    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.0542   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 779000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0014
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.285     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.3      |
| eval/normalized_episode_reward_std | 8.21      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0628    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 780000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.195    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.7     |
| eval/normalized_episode_reward_std | 24       |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0421   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 781000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.156   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -706     |
| loss/alpha                         | 0.0471   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 782000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.23    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 8.27     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.00809 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 783000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0026
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.319   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 17.7     |
| loss/actor                         | -706     |
| loss/alpha                         | -0.102   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 784000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.09e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.473     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -706      |
| loss/alpha                         | 0.0237    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 785000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0258
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.931    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -706      |
| loss/alpha                         | -0.017    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 786000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0234
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.512     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.62      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.0315    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 787000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0228
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | -0.376    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -707      |
| loss/alpha                         | -0.0223   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 788000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0446
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.933   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -707     |
| loss/alpha                         | 0.0202   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 789000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0304
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | 0.506     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -707      |
| loss/alpha                         | 0.00374   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 790000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.745    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0394   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 791000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.449    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.038    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 792000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0232
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.286    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 793000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.627    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0869   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 794000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.895    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.0352  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 795000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-09 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.788    |
| adv_dynamics_update/all_loss       | -54.6     |
| adv_dynamics_update/sl_loss        | -54.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.6       |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0185    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 796000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.0944    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0113    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 797000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.418    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0371   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 798000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0323
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.87e-11 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | -0.0458   |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.131    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 799000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0044
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0273
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.116    |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -708      |
| loss/alpha                         | 0.0361    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 800000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0121
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.867   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.101    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 801000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.369     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0197   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 802000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.47e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.49    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -708     |
| loss/alpha                         | -0.041   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 803000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0157
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.318    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 7.82     |
| loss/actor                         | -708     |
| loss/alpha                         | 0.0683   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 804000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0148
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-09 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.304    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -708      |
| loss/alpha                         | -0.0449   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 805000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0448
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.1e-10  |
| adv_dynamics_update/adv_log_prob   | 42.7     |
| adv_dynamics_update/adv_loss       | 0.256    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.00767  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 806000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 1.01      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.8      |
| eval/normalized_episode_reward_std | 23.6      |
| loss/actor                         | -709      |
| loss/alpha                         | -0.0203   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 807000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.185   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0853  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 808000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.237    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -709      |
| loss/alpha                         | 0.0385    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 809000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.247    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0577  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 810000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.372   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.073    |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 811000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0205
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.91e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.575    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0404  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 812000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0128
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.56e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.436    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -709      |
| loss/alpha                         | 0.000313  |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 813000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 42.6     |
| adv_dynamics_update/adv_loss       | -0.0378  |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 6.98     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0101  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 814000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.814    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.2      |
| eval/normalized_episode_reward_std | 19        |
| loss/actor                         | -709      |
| loss/alpha                         | -0.00697  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 815000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0009
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0099
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.54e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.17     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.34     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 816000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0009
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 1.47      |
| adv_dynamics_update/all_loss       | -54.7     |
| adv_dynamics_update/sl_loss        | -54.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -710      |
| loss/alpha                         | 0.00578   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 817000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.441   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0469  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 818000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.39    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 13.7     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.146    |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 819000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0318
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | -0.167   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 820000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0266
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.398     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -710      |
| loss/alpha                         | 0.0378    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 821000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.188     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -710      |
| loss/alpha                         | 0.0561    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 822000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-11  |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.0147  |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0503  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 823000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0447
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.27     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 8.12     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 824000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.644    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0475  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 825000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0121
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0113
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.466    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.2     |
| eval/normalized_episode_reward_std | 24.4     |
| loss/actor                         | -710     |
| loss/alpha                         | 0.0404   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 826000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.509    |
| adv_dynamics_update/all_loss       | -54.8     |
| adv_dynamics_update/sl_loss        | -54.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -710      |
| loss/alpha                         | -0.0463   |
| loss/critic1                       | 16.5      |
| loss/critic2                       | 16.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 827000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0305
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.669    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0669  |
| loss/critic1                       | 16.3     |
| loss/critic2                       | 16.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 828000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.535   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0399   |
| loss/critic1                       | 14.9     |
| loss/critic2                       | 15       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 829000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.92e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.745    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.00306  |
| loss/critic1                       | 15.6     |
| loss/critic2                       | 15.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 830000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0307
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.839     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -709      |
| loss/alpha                         | -0.0303   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 831000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0145
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.117    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0535  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 832000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0416
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.602    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.9     |
| eval/normalized_episode_reward_std | 19.8     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0112   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 833000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0074
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.0141  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0765   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 834000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0179
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-11 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.749    |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0847   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 835000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.274   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0359  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 836000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.673    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.00256  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 837000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.2e-10  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.302    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.9     |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -709     |
| loss/alpha                         | -0.0609  |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 838000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0178
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.0674   |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.55      |
| loss/actor                         | -709      |
| loss/alpha                         | -0.0384   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 839000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.508    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -709     |
| loss/alpha                         | 0.0257   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 840000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.0352    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -710      |
| loss/alpha                         | 0.0428    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 841000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0138
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.375     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -710      |
| loss/alpha                         | -0.0477   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 842000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0238
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.0928   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -710      |
| loss/alpha                         | 0.121     |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 843000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.444     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -710      |
| loss/alpha                         | -0.000323 |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 844000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.507    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 5.44     |
| loss/actor                         | -710     |
| loss/alpha                         | -0.0394  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 845000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0206
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.232   |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0425   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 846000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.24e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | 1.09      |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 10.1      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.014    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 847000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.43e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.387   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.98     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0455  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 848000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0146  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 849000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0251
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.21      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 4.05      |
| loss/actor                         | -711      |
| loss/alpha                         | 0.107     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 850000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0270
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.94e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.57    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0166   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 851000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.19    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.1     |
| eval/normalized_episode_reward_std | 21.6     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0192   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 852000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.556     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0204   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 853000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0241
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.54e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.964     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.047    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 854000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.522     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.41      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.00727  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 855000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0332
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.33e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.549   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0761  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 856000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0319
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.56e-12 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.275     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 5.4       |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0455    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 857000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.584    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.66     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 858000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0143
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 1.11     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -711     |
| loss/alpha                         | -0.0447  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 859000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.406   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0597   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 860000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.249    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -711      |
| loss/alpha                         | 0.0146    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 861000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0151
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.245    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0323   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 862000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.277     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.0211   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 863000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.47e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.391    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -711     |
| loss/alpha                         | 0.0544   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 864000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.2e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.866     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -711      |
| loss/alpha                         | -0.000118 |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 865000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.593     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0921   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 866000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0082
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0334   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 867000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.113    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0531  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 868000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0099
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.472     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.105     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 869000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0244
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.516    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 4.07     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 870000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0231
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.0249    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0352    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 871000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.241     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0313    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 872000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0268
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.239    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0298   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 873000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.08e-11 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.0168    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.033    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 874000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.9e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.0557   |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 6.5      |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0517  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 875000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.65      |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0254    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 876000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.66e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.981    |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0111   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 877000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.491     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.5      |
| eval/normalized_episode_reward_std | 7.66      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0471   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 878000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.87e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.287    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 12.7     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 879000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0090
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.85e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.791    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.63      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.022     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 880000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0211
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.415     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0269    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 881000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.38e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.431    |
| adv_dynamics_update/all_loss       | -54.8    |
| adv_dynamics_update/sl_loss        | -54.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -712     |
| loss/alpha                         | 0.0219   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 882000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.8e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.125    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0513  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 883000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.55e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.188   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00427 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 884000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0390
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.13e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.707     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.0321    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 885000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0401
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.476    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.00778 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 886000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0331
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.335    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -712     |
| loss/alpha                         | -0.0621  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 887000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.215    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -712      |
| loss/alpha                         | -0.0388   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 888000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0159
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.355    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -712      |
| loss/alpha                         | 0.123     |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 889000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.98e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.0565  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0259  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 890000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 5.69e-05  |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0421    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 891000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0235
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0451    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.00628   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 892000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0015
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.209   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.2     |
| eval/normalized_episode_reward_std | 20.6     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.01     |
| timestep                           | 893000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0175
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.0774   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0211   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 894000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.97e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.767    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 9.43      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.036    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 895000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | -0.0628  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0773  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 896000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0380
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.409    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00735  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 897000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.303    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0131    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 898000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.0295   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0672    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 899000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.41e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.123    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 20.4      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0525    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 900000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.8e-10  |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | -0.809   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 9.02     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.082   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 901000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.142    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 60.9     |
| eval/normalized_episode_reward_std | 27.6     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0381   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 902000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.46e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.128    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 2.75     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0456   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 903000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.2e-10  |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.411    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 20.7     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0225   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 904000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0173
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 42.7      |
| adv_dynamics_update/adv_loss       | 0.0142    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.7      |
| eval/normalized_episode_reward_std | 13.6      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 905000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0448
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.33e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.476     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0719   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 906000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0214
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.249   |
| adv_dynamics_update/all_loss       | -54.9    |
| adv_dynamics_update/sl_loss        | -54.9    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0669  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 907000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -1.27    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.5     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0629   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 908000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0153
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.31e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.366   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0357  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 909000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.248    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 4.02     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.00262 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 910000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 1.22     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0481   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 911000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.598     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 17        |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0152   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 912000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0125
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.68e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.571     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.000664  |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 913000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0103
num rollout transitions: 250000, reward mean: 5.0215
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 1.06      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.2      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0535   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 914000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.4e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.127    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0507  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 915000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0056
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.476    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 916000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.0746  |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0445   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 917000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0402
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.523    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.063   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 918000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.173   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 919000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.721   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00962  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 920000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0164
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.352     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71        |
| eval/normalized_episode_reward_std | 17.9      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0268   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 921000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.14e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.248    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.7      |
| eval/normalized_episode_reward_std | 21.8      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0434   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 922000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0280
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.966    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0716   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 923000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0313
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.1e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 1.41     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0403   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 924000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0362
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.57e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.53      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.00479   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 925000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0246
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.24e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.211    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0369    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 926000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.377    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.1     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0121   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 927000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.0787   |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0301   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 928000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0319
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-12 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.486     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.67      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0296    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 929000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0175
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.593     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.00607   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 930000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.275    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0237   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 931000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0341
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.85e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.464     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0546   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 932000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0240
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.222    |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0435    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 933000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.353   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0249   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 934000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.161   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.000269 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 935000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 1.34     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.00477  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 936000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.78    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 4.13     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.051   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 937000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0177
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.0425   |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.111     |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 938000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | -0.25     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0222   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 939000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.609     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.1      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0677   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 940000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0127
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.497   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0289   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 941000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0272
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.353    |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0104    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 942000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0273
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.0719    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.9      |
| eval/normalized_episode_reward_std | 7.62      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0649    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 943000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.944    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.058   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 944000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.85e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.0291   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.36     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0502  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 945000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0294
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.864     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.7      |
| eval/normalized_episode_reward_std | 21.9      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0592    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 946000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0477
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.33e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | 0.722     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.00621   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 947000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0108
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.781    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 16.5     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0334   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 948000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-09 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.197     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0035   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 949000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.00988  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.00219  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 950000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.722   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0316  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 951000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0092
num rollout transitions: 250000, reward mean: 5.0128
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0133
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.393     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.5      |
| eval/normalized_episode_reward_std | 20.5      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0563    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 952000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | 0.242     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.016    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 953000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0182
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.175    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 5.14      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0385   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 954000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0416
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.99    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.63     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0457   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 955000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0247
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.229     |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0317   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 956000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0459
num rollout transitions: 250000, reward mean: 5.0404
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 1.35      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 16.4      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.00394  |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 957000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0152
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.584    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0622  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 958000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.522     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -714      |
| loss/alpha                         | 0.00188   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 959000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.629    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0233  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 960000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0390
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.2e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.639    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0214   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 961000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0348
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.208     |
| adv_dynamics_update/all_loss       | -55       |
| adv_dynamics_update/sl_loss        | -55       |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0572   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 962000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.29e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.539    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -714     |
| loss/alpha                         | 0.0264   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 963000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0333
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.845     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.1      |
| eval/normalized_episode_reward_std | 19.5      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0282    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 964000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -1       |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.6     |
| eval/normalized_episode_reward_std | 19.7     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0966  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 965000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0412
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.35e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.0739   |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.2      |
| eval/normalized_episode_reward_std | 23.3      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0194    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 966000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.275   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.1     |
| eval/normalized_episode_reward_std | 12.8     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0264  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 967000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0413
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.271    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0603   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 968000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0076
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.00647 |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 969000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0368
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.636   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0668   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 970000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 1.16      |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.89      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0211    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 971000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0236
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.12    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0113  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 972000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0459
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0380
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.24      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.5      |
| eval/normalized_episode_reward_std | 13.4      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00927  |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 973000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.76e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.59     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.016   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 974000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.71e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.634     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0458    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 975000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0150
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.0501  |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 20.9     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00588  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 976000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0341
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | 0.566     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 22.2      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0106   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 977000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0295
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.974    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 978000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.55    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0981  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 979000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0261
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.0294   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.7      |
| eval/normalized_episode_reward_std | 14.3      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0454    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 980000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0106
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.164   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.057    |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 981000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0111
num rollout transitions: 250000, reward mean: 5.0430
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-12 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | -0.874   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.00314  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 982000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0410
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.549   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0367  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 983000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0096
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0319
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.975     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0414    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 984000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.784    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.49      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00883  |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 985000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0059
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.369   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.00744 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 986000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0303
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.56     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0331    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 987000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0299
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.0241   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0309   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 988000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0426
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.0881    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 64.6      |
| eval/normalized_episode_reward_std | 14.5      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0328   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 989000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0456
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.67e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.031   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73       |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0173  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 990000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.1e-11  |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.564    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0545   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 991000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0093
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.114   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0744   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 992000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0406
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.19e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.0682  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0857  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 993000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0335
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.4e-10  |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.133    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0757   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 994000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.0797   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0272  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 995000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.223     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 996000    |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.109    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.7     |
| eval/normalized_episode_reward_std | 21.4     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0316  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 997000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0091
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.883   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67       |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00134  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 998000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0193
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.252   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0131  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 999000   |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.71e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.295     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0238   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1000000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.468    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.4      |
| eval/normalized_episode_reward_std | 24.3      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0173   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1001000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0439
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0211
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.35e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.244     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.087     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1002000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.634   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1003000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0275
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-09 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.35     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.024     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1004000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0048
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0238
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.434    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.4     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0688  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1005000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0299
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.132    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69       |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1006000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0269
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.382     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.029     |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1007000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.869    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00109   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1008000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.04e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.171    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0448   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1009000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0455
num rollout transitions: 250000, reward mean: 5.0275
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 10.5     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0749  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1010000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0331
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.0828   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.99     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0591   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1011000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.0428   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 6.09     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0701  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1012000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0390
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.264   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 8.15     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.021   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1013000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.0209  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.67     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0477   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1014000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.362    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0485  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1015000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0232
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.415   |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0666   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1016000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0266
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.63     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.5     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1017000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.101     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.00478  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1018000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0184
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-11 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.998     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 14        |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00514  |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1019000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0286
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0427  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1020000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 1.15     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0365  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1021000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.4e-10  |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.359   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.024    |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1022000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0145
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.625     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.2      |
| eval/normalized_episode_reward_std | 24.5      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0202   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1023000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.67     |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1024000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0302
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.266    |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -717      |
| loss/alpha                         | 0.051     |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1025000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0087
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.411    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0193   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1026000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.0604  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.107   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1027000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0116
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.5e-10  |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | -0.535   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.7     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00654  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1028000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0334
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.434    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1029000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0389
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.19e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | -0.611    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 9.3       |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0957    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1030000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0344
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.566    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 4.59     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0398   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1031000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0364
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-09 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.0153    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00656   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1032000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0141
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.896   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.159    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00232  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1033000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0338
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.0353   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.158    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.9     |
| eval/normalized_episode_reward_std | 9.4      |
| loss/actor                         | -717     |
| loss/alpha                         | -0.094   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1034000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0118
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0094
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.89e-11 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.897    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.026     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1035000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0301
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.00984  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.059   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1036000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0493
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.52     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 13.2      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0395   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1037000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0119
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.1e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 1.6      |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0215   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1038000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0413
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.115   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0155   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1039000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.36e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.223   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 64.3     |
| eval/normalized_episode_reward_std | 19.5     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0468   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1040000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.99     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 68.4      |
| eval/normalized_episode_reward_std | 19.3      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.109    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1041000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0224
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.94e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.695    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 12.3      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0841   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1042000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0446
num rollout transitions: 250000, reward mean: 5.0413
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.892    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0787   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1043000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0416
num rollout transitions: 250000, reward mean: 5.0267
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.74e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2      |
| adv_dynamics_update/adv_loss       | -0.4      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 7.45      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0235    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1044000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0379
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | 0.813     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 6.97      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0074    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1045000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0390
num rollout transitions: 250000, reward mean: 5.0443
num rollout transitions: 250000, reward mean: 5.0362
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.47e-11 |
| adv_dynamics_update/adv_log_prob   | 42.9      |
| adv_dynamics_update/adv_loss       | 0.958     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00492  |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1046000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0296
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.95e-11 |
| adv_dynamics_update/adv_log_prob   | 42.8     |
| adv_dynamics_update/adv_loss       | 0.672    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0711  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1047000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.295    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 8.51     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0653   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1048000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.428     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0648    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1049000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0432
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.347     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0796   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1050000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0373
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.79e-10 |
| adv_dynamics_update/adv_log_prob   | 42.8      |
| adv_dynamics_update/adv_loss       | 0.328     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00669   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1051000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0222
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.75e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.999     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 4.06      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00694  |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1052000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0406
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.763    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.28     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0076   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1053000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0354
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.0992   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.2     |
| eval/normalized_episode_reward_std | 10.7     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1054000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0412
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.76e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.851    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1055000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0362
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.45e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -0.713   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0451   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1056000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0361
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.238     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00767   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1057000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0256
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.9e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.418    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 20.5     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0483  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1058000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.457    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.2     |
| eval/normalized_episode_reward_std | 19.2     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0408  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1059000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.14e-11 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.945    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 68.9     |
| eval/normalized_episode_reward_std | 21.7     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0631   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1060000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0408
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0282
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.686     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 18.2      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0199   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1061000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.285    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.4      |
| eval/normalized_episode_reward_std | 14.8      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0804    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1062000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.593   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.2     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0193   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1063000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.71e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.013   |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0204   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1064000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.6e-10  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.0862  |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 8.26     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.068   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1065000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0327
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 0.277    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 66.6     |
| eval/normalized_episode_reward_std | 18.5     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0712   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1066000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.0384   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00255   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1067000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0389
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0479
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.0902   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1068000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0029
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0339
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.0278   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 12.5      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0064    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1069000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.53e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 1.69      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.5      |
| eval/normalized_episode_reward_std | 22.8      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0174    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1070000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0251
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.578     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.6      |
| eval/normalized_episode_reward_std | 20.1      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0762   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1071000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0402
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.303    |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0252    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1072000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.0719   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00705  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1073000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.0303   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0604   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1074000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0312
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.17e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.0339   |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0113   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1075000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.1e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.432    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0518  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1076000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0320
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.57e-11  |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.43     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.000713 |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1077000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.942     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0294    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1078000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.52    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00203 |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1079000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.418    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0296  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1080000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0055
num rollout transitions: 250000, reward mean: 5.0015
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.343     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 10.6      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.00631   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1081000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0438
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.75e-10 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 0.00699  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0218   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1082000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0331
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-11 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 1.22     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.108    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1083000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0348
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.388     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.015    |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1084000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0445
num rollout transitions: 250000, reward mean: 5.0290
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.25e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.33      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.8       |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0451   |
| loss/critic1                       | 14.3      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1085000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.35     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -716      |
| loss/alpha                         | -0.00149  |
| loss/critic1                       | 14.7      |
| loss/critic2                       | 14.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1086000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0472
num rollout transitions: 250000, reward mean: 5.0324
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.159     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 11.9      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0395    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1087000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.4e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0303   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00655  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1088000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0431
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0334
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.24     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 3.71     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.103   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1089000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | -0.713   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0233  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1090000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4     |
| adv_dynamics_update/adv_loss       | 0.592    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00742  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1091000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.155     |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.38      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.036     |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1092000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0321
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.29      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0194   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1093000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.46      |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0325   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1094000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6e-10   |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.406    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0536   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1095000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0275
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.8e-10  |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0355   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1096000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0285
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.511    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0118    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1097000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0339
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.874    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.106   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1098000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4e-10   |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.323    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 8        |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0199   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1099000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | 0.813     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.00446  |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1100000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0317
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43        |
| adv_dynamics_update/adv_loss       | -0.597    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 19        |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0451    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1101000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0300
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.1e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.59     |
| adv_dynamics_update/all_loss       | -55      |
| adv_dynamics_update/sl_loss        | -55      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.042   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1102000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0356
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.5e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.206    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.92     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0821   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1103000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0374
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | -1.13    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0833   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1104000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0465
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.427     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.161     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0502    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1105000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0318
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.35e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 1.65      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.15     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1106000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
num rollout transitions: 250000, reward mean: 5.0454
num rollout transitions: 250000, reward mean: 5.0430
num rollout transitions: 250000, reward mean: 5.0383
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.869     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.8      |
| eval/normalized_episode_reward_std | 10.1      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0414   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1107000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0110
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.63e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | 0.69     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0698   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1108000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0378
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.0926   |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0411    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1109000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0554
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.27e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.681    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0188   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1110000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.08e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 1.64     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72       |
| eval/normalized_episode_reward_std | 18.6     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0273   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1111000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0140
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.376    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.159     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0111   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1112000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.445   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0933  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1113000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0416
num rollout transitions: 250000, reward mean: 5.0307
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | -0.858   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.092    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1114000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0497
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0234
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.812     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.027    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1115000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0343
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.37e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1      |
| adv_dynamics_update/adv_loss       | -0.59     |
| adv_dynamics_update/all_loss       | -55.2     |
| adv_dynamics_update/sl_loss        | -55.2     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0329   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1116000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0396
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.97e-10 |
| adv_dynamics_update/adv_log_prob   | 43.1     |
| adv_dynamics_update/adv_loss       | 0.734    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0251  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1117000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0295
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.24e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | 1.43     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.7     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0563  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1118000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0098
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.774    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0161    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1119000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-11 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.808    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00694 |
| loss/critic1                       | 15.2     |
| loss/critic2                       | 15.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1120000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0368
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.949     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0115   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1121000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0168
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.43e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.304    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 20.8      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0642    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1122000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0367
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.51e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.982     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 7.16      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0795   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1123000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.361     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00194  |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1124000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0211
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.624   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0508   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1125000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0402
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0352
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.431    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0825   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1126000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0307
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.77e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.575     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0374   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1127000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0162
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0258
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.93e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 1.13      |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 9.48      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00163   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1128000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0430
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.94e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.407     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0447   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1129000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0256
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.206   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0715   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1130000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.5e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.0121   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0466  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1131000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0237
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 1.45      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0258   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1132000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0174
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.39e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.179     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.00627   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1133000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.00672  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 3.43     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0833  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1134000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | 0.391     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0323    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1135000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.397    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 10.8     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.084    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1136000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5      |
| adv_dynamics_update/adv_loss       | -0.229    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.023     |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1137000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0266
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.49     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74       |
| eval/normalized_episode_reward_std | 13.2     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0134  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1138000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0274
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.0331    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0168    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1139000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0123
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0326    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00839   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.01      |
| timestep                           | 1140000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0209
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.135    |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0298  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1141000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0312   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1142000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.707    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.017   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1143000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0273
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.22e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.274    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 8.63      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0539    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1144000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0365
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.274    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.7     |
| eval/normalized_episode_reward_std | 16.7     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.155   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1145000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 42.5     |
| adv_dynamics_update/adv_loss       | 0.67     |
| adv_dynamics_update/all_loss       | -55.1    |
| adv_dynamics_update/sl_loss        | -55.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0566   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1146000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0415
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.34e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.36     |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.6     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0265  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1147000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0346
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.14      |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.00985   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1148000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.551   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.0147  |
| loss/critic1                       | 15.1     |
| loss/critic2                       | 15.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1149000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0219
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.73e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.189    |
| adv_dynamics_update/all_loss       | -55.3    |
| adv_dynamics_update/sl_loss        | -55.3    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.00995  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1150000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0415
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.113     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0444    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1151000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.718    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.98      |
| loss/actor                         | -713      |
| loss/alpha                         | -0.0504   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1152000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0218
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-12 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.299    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.4      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -713      |
| loss/alpha                         | 0.0736    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1153000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0292   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.3     |
| eval/normalized_episode_reward_std | 6.16     |
| loss/actor                         | -713     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1154000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.95e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.488   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -713     |
| loss/alpha                         | -0.046   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1155000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0476
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0393
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3      |
| adv_dynamics_update/adv_loss       | 0.436     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0158    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1156000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0268
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.91e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0145    |
| loss/critic1                       | 15        |
| loss/critic2                       | 15        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1157000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0423
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-09 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 1.78      |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.31      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0119    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1158000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0320
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.643    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -714      |
| loss/alpha                         | -0.0248   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1159000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0410
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 43.2     |
| adv_dynamics_update/adv_loss       | -0.0648  |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.1     |
| eval/normalized_episode_reward_std | 6.5      |
| loss/actor                         | -714     |
| loss/alpha                         | -0.0559  |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1160000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.21e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -714      |
| loss/alpha                         | 0.0267    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1161000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0478
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.265     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.57      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0311   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1162000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0216
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.7e-10 |
| adv_dynamics_update/adv_log_prob   | 42.9     |
| adv_dynamics_update/adv_loss       | -0.0546  |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0211   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1163000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0248
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.672     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0415    |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1164000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0422
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.243    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.00822  |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1165000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.96e-11 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.406    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.42     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0083   |
| loss/critic1                       | 14.2     |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1166000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0414
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0365
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | 0.967    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.9     |
| eval/normalized_episode_reward_std | 22.8     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1167000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0241
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 1.9      |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0391  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1168000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0427
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0260
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.215    |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0844   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1169000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.7e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.0213  |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.04     |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1170000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0901  |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0157  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1171000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.419    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.8     |
| eval/normalized_episode_reward_std | 18.7     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00787 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1172000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.18e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.0072   |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.042    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1173000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.8e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.678    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 18.4     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0342  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1174000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0115
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.59e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.153    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.28      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0143   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1175000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0427
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0338
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-12 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.206    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.019    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1176000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0561    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 3.47      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0674   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1177000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0446
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0365
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.141    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0774    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1178000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0365
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.549     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.1      |
| eval/normalized_episode_reward_std | 11.2      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0229    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1179000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.302    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 2.65     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0473   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1180000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.3e-10 |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.0848   |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 22.6     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.033   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1181000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0222
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.212    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.3      |
| eval/normalized_episode_reward_std | 13        |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0368   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1182000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0291
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.367     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.8      |
| eval/normalized_episode_reward_std | 21.9      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0684    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1183000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.534    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 14.7     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0851   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1184000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.15e-11 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.0743   |
| adv_dynamics_update/all_loss       | -55.1     |
| adv_dynamics_update/sl_loss        | -55.1     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0466   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1185000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0452
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.1     |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0107   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1186000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.4e-10  |
| adv_dynamics_update/adv_log_prob   | 43.3     |
| adv_dynamics_update/adv_loss       | 0.612    |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0864   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1187000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0409
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.77e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.65      |
| adv_dynamics_update/all_loss       | -55.3     |
| adv_dynamics_update/sl_loss        | -55.3     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0489   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1188000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0490
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0416
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.346     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.024    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1189000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0315
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.455     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 8.57      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0439   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1190000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0251
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.874     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00142  |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1191000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0410
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-11 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | 0.318     |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0271    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1192000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0281
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.44e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.231    |
| adv_dynamics_update/all_loss       | -55.4     |
| adv_dynamics_update/sl_loss        | -55.4     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 7.75      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00393   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1193000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0284
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.43e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.878     |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 14.8      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0248   |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1194000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0422
num rollout transitions: 250000, reward mean: 5.0369
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.89e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 1.43      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00235  |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1195000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0037
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.256   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0184  |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1196000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.247     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0388   |
| loss/critic1                       | 14.5      |
| loss/critic2                       | 14.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1197000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0367
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-12 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.286    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0108   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1198000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.12      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0656    |
| loss/critic1                       | 14.8      |
| loss/critic2                       | 14.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1199000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0219
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 6.67      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0155   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1200000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0070
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0192
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.5e-10  |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.0318  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1201000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0342
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.944   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0227  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1202000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.229   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0792   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1203000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0428
num rollout transitions: 250000, reward mean: 5.0294
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.305    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1204000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0300
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.17e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.333     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.013     |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1205000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | -0.8     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.1     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0585  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1206000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 43.5     |
| adv_dynamics_update/adv_loss       | -0.216   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0332   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1207000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0270
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.187   |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00211 |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1208000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0164
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.69e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.559    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0224  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1209000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.35     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0079  |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1210000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0129
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 0.314     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.5      |
| eval/normalized_episode_reward_std | 10.4      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0224   |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1211000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0345
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.192     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0111    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1212000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0327
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.449     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.11     |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1213000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.322     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0648   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1214000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0136   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0391   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1215000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.29e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.66     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0424    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1216000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0342
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.64e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | -0.791    |
| adv_dynamics_update/all_loss       | -55.5     |
| adv_dynamics_update/sl_loss        | -55.5     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0187    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1217000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.674    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0249   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1218000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0410
num rollout transitions: 250000, reward mean: 5.0391
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.167    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0865    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1219000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0185
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.0178  |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0236  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1220000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0201
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.452     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0108    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1221000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.43e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.218   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0798  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1222000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.134    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0427  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1223000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0395
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.343    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0649   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1224000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.89e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.0943  |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1225000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0327
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.51     |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00439  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1226000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.12e-12 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.466    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00363  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1227000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0147
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.452    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.85     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00821 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1228000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.11e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.522     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.4      |
| eval/normalized_episode_reward_std | 8.69      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.04      |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1229000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0357
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | 0.478     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00924   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1230000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.335   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.45     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00564  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1231000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0137
num rollout transitions: 250000, reward mean: 5.0166
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.217    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0133    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1232000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0405
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.385    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0409   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 14       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1233000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0279
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.751    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00186  |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1234000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.7     |
| adv_dynamics_update/adv_loss       | 0.0712   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.8     |
| eval/normalized_episode_reward_std | 22.4     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0753  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1235000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0409
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0438
num rollout transitions: 250000, reward mean: 5.0347
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.313     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67        |
| eval/normalized_episode_reward_std | 22.4      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00466  |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1236000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0263
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.8e-11  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.749    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0101   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1237000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0369
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.22e-12 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.592     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.52      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0288    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1238000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0410
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0320
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.0664  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70       |
| eval/normalized_episode_reward_std | 20.8     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0087  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1239000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.826    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00179  |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1240000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0190
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -1.05     |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0611    |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1241000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0160
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.295     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.023     |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1242000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.441    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -719      |
| loss/alpha                         | -0.00791  |
| loss/critic1                       | 14        |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1243000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0449
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0440
num rollout transitions: 250000, reward mean: 5.0325
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 1.12      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0254    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1244000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0348
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.0131    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.158     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0243   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1245000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0393
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.07e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | 0.763    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0239  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1246000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0171
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.72e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.164   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0661  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1247000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0230
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 43.7      |
| adv_dynamics_update/adv_loss       | -0.259    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.2      |
| eval/normalized_episode_reward_std | 10.4      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0534    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1248000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-11 |
| adv_dynamics_update/adv_log_prob   | 43       |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55.2    |
| adv_dynamics_update/sl_loss        | -55.2    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 6.82     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0494  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1249000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.0423   |
| adv_dynamics_update/all_loss       | -55.5    |
| adv_dynamics_update/sl_loss        | -55.5    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0379  |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1250000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0304
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.0144    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 67.6      |
| eval/normalized_episode_reward_std | 22.3      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1251000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0504
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0409
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0903  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0153  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1252000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-12 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.0592    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.5       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.027    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1253000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0257
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.28e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.846    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.104    |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1254000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0409
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.76e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.255     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.36      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0606    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1255000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0214
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.97e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.427    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0191    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1256000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.0753  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.63     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0609  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1257000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.623    |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0167   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1258000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0222
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.276    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 70.3      |
| eval/normalized_episode_reward_std | 20.6      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0193   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1259000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0160
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.8e-10  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.762   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0255   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1260000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0123
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6     |
| adv_dynamics_update/adv_loss       | 0.253    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.3     |
| eval/normalized_episode_reward_std | 18.2     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0337   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1261000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.173    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.036   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1262000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.265     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0456   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1263000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0410
num rollout transitions: 250000, reward mean: 5.0306
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.266    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 4.3      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1264000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0336
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | 0.745     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0315   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1265000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0155
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.02e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.705     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.033    |
| loss/critic1                       | 14.4      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1266000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0297
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.27e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.575     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0567    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1267000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.993     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.7      |
| eval/normalized_episode_reward_std | 3.45      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0544    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1268000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.095    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.111   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1269000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0377
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0515  |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1270000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0320
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.353   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.7     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0168   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1271000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0250
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.95e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.306    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.000226 |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1272000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0109
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0390
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.29e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | 1.08      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.00176  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1273000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0305
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.28e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | 0.157    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.00352  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1274000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 1.53     |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.48     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.03     |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1275000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0237
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.303    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0247    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1276000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0180
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8      |
| adv_dynamics_update/adv_loss       | -0.326    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0202   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1277000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0147
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.59e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 1.14     |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 6.88     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.1      |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1278000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.706    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 62.7      |
| eval/normalized_episode_reward_std | 23.5      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0454    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1279000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.394    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.078   |
| loss/critic1                       | 14.6     |
| loss/critic2                       | 14.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1280000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.961    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0368   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1281000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.27     |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0162   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1282000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0326
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.71e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.827     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0671   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1283000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.31e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.04      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 14.3      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0378    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1284000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.249    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.5     |
| eval/normalized_episode_reward_std | 17.2     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0148  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1285000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0460
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-11 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.87      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0279   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1286000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0390
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-11 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.295   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.00384  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1287000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0398
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | 0.0895    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0604    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1288000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0341
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.15    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.157    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00219 |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1289000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0320
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-12 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.13    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0537  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1290000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0272
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.71e-11 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.58     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 16.9      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0262   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1291000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.87e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.771   |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0308  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1292000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0378
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.241    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.9     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0458   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1293000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0428
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0318
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | -0.34     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0748   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1294000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0093
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.464   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.8     |
| eval/normalized_episode_reward_std | 9.65     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00137  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1295000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0184
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.206    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00486 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1296000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0363
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 1.01      |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 18        |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0401   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1297000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0385
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.0144   |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.121     |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1298000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0275
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.595    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0225   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1299000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.74     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 66.1      |
| eval/normalized_episode_reward_std | 25.2      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0101   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1300000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0197
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.41e-11 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.547   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0277   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1301000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0266
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.52e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.215     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.3      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.005     |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1302000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0193
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1      |
| adv_dynamics_update/adv_loss       | -0.558    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.8      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0224    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1303000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0401
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.61e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9     |
| adv_dynamics_update/adv_loss       | -0.722   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.156    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0314   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1304000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0351
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.24e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -0.181    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.157     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0217   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1305000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0317
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.514     |
| adv_dynamics_update/all_loss       | -54.9     |
| adv_dynamics_update/sl_loss        | -54.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 6.22      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.101    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1306000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0208
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.325    |
| adv_dynamics_update/all_loss       | -55.4    |
| adv_dynamics_update/sl_loss        | -55.4    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0135   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1307000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0346
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.721     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 3.42      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0228    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1308000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0387
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.54e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.926    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0253    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1309000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0415
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10  |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.403    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0417  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1310000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0189
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.114   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0453   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1311000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0344
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.907    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 5.9      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1312000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0321
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 1.15      |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0432   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1313000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.134   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 65.2     |
| eval/normalized_episode_reward_std | 24.1     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.013    |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1314000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0457
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | 0.68     |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.3      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0312  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1315000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0327
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.486     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 65.7      |
| eval/normalized_episode_reward_std | 24.3      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0159   |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1316000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0163
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 43.9      |
| adv_dynamics_update/adv_loss       | -0.273    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 16.2      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0466    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1317000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0150
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.861     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0149    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1318000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0126
num rollout transitions: 250000, reward mean: 5.0302
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-12 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.263     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.154     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 4.71      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.032     |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1319000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0354
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.11    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.121   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1320000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0451
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0360
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.0572   |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 7.36      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.041    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1321000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.217    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0186  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1322000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.5e-10  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.402    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0405   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1323000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0447
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0179   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.9     |
| eval/normalized_episode_reward_std | 20.1     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0238   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1324000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0103
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.695    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.127    |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1325000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.066    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.33      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.104    |
| loss/critic1                       | 13        |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1326000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.68e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.378   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0607   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1327000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0319
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.6e-10  |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -0.32    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.4     |
| eval/normalized_episode_reward_std | 11.4     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1328000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 43.8     |
| adv_dynamics_update/adv_loss       | -0.494   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0466  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1329000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0107
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.69e-10 |
| adv_dynamics_update/adv_log_prob   | 43.4      |
| adv_dynamics_update/adv_loss       | -0.129    |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0307    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1330000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.178     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1331000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0414
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0166
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.655   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0214  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1332000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.199     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0339   |
| loss/critic1                       | 14.2      |
| loss/critic2                       | 14.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1333000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0280
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.947   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0136  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1334000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-11  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -1.08    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 11       |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0562   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1335000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0330
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.807    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0845  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1336000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0282
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.01e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.873    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0924   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1337000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0432
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0426
num rollout transitions: 250000, reward mean: 5.0485
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.5e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.534    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 14.8     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00199  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1338000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.249    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 19       |
| loss/actor                         | -718     |
| loss/alpha                         | -0.055   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1339000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0241
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | -1.14     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0479    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1340000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0205
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.603     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00615  |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1341000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0467
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.779     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0042   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1342000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0294
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.867    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.1      |
| eval/normalized_episode_reward_std | 2.65      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0176   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1343000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0240
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | -1.54    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.011   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1344000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0109  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.112    |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1345000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0425
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | -0.27     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -718      |
| loss/alpha                         | 0.054     |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1346000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0308
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.172   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.04     |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1347000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0425
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0468
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.29e-12 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.562     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.156     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0126    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1348000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0318
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0885   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.155     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0871   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1349000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0407
num rollout transitions: 250000, reward mean: 5.0367
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4      |
| adv_dynamics_update/adv_loss       | 0.377     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.135    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1350000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0514
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.99e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.0745   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0623   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1351000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.231   |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0123   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1352000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0436
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.0292   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0733  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1353000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.229    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0878   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1354000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0401
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | -0.686    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.7      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0219   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1355000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0299
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44       |
| adv_dynamics_update/adv_loss       | -0.00693 |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0447  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1356000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0207
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.35e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.138   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.054    |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1357000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0168
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0183
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.73    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0835   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1358000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0382
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.97e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -1.02    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0194   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1359000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0393
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0422
num rollout transitions: 250000, reward mean: 5.0369
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.256    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1360000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0394
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2     |
| adv_dynamics_update/adv_loss       | 0.134    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1361000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0337
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.763   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0829  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1362000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.0942   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.3       |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0221   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1363000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0151
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.561    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.4     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0197  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1364000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.06e-09 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0196  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0605   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1365000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.149    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 17.5      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0368   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1366000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -1.01    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0338  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1367000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0300
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0927    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.00289  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1368000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0406
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.96e-11 |
| adv_dynamics_update/adv_log_prob   | 43.6      |
| adv_dynamics_update/adv_loss       | 0.00466   |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -720      |
| loss/alpha                         | 0.00568   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1369000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | 0.904    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0136   |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1370000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0225
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.00328  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0605   |
| loss/critic1                       | 13.9     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1371000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.759     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.21      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0278    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1372000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0354
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.0804   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.046   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1373000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.626     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0517    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1374000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.538   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71       |
| eval/normalized_episode_reward_std | 20.2     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0435  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1375000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0103
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.292     |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.66      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0372   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1376000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0230
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.456    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.6     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1377000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | 0.192    |
| adv_dynamics_update/all_loss       | -55.8    |
| adv_dynamics_update/sl_loss        | -55.8    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.072   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1378000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.146   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 7.79     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1379000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0389
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0158
num rollout transitions: 250000, reward mean: 5.0332
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.157   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0381   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1380000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0307
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.129    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0169   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1381000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.48e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.694     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -716      |
| loss/alpha                         | 0.00999   |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1382000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0368
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.373   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0179   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1383000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.4     |
| eval/normalized_episode_reward_std | 23.6     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0419   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1384000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0320
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.564   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.1     |
| eval/normalized_episode_reward_std | 11.1     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0642   |
| loss/critic1                       | 14.4     |
| loss/critic2                       | 14.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1385000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.814   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0832   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1386000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0311
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.587    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0274  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1387000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.67e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.199    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.3     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -717     |
| loss/alpha                         | -0.116   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1388000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0377
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.53      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0505   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1389000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0297
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.978    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1390000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0440
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.78e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.144     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.3      |
| eval/normalized_episode_reward_std | 23.5      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0418    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1391000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0222
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 1.03     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0208   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1392000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.66e-11 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.283   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0173  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1393000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.134     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0081    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1394000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.31e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | -0.0627  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0432  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1395000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3e-10   |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.194   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0685   |
| loss/critic1                       | 13       |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1396000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0297
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.26e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.15     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.5      |
| eval/normalized_episode_reward_std | 9.64      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0224   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1397000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.525   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0411   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1398000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0125
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.584   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00785 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1399000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0304
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.32e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.00418   |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.56      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0744   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1400000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.907   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0565   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1401000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0281
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.258     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0563   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1402000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0154
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.569    |
| adv_dynamics_update/all_loss       | -55.8     |
| adv_dynamics_update/sl_loss        | -55.8     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0652    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1403000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.11e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.907    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.019     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1404000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0218
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.182    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.9     |
| eval/normalized_episode_reward_std | 5.13     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0463  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1405000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0393
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.00921 |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0505  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1406000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0284
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.52e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0108    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1407000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0348
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.189    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76        |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0169    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1408000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.94e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | -0.303   |
| adv_dynamics_update/all_loss       | -55.6    |
| adv_dynamics_update/sl_loss        | -55.6    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0804  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1409000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0179
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.632     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0571    |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1410000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0322
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.541     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 17.3      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0876   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1411000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0423
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.29     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0324  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1412000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.78e-11 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.187   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0945   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1413000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0279
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.12e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0619    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.13      |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1414000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -1.14    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.154    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0207  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1415000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.84e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.109    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0366  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1416000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0458
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0339    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1417000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0462
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0281
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.43e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 1.29      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.022    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1418000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0120
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0226
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.41e-11 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.498    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0946   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1419000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 4.9984
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0313
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.368   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0883   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1420000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0334
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.867     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0483   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1421000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0439
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5e-10   |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -1.92    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.028    |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1422000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0406
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0365
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.204    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.2      |
| eval/normalized_episode_reward_std | 15.8      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.074    |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1423000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0425
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0304
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.8e-11  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | -0.707   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0415   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1424000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.642     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69.5      |
| eval/normalized_episode_reward_std | 18.5      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0193   |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1425000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0384
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.17e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.348     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0358    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1426000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0181
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3     |
| adv_dynamics_update/adv_loss       | -0.0406  |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -720     |
| loss/alpha                         | 0.00183  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1427000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.94e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6     |
| adv_dynamics_update/adv_loss       | 0.492    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0597   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1428000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0348
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.84e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.384    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 23.2     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0592  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1429000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0342
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.24e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.727    |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0243  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1430000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0333
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.339     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00117   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1431000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0324
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.56e-11 |
| adv_dynamics_update/adv_log_prob   | 44.4     |
| adv_dynamics_update/adv_loss       | 0.812    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.1     |
| eval/normalized_episode_reward_std | 14.1     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.019   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1432000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.606   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.059    |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1433000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0275
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.39e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.109   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.000865 |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1434000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.36e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.656     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.3      |
| eval/normalized_episode_reward_std | 9.05      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.066    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1435000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0295
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.36e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.282   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00783 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1436000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0310
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.737    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0242    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1437000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0395
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.183    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0155  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1438000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0173
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.0199  |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0626  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1439000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0428
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.9e-10  |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | 0.676    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0258   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1440000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.913    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.057    |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1441000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.5      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0059    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1442000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0377
num rollout transitions: 250000, reward mean: 5.0439
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.265     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0321    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1443000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0288
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.577     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.59      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0864    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1444000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0247
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.618    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0294   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1445000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | 0.0484    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.6      |
| eval/normalized_episode_reward_std | 20.7      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.053    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1446000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 44.1     |
| adv_dynamics_update/adv_loss       | -0.0614  |
| adv_dynamics_update/all_loss       | -55.7    |
| adv_dynamics_update/sl_loss        | -55.7    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.6     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0508  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1447000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0258
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | 0.0247    |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.7      |
| eval/normalized_episode_reward_std | 10.3      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.00751   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1448000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0228
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.314    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0452  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1449000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.206     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0241   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1450000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0299
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.0949   |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0328    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1451000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0327
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 1.29      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.1      |
| eval/normalized_episode_reward_std | 19.9      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0957    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1452000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0334
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.326   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.4     |
| eval/normalized_episode_reward_std | 17       |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0325  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1453000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0381
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -1.02    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0128   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1454000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0472
num rollout transitions: 250000, reward mean: 5.0345
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.149    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.000282 |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1455000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0199
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.18e-10 |
| adv_dynamics_update/adv_log_prob   | 44.6      |
| adv_dynamics_update/adv_loss       | -0.0978   |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.000973 |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1456000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | 0.279     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0397   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1457000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0195
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -1.26    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.64     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00904  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1458000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0340
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.609    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0818   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1459000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.29e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.603     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.00596  |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1460000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0386
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.936    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0579  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1461000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0377
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0401
num rollout transitions: 250000, reward mean: 5.0261
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 1.31      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.32      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0579    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1462000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0386
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.26e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.0884  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.153    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.55     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0683   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1463000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0328
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.16e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.644    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.155    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.7     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0428  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1464000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0296
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.98e-11 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 1.44     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0691  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1465000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0391
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 1.04     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0354  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1466000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 44        |
| adv_dynamics_update/adv_loss       | 0.753     |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0433    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1467000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 1.32     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.05     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0158   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1468000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0297
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.333   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 67.2     |
| eval/normalized_episode_reward_std | 25.4     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0516  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1469000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0371
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.102   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0103   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1470000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.4e-11  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.301   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -721     |
| loss/alpha                         | 4.12e-05 |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1471000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.272   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.141   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1472000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0235
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -1.01    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0635   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1473000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0105
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.364    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0673   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1474000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.71e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0311   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 69.8     |
| eval/normalized_episode_reward_std | 17.5     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0182  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1475000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0161
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.806   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0709   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1476000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0364
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.878     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -721      |
| loss/alpha                         | 7.59e-05  |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1477000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0370
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.336   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76       |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0257  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1478000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0385
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.66     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0504  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1479000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0130
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0280
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.449   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -722     |
| loss/alpha                         | -0.00493 |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1480000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.9e-11  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.767    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 10.9     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.0511   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1481000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0261
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.15e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9      |
| adv_dynamics_update/adv_loss       | -0.261    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.23      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.000496 |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1482000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0129
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0337
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.043     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.4      |
| eval/normalized_episode_reward_std | 17.4      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.00539  |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1483000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0087
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-10  |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.158    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.68     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.00615 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1484000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0204
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.268   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -721     |
| loss/alpha                         | -0.00608 |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1485000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0139
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.503   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1486000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.757   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0412  |
| loss/critic1                       | 14.1     |
| loss/critic2                       | 14.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1487000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.715     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0357   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1488000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.206    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0736   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1489000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0343
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.15e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | -0.288    |
| adv_dynamics_update/all_loss       | -55.7     |
| adv_dynamics_update/sl_loss        | -55.7     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0152    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1490000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0338
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.35      |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0429    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1491000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0130
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.569    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0523   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1492000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.224   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.152    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -722     |
| loss/alpha                         | 0.0171   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1493000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0272
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.141     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -722      |
| loss/alpha                         | 0.0053    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1494000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0132
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0264
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.852    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.153     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.7      |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -722      |
| loss/alpha                         | -0.041    |
| loss/critic1                       | 14.1      |
| loss/critic2                       | 14.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1495000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0160
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.837    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0388  |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1496000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.74     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.4     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0197   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1497000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0272
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.167     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.152     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0408    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1498000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0333
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | -0.826    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -722      |
| loss/alpha                         | -0.0858   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1499000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0322
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 44.2      |
| adv_dynamics_update/adv_loss       | 0.35      |
| adv_dynamics_update/all_loss       | -55.6     |
| adv_dynamics_update/sl_loss        | -55.6     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -722      |
| loss/alpha                         | -0.0635   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1500000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0304
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.167     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.55      |
| loss/actor                         | -722      |
| loss/alpha                         | -0.0204   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1501000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.55e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.295    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0307   |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 14.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1502000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0353
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.64    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.4     |
| eval/normalized_episode_reward_std | 6.8      |
| loss/actor                         | -722     |
| loss/alpha                         | -0.0339  |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1503000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0340
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.939     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0323    |
| loss/critic1                       | 13.9      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1504000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0190
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.41e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.276    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0413  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1505000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0051
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.876    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.061   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1506000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0378
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.57e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.605     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -722      |
| loss/alpha                         | 0.0797    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1507000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0151
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.96e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.361    |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 15       |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0836   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1508000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0225
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -1.13     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.2      |
| eval/normalized_episode_reward_std | 9.66      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0106    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1509000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.673    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.2      |
| eval/normalized_episode_reward_std | 15        |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0446   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1510000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0261
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.106    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0654   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1511000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.798    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.112     |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1512000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0146
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.83e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0242   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1513000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0280
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.61e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.181    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0815  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1514000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.382    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0121  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1515000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0157
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.146     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -720      |
| loss/alpha                         | -0.00825  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1516000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0371
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 44.5     |
| adv_dynamics_update/adv_loss       | -0.829   |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.35     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.00953  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1517000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0423
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.138    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0635   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1518000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0458
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0409
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.647     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0166   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 14        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1519000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0408
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0342
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.8e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.432    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0823  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1520000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0411
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0252    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.00349   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1521000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 1.08     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0252   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1522000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0444
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.366   |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0126  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1523000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0127
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.769    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0062   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1524000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0336
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.685    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.5      |
| eval/normalized_episode_reward_std | 20.5      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0191   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1525000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0281
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.99e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.996    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.00708  |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1526000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0207
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0353
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.0945    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0177    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1527000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0402
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0198
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.68e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.0868    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.4      |
| eval/normalized_episode_reward_std | 23        |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0349    |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1528000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.551    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0358   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1529000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.06e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.312    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0259   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1530000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0393
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0419
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | 0.445     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0393   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1531000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0427
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.75     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.6     |
| eval/normalized_episode_reward_std | 3.65     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0413  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1532000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0078
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0324
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.822     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0327   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1533000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.198   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0181   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1534000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0310
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.24e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.517    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.00587  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1535000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0286
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.798    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0692   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1536000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0337
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.34e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.0109   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0188  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1537000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0175
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0275
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.126    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0413   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1538000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0399
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0215
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.312   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0171  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1539000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | -0.309   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.00487  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1540000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.193    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0487   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1541000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0227
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.456    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0271   |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1542000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0229
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.79e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.569    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0549    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1543000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0459
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0233
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.82e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.682    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.111    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1544000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0138
num rollout transitions: 250000, reward mean: 5.0132
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.585     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73        |
| eval/normalized_episode_reward_std | 16.9      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0625    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1545000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.08e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.0119  |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0356  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1546000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.494    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75        |
| eval/normalized_episode_reward_std | 11.5      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0156   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1547000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.759    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0524   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1548000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0342
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.78    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0624  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1549000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.35e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.648     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.7       |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0153    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1550000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0333
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | -0.335    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0117    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1551000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0429
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0216
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.247     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.00342   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1552000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0460
num rollout transitions: 250000, reward mean: 5.0261
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.803   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0208  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1553000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0241
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 44.9     |
| adv_dynamics_update/adv_loss       | 0.095    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.5     |
| eval/normalized_episode_reward_std | 2.72     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0173   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1554000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0153
num rollout transitions: 250000, reward mean: 5.0084
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0195
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7      |
| adv_dynamics_update/adv_loss       | -0.517    |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.8      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0121   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1555000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.87e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.0566    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 3.62      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.071     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1556000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0250
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.355   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0027  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1557000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.194    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 11.3     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0188  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1558000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0450
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0335
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.495    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0651  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1559000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0327
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.718   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0436   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1560000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.9e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | 0.265    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.29     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0436   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1561000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0430
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0238  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1562000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0451
num rollout transitions: 250000, reward mean: 5.0451
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.3e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.863   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0324  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1563000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0251
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.665    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0323   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1564000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0268
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.337   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0432   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1565000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.215    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.76     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.123   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1566000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0333
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0464  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1567000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0345
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.3e-10   |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.533     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72.2      |
| eval/normalized_episode_reward_std | 21.5      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.000535 |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1568000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | -0.403   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 11.5     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0636   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1569000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0172
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.201    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -720     |
| loss/alpha                         | -0.00949 |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1570000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0204
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.342     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.123     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1571000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0263
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.0807    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 16.9      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.00568   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1572000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0423
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.64e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -1.1      |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.15      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00268   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1573000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1      |
| adv_dynamics_update/adv_loss       | 0.781     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.151     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00135   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1574000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0176
num rollout transitions: 250000, reward mean: 5.0303
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.086     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0444   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1575000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0344
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 0.592     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0262    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1576000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0455
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.61e-10 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | -0.275   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0628  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1577000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.846    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0239   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1578000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0349
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2      |
| adv_dynamics_update/adv_loss       | -0.226    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00366  |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1579000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0246
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2e-10    |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.431   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0862   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1580000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.303   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0399  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1581000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0322
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.572   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.31     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0196   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1582000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.00014  |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0119    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1583000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0372
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.98e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0604  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.15     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.23     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0097  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1584000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0278
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.808     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0847   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1585000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.7e-11  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.384    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0246   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1586000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0284
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.18e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.137    |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0532    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1587000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0438
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0231
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.22e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.332     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.013    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1588000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0256
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.98e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.756     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.2       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0528   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1589000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0432
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -1.33    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.5     |
| eval/normalized_episode_reward_std | 3.37     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00816  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1590000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.347     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0614    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1591000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.978   |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 17.6     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0631  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1592000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0376
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.052     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00671   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1593000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0224
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.82e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.302    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.81      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0476   |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1594000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0271
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-11 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | -0.0708   |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0754    |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1595000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0463
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.51e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.494    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 72        |
| eval/normalized_episode_reward_std | 18.6      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.044    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1596000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0282
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.141    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0301   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1597000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0167
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.69e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -0.13    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.2     |
| eval/normalized_episode_reward_std | 13.3     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0146   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1598000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0355
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.36e-12 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.6       |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0448    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1599000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.24e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.466   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00445  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1600000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 1.28      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0803   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1601000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.442   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0392  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1602000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.93     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0395  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1603000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0298
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-13 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.107    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0503    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1604000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0279
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.2e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.556    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00577  |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1605000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0269
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.739    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.79      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0807    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1606000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.421     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.11     |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1607000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.03e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.259   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.58     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0216  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1608000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0258
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.133    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0495   |
| loss/critic1                       | 14       |
| loss/critic2                       | 14.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1609000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0205
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.719    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.119    |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1610000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.81e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.134    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.151    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.8     |
| eval/normalized_episode_reward_std | 7.05     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0159   |
| loss/critic1                       | 13.5     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1611000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0407
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.85e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.348     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.13      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0476   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1612000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.0855   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0644  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1613000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0134
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0339
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.03e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.882    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0124    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1614000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0390
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.12e-11 |
| adv_dynamics_update/adv_log_prob   | 44.7     |
| adv_dynamics_update/adv_loss       | 0.182    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00731 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1615000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0154
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.384   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0153   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1616000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0406
num rollout transitions: 250000, reward mean: 5.0373
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.15e-11 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.835    |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 4.13     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.043    |
| loss/critic1                       | 13.6     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1617000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0217
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.19e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.286    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00361  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1618000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0226
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.55e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.238     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00875   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1619000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0425
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0418
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0524   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1620000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0347
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.373    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.6      |
| eval/normalized_episode_reward_std | 13.7      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0336   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1621000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0331
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -1.33     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0258    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1622000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0119
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.74     |
| adv_dynamics_update/all_loss       | -56.1    |
| adv_dynamics_update/sl_loss        | -56.1    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0514   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1623000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0362
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.98e-11 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.313     |
| adv_dynamics_update/all_loss       | -55.9     |
| adv_dynamics_update/sl_loss        | -55.9     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0834   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1624000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0215
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.82e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8      |
| adv_dynamics_update/adv_loss       | 0.166     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 3.54      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0464    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1625000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.17e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.117    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 14.9     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0573  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1626000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0380
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.19e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.886     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 5.32      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0463    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1627000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0363
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.42e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.011     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.048    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1628000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0435
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.06e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.305   |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0323   |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1629000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0337
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.485    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.04    |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1630000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.4e-10  |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.155   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 5.25     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.015    |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1631000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0420
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0374
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.0922  |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.042    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1632000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0419
num rollout transitions: 250000, reward mean: 5.0410
num rollout transitions: 250000, reward mean: 5.0479
num rollout transitions: 250000, reward mean: 5.0427
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.1e-11  |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.944   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.2     |
| eval/normalized_episode_reward_std | 3.25     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0232  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1633000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0312
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.65e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | -0.00652  |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0521   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1634000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0462
num rollout transitions: 250000, reward mean: 5.0306
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0444
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.6      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00472  |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1635000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0379
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.601    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00282  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1636000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -1.57    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.2     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0211  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1637000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0073
num rollout transitions: 250000, reward mean: 5.0141
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.094     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0517    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1638000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.282    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00222 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1639000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.314    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0389  |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1640000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0123
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0324
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.22e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.164    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.09      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.052     |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1641000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.28e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.396    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0627   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1642000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0364
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.714     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0957    |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1643000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0200
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.53e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.232    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.000152 |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1644000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0252
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.462    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.81     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0675  |
| loss/critic1                       | 13.7     |
| loss/critic2                       | 13.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1645000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0295
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.63e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.57     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 73.8      |
| eval/normalized_episode_reward_std | 13.3      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.049     |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1646000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0126
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.79e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.288     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0425   |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1647000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.37e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.0133   |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0811    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1648000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0173
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0254
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.92e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.246    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.144   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1649000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0315
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.197    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0363  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1650000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0401
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0364
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.399    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0168  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1651000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0337
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.317    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.41      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0556    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1652000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.653     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.104     |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1653000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.829     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0298   |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1654000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0309
num rollout transitions: 250000, reward mean: 5.0290
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.57e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.352     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78        |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.000228 |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1655000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0460
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.66e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 1.3       |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.69      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00801  |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1656000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0262
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.19e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.884   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0189   |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1657000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0146
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.61e-12 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.408    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0377  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1658000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 1         |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -718      |
| loss/alpha                         | -0.075    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1659000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.014    |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1660000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0357
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.0771    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 2.57      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0241    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1661000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0287
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.788     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0242   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1662000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0269
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-12 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | 0.144    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.107    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1663000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0376
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.78e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.492     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.2      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.00757  |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1664000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0243
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.34e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.549    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0506   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1665000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.88e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.242    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0137   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1666000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0135
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.486    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0406   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1667000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0432
num rollout transitions: 250000, reward mean: 5.0449
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0363
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.62e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | 0.881     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0824    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1668000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0291
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.21e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.667     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0358    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1669000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0382
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.572    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0333  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1670000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0374
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.1       |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 2.97      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0368    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1671000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0141
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0244
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.781     |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0631   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1672000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0308
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.14e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.771   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0966   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1673000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0443
num rollout transitions: 250000, reward mean: 5.0469
num rollout transitions: 250000, reward mean: 5.0270
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.766     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0226   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1674000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0217
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.45e-11 |
| adv_dynamics_update/adv_log_prob   | 44.5      |
| adv_dynamics_update/adv_loss       | 0.214     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0367   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1675000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0324
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.37e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.16     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0389   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1676000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0221
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.22e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.728    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0226    |
| loss/critic1                       | 14        |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1677000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0124
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0194
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.382   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.52     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0299  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1678000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0361
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.0525   |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.3      |
| eval/normalized_episode_reward_std | 20.2      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.00745   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1679000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.12e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.202   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.2     |
| eval/normalized_episode_reward_std | 3.51     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0588  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1680000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0352
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.28e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.118     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74        |
| eval/normalized_episode_reward_std | 20.4      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0139   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1681000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0352
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.592    |
| adv_dynamics_update/all_loss       | -56      |
| adv_dynamics_update/sl_loss        | -56      |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 3.38     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0276   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1682000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0260
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 1.54     |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.17     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0629   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1683000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0212
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.5e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.501    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0216   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1684000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.73e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.865     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.9      |
| eval/normalized_episode_reward_std | 3.27      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.00059  |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1685000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0320
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.92e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.342    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.146    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1686000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0372
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.42      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.027     |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1687000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0411
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0188
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10   |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.626   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00856 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1688000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0202
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.312   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0804   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1689000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0202
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.86e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.14      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 12.9      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0318    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1690000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0177
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.472    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.0144   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1691000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0406
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0330
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.568     |
| adv_dynamics_update/all_loss       | -56       |
| adv_dynamics_update/sl_loss        | -56       |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0527    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1692000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0286
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.73e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.797    |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.149     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.000927 |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1693000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.301   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.149    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.9     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0378  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1694000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0086
num rollout transitions: 250000, reward mean: 5.0189
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0185
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.54e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.871     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.00881   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1695000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0350
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.73e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.268     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.148     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0326   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1696000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0295
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.86e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.38      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.00703   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1697000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0122
num rollout transitions: 250000, reward mean: 5.0188
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.144     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.3      |
| eval/normalized_episode_reward_std | 3.33      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0148    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1698000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0418
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.559   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -721     |
| loss/alpha                         | -0.0397  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1699000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0224
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.26e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.75     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.00677  |
| loss/critic1                       | 13.8      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1700000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0289
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.581    |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 11.7     |
| loss/actor                         | -721     |
| loss/alpha                         | 0.00492  |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1701000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0170
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.171    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.24      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0379   |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1702000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0365
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -1.05     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.26      |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0321   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1703000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.476     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -721      |
| loss/alpha                         | 0.0219    |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1704000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0181
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0293
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.451    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -721      |
| loss/alpha                         | -0.0381   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1705000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0265
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.75e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.206   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -721     |
| loss/alpha                         | 0.0663   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1706000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0249
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.89e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.409     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0493   |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1707000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0335
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.99e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.306     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.6      |
| eval/normalized_episode_reward_std | 13.4      |
| loss/actor                         | -720      |
| loss/alpha                         | 0.0381    |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1708000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0214
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0291
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.07e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.0777    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.0296   |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1709000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0297
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1e-10   |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.463   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 73.7     |
| eval/normalized_episode_reward_std | 21.5     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0307  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1710000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0079
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0236
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.77e-10 |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | -0.292   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -720     |
| loss/alpha                         | 0.041    |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1711000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.41e-11 |
| adv_dynamics_update/adv_log_prob   | 45.3     |
| adv_dynamics_update/adv_loss       | -1.38    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -720     |
| loss/alpha                         | -0.0183  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1712000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0360
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.85e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.399    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 4.24      |
| loss/actor                         | -720      |
| loss/alpha                         | -0.00915  |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1713000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.37      |
| adv_dynamics_update/all_loss       | -56.2     |
| adv_dynamics_update/sl_loss        | -56.2     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0683    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1714000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0212
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.26e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.251     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.53      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0415   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1715000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0462
num rollout transitions: 250000, reward mean: 5.0271
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.43      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0381   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1716000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0249
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.4     |
| eval/normalized_episode_reward_std | 23.4     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0424   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1717000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0342
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.17e-11 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.638    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.041    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1718000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0495
num rollout transitions: 250000, reward mean: 5.0309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.87e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.622     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0996    |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1719000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0284
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 2.33     |
| adv_dynamics_update/all_loss       | -55.9    |
| adv_dynamics_update/sl_loss        | -55.9    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0205   |
| loss/critic1                       | 13.3     |
| loss/critic2                       | 13.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1720000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0458
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.28    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.068   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1721000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0233
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.25e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.891    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0583  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1722000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0283
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.08e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.147    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0132   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1723000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0270
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.36e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.311   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.048   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1724000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0295
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.146     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0368    |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1725000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0218
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.54e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.0072   |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0717   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1726000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0352
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.21e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.0581   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0736   |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1727000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.72e-10 |
| adv_dynamics_update/adv_log_prob   | 45        |
| adv_dynamics_update/adv_loss       | 0.513     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.59      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0339    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1728000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0231
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.39e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.88      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0827    |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1729000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0152
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.283    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0582  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1730000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0420
num rollout transitions: 250000, reward mean: 5.0432
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.755    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0751  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1731000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0228
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0300
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.47e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00132  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1732000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0154
num rollout transitions: 250000, reward mean: 5.0351
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.133   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0343  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1733000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0272
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.92e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.027    |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1734000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0359
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.315   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0598  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1735000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0150
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0208
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.00798   |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.7      |
| eval/normalized_episode_reward_std | 21.7      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.101     |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1736000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0334
num rollout transitions: 250000, reward mean: 5.0388
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-10  |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.29     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.91     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0261   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1737000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0401
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 1.12     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0586  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1738000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0163
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.37    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77       |
| eval/normalized_episode_reward_std | 3.1      |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0137  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1739000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0386
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.42     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00121 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1740000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0293
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.89e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.531    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0133  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1741000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0213
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.886     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0347   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1742000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0269
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.69e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.357     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0229    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1743000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0312
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.89e-12 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.42     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0177   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1744000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0267
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.26e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.203    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.5     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00278  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1745000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0412
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0377
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.269    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 74.8      |
| eval/normalized_episode_reward_std | 2.74      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.000371 |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1746000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0400
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45       |
| adv_dynamics_update/adv_loss       | 0.0863   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0281  |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1747000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0364
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0176
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.299     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0607    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1748000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0303
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.6e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.538    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 70.3     |
| eval/normalized_episode_reward_std | 16.8     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00942  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1749000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0144
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0386
num rollout transitions: 250000, reward mean: 5.0257
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.604     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00712   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1750000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0140
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0381
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.51e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 1.22      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.7      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0249   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1751000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0335
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.44e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.476     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77        |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0423   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1752000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0336
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.47     |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.6      |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00967  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1753000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0321
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0329
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | -0.703   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0303   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1754000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0371
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.208    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0615   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1755000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0337
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.41e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.408    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00528   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1756000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.91e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.747   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.013   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1757000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0434
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.129    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 3.08      |
| loss/actor                         | -719      |
| loss/alpha                         | 3.13e-05  |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1758000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0245
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.372   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.15     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0212  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1759000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0305
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.04e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.917    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.64      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0143   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1760000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0430
num rollout transitions: 250000, reward mean: 5.0196
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.255   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0382  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1761000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0409
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.148    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.00767  |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1762000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0117
num rollout transitions: 250000, reward mean: 5.0227
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.2e-11  |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.16    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0691   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1763000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0081
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0168
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | 0.596    |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0554  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1764000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0373
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | 0.0945   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 2.89     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.051    |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1765000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0266
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.97e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.595   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00662 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1766000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0315
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -1.16     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0428   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1767000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.1e-10  |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.0407   |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0181  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1768000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0131
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.49e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.601     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.85      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0101    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1769000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0255
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.372    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.00635  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1770000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0124
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.85e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.972    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0209  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1771000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.29e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.613    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.78     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0502  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1772000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0450
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 1.25     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.025   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1773000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0206
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.61e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4      |
| adv_dynamics_update/adv_loss       | -0.177    |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -718      |
| loss/alpha                         | -8.41e-05 |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1774000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0389
num rollout transitions: 250000, reward mean: 5.0315
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.61      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0483    |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1775000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0343
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.91e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.353   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00248  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1776000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0297
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.63e-10 |
| adv_dynamics_update/adv_log_prob   | 44.8     |
| adv_dynamics_update/adv_loss       | -0.0643  |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0657   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1777000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0358
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.32e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.391     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.2      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0584   |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1778000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0319
num rollout transitions: 250000, reward mean: 5.0433
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0322
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.01e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 1.21      |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.99      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0768    |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1779000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0192
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0231
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.45e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 1.1      |
| adv_dynamics_update/all_loss       | -56.3    |
| adv_dynamics_update/sl_loss        | -56.3    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.74     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0318  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1780000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0301
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.349   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.6     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0539  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1781000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0425
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.2e-10  |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 1.06     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0531   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1782000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0333
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.73e-12 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.255   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.7      |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0104  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1783000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0192
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.41      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0422    |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1784000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0434
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0397
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.307    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.6      |
| eval/normalized_episode_reward_std | 12.9      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0127   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1785000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0446
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0398
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.12e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.635     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.4      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00714  |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1786000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0422
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.27e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.278    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 2.67     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0582   |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1787000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0344
num rollout transitions: 250000, reward mean: 5.0198
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.45e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.878    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0311  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1788000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0167
num rollout transitions: 250000, reward mean: 5.0166
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0183
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.538     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0542   |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1789000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0312
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0278
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.217   |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.8     |
| eval/normalized_episode_reward_std | 15.2     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0261   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1790000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0316
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.87e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.287    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0292  |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1791000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0169
num rollout transitions: 250000, reward mean: 5.0357
num rollout transitions: 250000, reward mean: 5.0321
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.82e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.361    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00662  |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1792000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0136
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.58e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.146    |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.063    |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1793000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0260
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -1.36     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0554   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1794000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0265
num rollout transitions: 250000, reward mean: 5.0201
num rollout transitions: 250000, reward mean: 5.0261
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.4e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.0448  |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0346  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1795000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0243
num rollout transitions: 250000, reward mean: 5.0257
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.86e-12 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.933    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 71.4      |
| eval/normalized_episode_reward_std | 19.6      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0329    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 13.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1796000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0370
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.804    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00193  |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1797000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0323
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0367
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 1.33     |
| adv_dynamics_update/all_loss       | -56.2    |
| adv_dynamics_update/sl_loss        | -56.2    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0299   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1798000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0276
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0376
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.95e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.509   |
| adv_dynamics_update/all_loss       | -56.4    |
| adv_dynamics_update/sl_loss        | -56.4    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 76.8     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00718 |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1799000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0180
num rollout transitions: 250000, reward mean: 5.0162
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.547   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.85     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0203  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1800000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.304    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.045    |
| loss/critic1                       | 13.4     |
| loss/critic2                       | 13.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1801000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0370
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.581    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.5     |
| eval/normalized_episode_reward_std | 13.9     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.012    |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1802000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0424
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0289
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.23e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.216     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0445   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1803000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0201
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.63e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.0935   |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0806    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1804000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0370
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0139
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.17e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.828    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0121   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1805000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.2e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.0334  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.73     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0563  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1806000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0411
num rollout transitions: 250000, reward mean: 5.0389
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.169    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0536   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1807000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0191
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.84e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.249    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0724    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1808000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0413
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0259
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.81e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.12     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.82      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.039     |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1809000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0349
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0302
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.84e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.00378   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.1      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0646    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1810000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0308
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.039   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.9     |
| eval/normalized_episode_reward_std | 11.6     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.043   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1811000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0403
num rollout transitions: 250000, reward mean: 5.0398
num rollout transitions: 250000, reward mean: 5.0362
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.46      |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.7      |
| eval/normalized_episode_reward_std | 2.56      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0455   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1812000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0333
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0378
num rollout transitions: 250000, reward mean: 5.0382
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.64e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.546     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 2.92      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0687   |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1813000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0447
num rollout transitions: 250000, reward mean: 5.0444
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.67e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.522    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.7      |
| eval/normalized_episode_reward_std | 3.53      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0727    |
| loss/critic1                       | 13.5      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1814000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0395
num rollout transitions: 250000, reward mean: 5.0405
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.66e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.707    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0897   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1815000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.94e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.25     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 3.01      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0026    |
| loss/critic1                       | 13.6      |
| loss/critic2                       | 13.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1816000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.01e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.345     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0304   |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1817000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0193
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0203
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.599     |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0235   |
| loss/critic1                       | 13.4      |
| loss/critic2                       | 13.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1818000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.34e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.239    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.000307 |
| loss/critic1                       | 13       |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1819000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.38e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.292    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 71.2     |
| eval/normalized_episode_reward_std | 22.5     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0309  |
| loss/critic1                       | 13.8     |
| loss/critic2                       | 13.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1820000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0361
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.35e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.308   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.21     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0442   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1821000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0383
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.78e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | -0.0041   |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.147     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0803    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1822000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0213
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.5e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.306    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.148    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.82     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0707  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1823000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0239
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.58e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.257     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0712   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1824000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0375
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0271
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.71e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.245     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00213   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1825000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0202
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0266
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.805     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0178   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 13        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1826000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0480
num rollout transitions: 250000, reward mean: 5.0367
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.22e-12 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -1.06     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0697    |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1827000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0305
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.48e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.223    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.00206 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1828000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0211
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.298     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0383   |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1829000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0076
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0186
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.9e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.326    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0279   |
| loss/critic1                       | 12.9     |
| loss/critic2                       | 13.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1830000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0329
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.07e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5     |
| adv_dynamics_update/adv_loss       | -0.426   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0954  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1831000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0249
num rollout transitions: 250000, reward mean: 5.0290
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.7e-10  |
| adv_dynamics_update/adv_log_prob   | 45.2     |
| adv_dynamics_update/adv_loss       | 0.661    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 4.26     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0619  |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1832000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0332
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0254
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | -0.56     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0319    |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1833000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0301
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 1.49     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0392   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1834000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0363
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.15e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 2.15     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.83     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0755   |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1835000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0116
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.05e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.709    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0408   |
| loss/critic1                       | 13       |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1836000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0298
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.443    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00617  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1837000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0459
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | -0.677    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.146     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.5      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0594   |
| loss/critic1                       | 12.7      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1838000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0140
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.86e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -1.67    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.2     |
| eval/normalized_episode_reward_std | 2.84     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.198   |
| loss/critic1                       | 13.2     |
| loss/critic2                       | 13.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1839000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0252
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0253
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.39e-10 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.402     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 2.62      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0917    |
| loss/critic1                       | 13.7      |
| loss/critic2                       | 14.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1840000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0394
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.46e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.539   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.05     |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1841000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0303
num rollout transitions: 250000, reward mean: 5.0234
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.3e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.875    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0626   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1842000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0380
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.1e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.211    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.146    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0774   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1843000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0371
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0216
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.02e-11 |
| adv_dynamics_update/adv_log_prob   | 45.1     |
| adv_dynamics_update/adv_loss       | 0.676    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.147    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0471  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1844000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0330
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.03e-11 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.463    |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 2.52      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0999   |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1845000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0284
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.05e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.743     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0389    |
| loss/critic1                       | 13        |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1846000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0171
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0285
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.23e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.614   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.18     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0474   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1847000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0275
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 1.01     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.7     |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1848000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0442
num rollout transitions: 250000, reward mean: 5.0196
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.73e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.55     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.145    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.3     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0159   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1849000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.47e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.164     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.8      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0281   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1850000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0402
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0579
num rollout transitions: 250000, reward mean: 5.0309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.213     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.5      |
| eval/normalized_episode_reward_std | 3.34      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0628   |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1851000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.331    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.69     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00956  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1852000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0236
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.284     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.06      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.018    |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1853000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0222
num rollout transitions: 250000, reward mean: 5.0400
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.02e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.736    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0483   |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1854000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0349
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.15e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.669    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0254   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1855000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0270
num rollout transitions: 250000, reward mean: 5.0262
num rollout transitions: 250000, reward mean: 5.0323
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.445     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00427   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1856000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0245
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.752     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.1      |
| eval/normalized_episode_reward_std | 3.02      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0165    |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1857000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0198
num rollout transitions: 250000, reward mean: 5.0267
num rollout transitions: 250000, reward mean: 5.0209
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.03e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.153     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0187    |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1858000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0389
num rollout transitions: 250000, reward mean: 5.0260
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.14e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 1.22     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78       |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0643  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1859000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0398
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.5e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.834    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.01     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0129   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1860000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0356
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.55e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.0544   |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.00787  |
| loss/critic1                       | 11.2      |
| loss/critic2                       | 11.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1861000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0232
num rollout transitions: 250000, reward mean: 5.0107
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.59e-12 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.279    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.49      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0307    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1862000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0399
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.03e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.993    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.9     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0269  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1863000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0187
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0268
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.37e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.419    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00701   |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1864000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0230
num rollout transitions: 250000, reward mean: 5.0227
num rollout transitions: 250000, reward mean: 5.0330
num rollout transitions: 250000, reward mean: 5.0299
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.33e-11 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 1.29      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0327    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1865000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0352
num rollout transitions: 250000, reward mean: 5.0281
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.98e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.929    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0576   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1866000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0206
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0240
num rollout transitions: 250000, reward mean: 5.0226
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-10 |
| adv_dynamics_update/adv_log_prob   | 45.4     |
| adv_dynamics_update/adv_loss       | 0.557    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 72.5     |
| eval/normalized_episode_reward_std | 22       |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0743  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1867000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0271
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0325
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.56e-11 |
| adv_dynamics_update/adv_log_prob   | 45.5      |
| adv_dynamics_update/adv_loss       | 0.284     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0497   |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1868000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0377
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.96e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.716   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0671  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1869000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0326
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.59e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.195     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0329    |
| loss/critic1                       | 12        |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1870000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0502
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0433
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 1.13      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.46      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0236    |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1871000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0442
num rollout transitions: 250000, reward mean: 5.0413
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0362
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.9e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.488    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.54     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.055    |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1872000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0483
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0352
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.31e-11 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | -0.301    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.95      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.019    |
| loss/critic1                       | 11.3      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1873000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0172
num rollout transitions: 250000, reward mean: 5.0159
num rollout transitions: 250000, reward mean: 5.0310
num rollout transitions: 250000, reward mean: 5.0210
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.2e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.0229   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0206  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1874000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0194
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0261
num rollout transitions: 250000, reward mean: 5.0293
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.36e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.266    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0175   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1875000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0208
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0117
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.09e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.318     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.73      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0329   |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1876000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0213
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0163
num rollout transitions: 250000, reward mean: 5.0292
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.85e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.333     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.000765  |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1877000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0381
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0402
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.281    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3.32     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0638  |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1878000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0303
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.689     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.2      |
| eval/normalized_episode_reward_std | 2.77      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0661    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1879000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0277
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0258
num rollout transitions: 250000, reward mean: 5.0314
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.45e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.044    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3         |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0256    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1880000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0101
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0248
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | 0.382    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0229  |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1881000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0342
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0242
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.253     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.3      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.00991   |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1882000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0175
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.653    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.15      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0347   |
| loss/critic1                       | 13.3      |
| loss/critic2                       | 13.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1883000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0277
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.3      |
| adv_dynamics_update/adv_loss       | 1.39      |
| adv_dynamics_update/all_loss       | -56.3     |
| adv_dynamics_update/sl_loss        | -56.3     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 3.04      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.118     |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1884000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0408
num rollout transitions: 250000, reward mean: 5.0307
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.74e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.474    |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0213    |
| loss/critic1                       | 13.2      |
| loss/critic2                       | 13.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1885000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0397
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.56e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.18      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.145     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.8      |
| eval/normalized_episode_reward_std | 3.19      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00433   |
| loss/critic1                       | 12.9      |
| loss/critic2                       | 13.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1886000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0411
num rollout transitions: 250000, reward mean: 5.0317
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.92e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.421    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.71     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0621  |
| loss/critic1                       | 12.6     |
| loss/critic2                       | 12.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1887000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.546     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.8      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0278   |
| loss/critic1                       | 12.8      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1888000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0182
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0248
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.52e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.479     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00504  |
| loss/critic1                       | 13.1      |
| loss/critic2                       | 13.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1889000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0372
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0343
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.19e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.746     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 2.91      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0217    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1890000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0336
num rollout transitions: 250000, reward mean: 5.0383
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0346
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.702    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.26     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.032   |
| loss/critic1                       | 12.7     |
| loss/critic2                       | 12.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1891000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0165
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0276
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.21e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.506   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.1     |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0452   |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1892000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0311
num rollout transitions: 250000, reward mean: 5.0448
num rollout transitions: 250000, reward mean: 5.0427
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.684   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0707  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1893000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.79e-12 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.902    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0383   |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1894000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0405
num rollout transitions: 250000, reward mean: 5.0454
num rollout transitions: 250000, reward mean: 5.0280
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.45e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.113     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0155    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1895000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0142
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0210
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2e-10    |
| adv_dynamics_update/adv_log_prob   | 45.7      |
| adv_dynamics_update/adv_loss       | 0.936     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.96      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.000354 |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1896000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0188
num rollout transitions: 250000, reward mean: 5.0250
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0367
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.03e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.159     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.14      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.04      |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1897000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0068
num rollout transitions: 250000, reward mean: 5.0309
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.54e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -1.43     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00381  |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1898000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0384
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.99e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 1.38     |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0285   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1899000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0284
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0225
num rollout transitions: 250000, reward mean: 5.0364
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.23e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.998    |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.04     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0663  |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1900000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0351
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -8.14e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.42      |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 4.45      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.00874   |
| loss/critic1                       | 12.2      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1901000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0338
num rollout transitions: 250000, reward mean: 5.0283
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0326
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.2e-11  |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.0602  |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.8     |
| eval/normalized_episode_reward_std | 2.99     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00905 |
| loss/critic1                       | 12.8     |
| loss/critic2                       | 13       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1902000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0276
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.83e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.00468  |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 3.37      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0606   |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1903000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0156
num rollout transitions: 250000, reward mean: 5.0185
num rollout transitions: 250000, reward mean: 5.0138
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.89e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 1.37     |
| adv_dynamics_update/all_loss       | -56.5    |
| adv_dynamics_update/sl_loss        | -56.5    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 4.33     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.000161 |
| loss/critic1                       | 12.5     |
| loss/critic2                       | 12.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1904000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0442
num rollout transitions: 250000, reward mean: 5.0536
num rollout transitions: 250000, reward mean: 5.0521
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.25e-10 |
| adv_dynamics_update/adv_log_prob   | 47        |
| adv_dynamics_update/adv_loss       | 1.92      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 3.17      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.0778   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.05      |
| timestep                           | 1905000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0210
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0319
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.16e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | 0.0991    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.139     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 69        |
| eval/normalized_episode_reward_std | 22.2      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0185    |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1906000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0231
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0343
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.66e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.524    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 2.86      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0747    |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1907000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0388
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.27e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.219    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80       |
| eval/normalized_episode_reward_std | 3.13     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0344  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1908000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0178
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0359
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.6      |
| adv_dynamics_update/adv_loss       | 0.364     |
| adv_dynamics_update/all_loss       | -56.4     |
| adv_dynamics_update/sl_loss        | -56.4     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 76.9      |
| eval/normalized_episode_reward_std | 11.2      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0409    |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1909000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0294
num rollout transitions: 250000, reward mean: 5.0314
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.19e-11 |
| adv_dynamics_update/adv_log_prob   | 45.6     |
| adv_dynamics_update/adv_loss       | -0.248   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0318  |
| loss/critic1                       | 10.8     |
| loss/critic2                       | 10.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1910000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0287
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.93e-10 |
| adv_dynamics_update/adv_log_prob   | 47       |
| adv_dynamics_update/adv_loss       | 1.06     |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00324 |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1911000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0211
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0324
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.8e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.618   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.93     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1912000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0343
num rollout transitions: 250000, reward mean: 5.0436
num rollout transitions: 250000, reward mean: 5.0450
num rollout transitions: 250000, reward mean: 5.0392
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.3e-10  |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.247   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 3.02     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.119    |
| loss/critic1                       | 11       |
| loss/critic2                       | 11       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1913000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0321
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.88e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | -0.621    |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.4       |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0821   |
| loss/critic1                       | 11.4      |
| loss/critic2                       | 11.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1914000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0418
num rollout transitions: 250000, reward mean: 5.0329
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0382
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | 0.163     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0995   |
| loss/critic1                       | 12        |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1915000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0368
num rollout transitions: 250000, reward mean: 5.0369
num rollout transitions: 250000, reward mean: 5.0347
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.8e-10  |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.491   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81.6     |
| eval/normalized_episode_reward_std | 2.94     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0688   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1916000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0186
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0415
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.43e-10 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.735    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.12     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.0656   |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1917000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0402
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.35e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.233   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0526  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1918000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0393
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0292
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.9e-10  |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 1.18     |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.8     |
| eval/normalized_episode_reward_std | 2.95     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0289   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1919000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0143
num rollout transitions: 250000, reward mean: 5.0133
num rollout transitions: 250000, reward mean: 5.0183
num rollout transitions: 250000, reward mean: 5.0191
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.31e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 1.75     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.9     |
| eval/normalized_episode_reward_std | 3.22     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0103  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1920000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0421
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0330
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.27e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.158     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.4      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0676   |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1921000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0089
num rollout transitions: 250000, reward mean: 5.0077
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.639    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.00647  |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12       |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1922000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0361
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0226
num rollout transitions: 250000, reward mean: 5.0325
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.78e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.439    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.139     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.4      |
| eval/normalized_episode_reward_std | 3.43      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0785   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1923000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0408
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0310
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.18e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.566     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.138     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.3      |
| eval/normalized_episode_reward_std | 2.98      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0482    |
| loss/critic1                       | 11.4      |
| loss/critic2                       | 11.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1924000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0326
num rollout transitions: 250000, reward mean: 5.0422
num rollout transitions: 250000, reward mean: 5.0333
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.21e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.289    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.139     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.5      |
| eval/normalized_episode_reward_std | 2.75      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0174    |
| loss/critic1                       | 11.2      |
| loss/critic2                       | 11.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1925000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0384
num rollout transitions: 250000, reward mean: 5.0223
num rollout transitions: 250000, reward mean: 5.0327
num rollout transitions: 250000, reward mean: 5.0413
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.93e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.265     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.139     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.7      |
| eval/normalized_episode_reward_std | 3.83      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.000244 |
| loss/critic1                       | 11.1      |
| loss/critic2                       | 11.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1926000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.246    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.155    |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1927000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0253
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0458
num rollout transitions: 250000, reward mean: 5.0295
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.28e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5     |
| adv_dynamics_update/adv_loss       | 0.235    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 2.91     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0423  |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1928000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0203
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0328
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 1.19      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.00674   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1929000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0246
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.19e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | -0.56     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.88      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0804   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1930000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0296
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0164
num rollout transitions: 250000, reward mean: 5.0264
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.65e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.485    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0668   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1931000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0404
num rollout transitions: 250000, reward mean: 5.0422
num rollout transitions: 250000, reward mean: 5.0417
num rollout transitions: 250000, reward mean: 5.0266
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.41e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.278   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75       |
| eval/normalized_episode_reward_std | 3.49     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0185  |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1932000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0149
num rollout transitions: 250000, reward mean: 5.0353
num rollout transitions: 250000, reward mean: 5.0281
num rollout transitions: 250000, reward mean: 5.0326
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.84e-10 |
| adv_dynamics_update/adv_log_prob   | 44.3      |
| adv_dynamics_update/adv_loss       | 0.984     |
| adv_dynamics_update/all_loss       | -56.1     |
| adv_dynamics_update/sl_loss        | -56.1     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.6      |
| eval/normalized_episode_reward_std | 3.29      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0347   |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1933000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0234
num rollout transitions: 250000, reward mean: 5.0374
num rollout transitions: 250000, reward mean: 5.0355
num rollout transitions: 250000, reward mean: 5.0326
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.16e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.627     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 2.58      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0257   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1934000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0289
num rollout transitions: 250000, reward mean: 5.0195
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0285
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.36e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.186     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.4      |
| eval/normalized_episode_reward_std | 2.83      |
| loss/actor                         | -715      |
| loss/alpha                         | 0.0164    |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1935000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0282
num rollout transitions: 250000, reward mean: 5.0274
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0277
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.38e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.562    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.6     |
| eval/normalized_episode_reward_std | 2.98     |
| loss/actor                         | -715     |
| loss/alpha                         | -0.0819  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1936000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0255
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0148
num rollout transitions: 250000, reward mean: 5.0306
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.03e-11 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.726     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.138     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.9      |
| eval/normalized_episode_reward_std | 2.94      |
| loss/actor                         | -715      |
| loss/alpha                         | -0.0333   |
| loss/critic1                       | 11        |
| loss/critic2                       | 11.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1937000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0373
num rollout transitions: 250000, reward mean: 5.0385
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0381
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.49e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.0272  |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.137    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -715     |
| loss/alpha                         | 0.0411   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1938000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0115
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0269
num rollout transitions: 250000, reward mean: 5.0392
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.13e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 0.344     |
| adv_dynamics_update/all_loss       | -56.6     |
| adv_dynamics_update/sl_loss        | -56.6     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.6      |
| eval/normalized_episode_reward_std | 3.03      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0825    |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1939000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0397
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0416
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.42e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.316     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 2.9       |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0456   |
| loss/critic1                       | 11.8      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1940000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0438
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 8.75e-11 |
| adv_dynamics_update/adv_log_prob   | 46.9     |
| adv_dynamics_update/adv_loss       | 0.245    |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 3.08     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.00915  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1941000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0480
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.453   |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 2.86     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.118    |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1942000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0155
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0212
num rollout transitions: 250000, reward mean: 5.0262
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.42e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8      |
| adv_dynamics_update/adv_loss       | 0.542     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.3      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0248   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1943000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0280
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0301
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.86e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.746   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 2.9      |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00594 |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1944000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0238
num rollout transitions: 250000, reward mean: 5.0320
num rollout transitions: 250000, reward mean: 5.0203
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.87e-10 |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.678   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.03     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.0305  |
| loss/critic1                       | 13.1     |
| loss/critic2                       | 13.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1945000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0380
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0291
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.86e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 0.451     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80        |
| eval/normalized_episode_reward_std | 3.35      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.000137  |
| loss/critic1                       | 12        |
| loss/critic2                       | 12.1      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1946000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0279
num rollout transitions: 250000, reward mean: 5.0357
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.08e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 1.07     |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -716     |
| loss/alpha                         | 0.102    |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1947000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0215
num rollout transitions: 250000, reward mean: 5.0233
num rollout transitions: 250000, reward mean: 5.0223
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.85e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.515   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.16     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0776  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1948000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0247
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0224
num rollout transitions: 250000, reward mean: 5.0201
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -5.76e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | -0.666    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.07      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0376    |
| loss/critic1                       | 12.6      |
| loss/critic2                       | 12.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1949000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0218
num rollout transitions: 250000, reward mean: 5.0200
num rollout transitions: 250000, reward mean: 5.0304
num rollout transitions: 250000, reward mean: 5.0288
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.09e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | -0.446   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.4      |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0227   |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1950000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0348
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0229
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.418    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.5     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0179   |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1951000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0339
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0346
num rollout transitions: 250000, reward mean: 5.0355
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.6e-10  |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | -0.0977  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.144    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.7     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 11       |
| loss/critic2                       | 11.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1952000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0440
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0248
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | -0.656    |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.144     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 3.05      |
| loss/actor                         | -717      |
| loss/alpha                         | 0.0126    |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1953000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0340
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0231
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.06e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.605     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.7      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -717      |
| loss/alpha                         | -0.061    |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1954000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0271
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.2e-10  |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | 0.594    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0125  |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1955000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0174
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0318
num rollout transitions: 250000, reward mean: 5.0350
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 6.96e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.595    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 2.97     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.046   |
| loss/critic1                       | 11.9     |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1956000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0440
num rollout transitions: 250000, reward mean: 5.0347
num rollout transitions: 250000, reward mean: 5.0240
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.54e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.736     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 2.76      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0528    |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1957000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0324
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0438
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.96e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | -0.0339  |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.8     |
| eval/normalized_episode_reward_std | 3.44     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0195   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1958000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0308
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0530
num rollout transitions: 250000, reward mean: 5.0441
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.22e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.107    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.4     |
| eval/normalized_episode_reward_std | 2.68     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.00326  |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1959000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0161
num rollout transitions: 250000, reward mean: 5.0245
num rollout transitions: 250000, reward mean: 5.0322
num rollout transitions: 250000, reward mean: 5.0327
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.25e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.383     |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.1      |
| eval/normalized_episode_reward_std | 3.11      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0383   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.4      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1960000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0467
num rollout transitions: 250000, reward mean: 5.0358
num rollout transitions: 250000, reward mean: 5.0428
num rollout transitions: 250000, reward mean: 5.0429
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.17e-10 |
| adv_dynamics_update/adv_log_prob   | 46.9      |
| adv_dynamics_update/adv_loss       | -0.375    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.5      |
| eval/normalized_episode_reward_std | 2.68      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0489    |
| loss/critic1                       | 12.5      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1961000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0328
num rollout transitions: 250000, reward mean: 5.0307
num rollout transitions: 250000, reward mean: 5.0291
num rollout transitions: 250000, reward mean: 5.0274
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.07e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.38    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.2     |
| eval/normalized_episode_reward_std | 3.2      |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0282   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1962000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0237
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0349
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.87e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1      |
| adv_dynamics_update/adv_loss       | 1.02      |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.9      |
| eval/normalized_episode_reward_std | 2.87      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0649   |
| loss/critic1                       | 12.3      |
| loss/critic2                       | 12.6      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1963000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0256
num rollout transitions: 250000, reward mean: 5.0350
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0407
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 9.22e-11 |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | -0.884   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.1     |
| eval/normalized_episode_reward_std | 2.88     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00746 |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1964000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0272
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0335
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.1e-11 |
| adv_dynamics_update/adv_log_prob   | 46.8     |
| adv_dynamics_update/adv_loss       | -0.408   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.4     |
| eval/normalized_episode_reward_std | 3.07     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0276   |
| loss/critic1                       | 12.4     |
| loss/critic2                       | 12.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1965000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0191
num rollout transitions: 250000, reward mean: 5.0299
num rollout transitions: 250000, reward mean: 5.0246
num rollout transitions: 250000, reward mean: 5.0390
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.24e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | 0.214     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.1      |
| eval/normalized_episode_reward_std | 3.1       |
| loss/actor                         | -719      |
| loss/alpha                         | -0.045    |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1966000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0331
num rollout transitions: 250000, reward mean: 5.0437
num rollout transitions: 250000, reward mean: 5.0247
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.17e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 1.35     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.5     |
| eval/normalized_episode_reward_std | 2.8      |
| loss/actor                         | -719     |
| loss/alpha                         | -0.022   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1967000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0420
num rollout transitions: 250000, reward mean: 5.0376
num rollout transitions: 250000, reward mean: 5.0445
num rollout transitions: 250000, reward mean: 5.0339
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.13e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | 0.46      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 81.1      |
| eval/normalized_episode_reward_std | 2.78      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0641    |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1968000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0356
num rollout transitions: 250000, reward mean: 5.0199
num rollout transitions: 250000, reward mean: 5.0356
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.48e-11 |
| adv_dynamics_update/adv_log_prob   | 46.3      |
| adv_dynamics_update/adv_loss       | -0.31     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.4      |
| eval/normalized_episode_reward_std | 2.84      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0332   |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1969000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0433
num rollout transitions: 250000, reward mean: 5.0292
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0323
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.9e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | 0.00356  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 81       |
| eval/normalized_episode_reward_std | 2.57     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00477 |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1970000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0220
num rollout transitions: 250000, reward mean: 5.0244
num rollout transitions: 250000, reward mean: 5.0453
num rollout transitions: 250000, reward mean: 5.0403
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.91e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.612     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.8      |
| eval/normalized_episode_reward_std | 3.47      |
| loss/actor                         | -719      |
| loss/alpha                         | 0.0068    |
| loss/critic1                       | 12.4      |
| loss/critic2                       | 12.5      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1971000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0239
num rollout transitions: 250000, reward mean: 5.0349
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.67e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.445   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0865   |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1972000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0362
num rollout transitions: 250000, reward mean: 5.0396
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0221
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.5e-10  |
| adv_dynamics_update/adv_log_prob   | 46.3     |
| adv_dynamics_update/adv_loss       | -0.316   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.142    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.6     |
| eval/normalized_episode_reward_std | 3.24     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.118   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1973000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0406
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0365
num rollout transitions: 250000, reward mean: 5.0387
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.5e-10  |
| adv_dynamics_update/adv_log_prob   | 45.7     |
| adv_dynamics_update/adv_loss       | 0.444    |
| adv_dynamics_update/all_loss       | -56.9    |
| adv_dynamics_update/sl_loss        | -56.9    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.3     |
| eval/normalized_episode_reward_std | 2.76     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0693   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.04     |
| timestep                           | 1974000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0285
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0305
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -3.68e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | -0.0733   |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.143     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 78.8      |
| eval/normalized_episode_reward_std | 2.93      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0779   |
| loss/critic1                       | 12.1      |
| loss/critic2                       | 12.3      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1975000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0286
num rollout transitions: 250000, reward mean: 5.0366
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0375
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.47e-10 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 0.00553  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.2     |
| eval/normalized_episode_reward_std | 3.11     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0166  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1976000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0400
num rollout transitions: 250000, reward mean: 5.0489
num rollout transitions: 250000, reward mean: 5.0335
num rollout transitions: 250000, reward mean: 5.0250
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.77e-10 |
| adv_dynamics_update/adv_log_prob   | 46.5      |
| adv_dynamics_update/adv_loss       | -0.623    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.14      |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 77.9      |
| eval/normalized_episode_reward_std | 3.22      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0128   |
| loss/critic1                       | 11.9      |
| loss/critic2                       | 12        |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.04      |
| timestep                           | 1977000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0197
num rollout transitions: 250000, reward mean: 5.0241
num rollout transitions: 250000, reward mean: 5.0313
num rollout transitions: 250000, reward mean: 5.0237
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.75e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4     |
| adv_dynamics_update/adv_loss       | 1.31     |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.8     |
| eval/normalized_episode_reward_std | 3.14     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.00592 |
| loss/critic1                       | 12.2     |
| loss/critic2                       | 12.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1978000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0260
num rollout transitions: 250000, reward mean: 5.0429
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0273
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.75e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.904    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 2.87     |
| loss/actor                         | -719     |
| loss/alpha                         | -0.0279  |
| loss/critic1                       | 11.7     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1979000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0287
num rollout transitions: 250000, reward mean: 5.0345
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0363
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.3e-11  |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.184   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 3.19     |
| loss/actor                         | -719     |
| loss/alpha                         | 0.0796   |
| loss/critic1                       | 12       |
| loss/critic2                       | 12.1     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1980000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0229
num rollout transitions: 250000, reward mean: 5.0145
num rollout transitions: 250000, reward mean: 5.0325
num rollout transitions: 250000, reward mean: 5.0345
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.63e-10 |
| adv_dynamics_update/adv_log_prob   | 46.2     |
| adv_dynamics_update/adv_loss       | -0.607   |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.1     |
| eval/normalized_episode_reward_std | 3.27     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0433   |
| loss/critic1                       | 12.3     |
| loss/critic2                       | 12.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1981000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0302
num rollout transitions: 250000, reward mean: 5.0341
num rollout transitions: 250000, reward mean: 5.0293
num rollout transitions: 250000, reward mean: 5.0284
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.57e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7      |
| adv_dynamics_update/adv_loss       | 0.377     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.2      |
| eval/normalized_episode_reward_std | 3.12      |
| loss/actor                         | -719      |
| loss/alpha                         | -0.0727   |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1982000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0139
num rollout transitions: 250000, reward mean: 5.0298
num rollout transitions: 250000, reward mean: 5.0278
num rollout transitions: 250000, reward mean: 5.0311
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -4.41e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 1.25      |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.6      |
| eval/normalized_episode_reward_std | 3.39      |
| loss/actor                         | -718      |
| loss/alpha                         | 0.0531    |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.8      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1983000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0290
num rollout transitions: 250000, reward mean: 5.0360
num rollout transitions: 250000, reward mean: 5.0359
num rollout transitions: 250000, reward mean: 5.0302
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.19e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.19     |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.3     |
| eval/normalized_episode_reward_std | 2.77     |
| loss/actor                         | -718     |
| loss/alpha                         | 0.0847   |
| loss/critic1                       | 12.1     |
| loss/critic2                       | 12.2     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1984000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0392
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0264
num rollout transitions: 250000, reward mean: 5.0259
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 5.14e-10 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | -0.546   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.143    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.7     |
| eval/normalized_episode_reward_std | 3.33     |
| loss/actor                         | -718     |
| loss/alpha                         | -0.0481  |
| loss/critic1                       | 11.2     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1985000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0354
num rollout transitions: 250000, reward mean: 5.0251
num rollout transitions: 250000, reward mean: 5.0236
num rollout transitions: 250000, reward mean: 5.0404
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.33e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6      |
| adv_dynamics_update/adv_loss       | 0.495     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.142     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 80.6      |
| eval/normalized_episode_reward_std | 3.16      |
| loss/actor                         | -718      |
| loss/alpha                         | -0.0325   |
| loss/critic1                       | 11.7      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1986000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0295
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0315
num rollout transitions: 250000, reward mean: 5.0398
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.01e-11 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | 0.559    |
| adv_dynamics_update/all_loss       | -56.6    |
| adv_dynamics_update/sl_loss        | -56.6    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.1     |
| eval/normalized_episode_reward_std | 3.39     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0112  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1987000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0184
num rollout transitions: 250000, reward mean: 5.0217
num rollout transitions: 250000, reward mean: 5.0204
num rollout transitions: 250000, reward mean: 5.0389
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.29e-10 |
| adv_dynamics_update/adv_log_prob   | 45.8     |
| adv_dynamics_update/adv_loss       | 0.691    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.141    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 77.9     |
| eval/normalized_episode_reward_std | 3.06     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0143  |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1988000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0209
num rollout transitions: 250000, reward mean: 5.0337
num rollout transitions: 250000, reward mean: 5.0255
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -6.21e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9      |
| adv_dynamics_update/adv_loss       | 1.4       |
| adv_dynamics_update/all_loss       | -56.7     |
| adv_dynamics_update/sl_loss        | -56.7     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.25      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00652  |
| loss/critic1                       | 11.6      |
| loss/critic2                       | 11.9      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.02      |
| timestep                           | 1989000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0263
num rollout transitions: 250000, reward mean: 5.0235
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0239
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.82e-11 |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.721    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.6     |
| eval/normalized_episode_reward_std | 2.79     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.115   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1990000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0387
num rollout transitions: 250000, reward mean: 5.0273
num rollout transitions: 250000, reward mean: 5.0316
num rollout transitions: 250000, reward mean: 5.0301
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -9.15e-11 |
| adv_dynamics_update/adv_log_prob   | 46.4      |
| adv_dynamics_update/adv_loss       | -0.685    |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.137     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79        |
| eval/normalized_episode_reward_std | 3.18      |
| loss/actor                         | -717      |
| loss/alpha                         | -0.00894  |
| loss/critic1                       | 11.5      |
| loss/critic2                       | 11.7      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1991000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0254
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0236
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 1.31e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -0.681   |
| adv_dynamics_update/all_loss       | -56.7    |
| adv_dynamics_update/sl_loss        | -56.7    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 75.3     |
| eval/normalized_episode_reward_std | 13.6     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.072    |
| loss/critic1                       | 11.1     |
| loss/critic2                       | 11.3     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1992000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0268
num rollout transitions: 250000, reward mean: 5.0435
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0321
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4e-10    |
| adv_dynamics_update/adv_log_prob   | 46       |
| adv_dynamics_update/adv_loss       | 0.769    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79       |
| eval/normalized_episode_reward_std | 2.92     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.00542  |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.6     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1993000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0242
num rollout transitions: 250000, reward mean: 5.0314
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0330
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -7.64e-11 |
| adv_dynamics_update/adv_log_prob   | 46.2      |
| adv_dynamics_update/adv_loss       | 0.398     |
| adv_dynamics_update/all_loss       | -56.5     |
| adv_dynamics_update/sl_loss        | -56.5     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 79.5      |
| eval/normalized_episode_reward_std | 2.89      |
| loss/actor                         | -716      |
| loss/alpha                         | 0.0599    |
| loss/critic1                       | 11.1      |
| loss/critic2                       | 11.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1994000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0317
num rollout transitions: 250000, reward mean: 5.0300
num rollout transitions: 250000, reward mean: 5.0367
num rollout transitions: 250000, reward mean: 5.0296
-----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -2.48e-10 |
| adv_dynamics_update/adv_log_prob   | 46        |
| adv_dynamics_update/adv_loss       | 0.182     |
| adv_dynamics_update/all_loss       | -56.8     |
| adv_dynamics_update/sl_loss        | -56.8     |
| alpha                              | 0.141     |
| eval/episode_length                | 1e+03     |
| eval/episode_length_std            | 0         |
| eval/normalized_episode_reward     | 75.9      |
| eval/normalized_episode_reward_std | 13.1      |
| loss/actor                         | -716      |
| loss/alpha                         | -0.0614   |
| loss/critic1                       | 11.1      |
| loss/critic2                       | 11.2      |
| rollout_info/num_transitions       | 2.5e+05   |
| rollout_info/reward_mean           | 5.03      |
| timestep                           | 1995000   |
-----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0351
num rollout transitions: 250000, reward mean: 5.0288
num rollout transitions: 250000, reward mean: 5.0297
num rollout transitions: 250000, reward mean: 5.0423
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 2.76e-10 |
| adv_dynamics_update/adv_log_prob   | 45.9     |
| adv_dynamics_update/adv_loss       | -1.39    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.3     |
| eval/normalized_episode_reward_std | 3.58     |
| loss/actor                         | -716     |
| loss/alpha                         | -0.00812 |
| loss/critic1                       | 11.4     |
| loss/critic2                       | 11.5     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1996000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0266
num rollout transitions: 250000, reward mean: 5.0379
num rollout transitions: 250000, reward mean: 5.0415
num rollout transitions: 250000, reward mean: 5.0201
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | -1.8e-10 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | 0.703    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 74.9     |
| eval/normalized_episode_reward_std | 17.8     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.00856 |
| loss/critic1                       | 11.3     |
| loss/critic2                       | 11.4     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 1997000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0259
num rollout transitions: 250000, reward mean: 5.0257
num rollout transitions: 250000, reward mean: 5.0221
num rollout transitions: 250000, reward mean: 5.0243
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 7.01e-11 |
| adv_dynamics_update/adv_log_prob   | 46.6     |
| adv_dynamics_update/adv_loss       | -0.0273  |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.139    |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 79.7     |
| eval/normalized_episode_reward_std | 3        |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0109   |
| loss/critic1                       | 11.5     |
| loss/critic2                       | 11.7     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1998000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0219
num rollout transitions: 250000, reward mean: 5.0177
num rollout transitions: 250000, reward mean: 5.0190
num rollout transitions: 250000, reward mean: 5.0334
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 4.79e-10 |
| adv_dynamics_update/adv_log_prob   | 46.7     |
| adv_dynamics_update/adv_loss       | 0.471    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 78.4     |
| eval/normalized_episode_reward_std | 3.09     |
| loss/actor                         | -717     |
| loss/alpha                         | 0.0063   |
| loss/critic1                       | 11.6     |
| loss/critic2                       | 11.8     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.02     |
| timestep                           | 1999000  |
----------------------------------------------------------------------------------
num rollout transitions: 250000, reward mean: 5.0179
num rollout transitions: 250000, reward mean: 5.0391
num rollout transitions: 250000, reward mean: 5.0248
num rollout transitions: 250000, reward mean: 5.0383
----------------------------------------------------------------------------------
| adv_dynamics_update/adv_advantage  | 3.11e-10 |
| adv_dynamics_update/adv_log_prob   | 46.1     |
| adv_dynamics_update/adv_loss       | 0.579    |
| adv_dynamics_update/all_loss       | -56.8    |
| adv_dynamics_update/sl_loss        | -56.8    |
| alpha                              | 0.14     |
| eval/episode_length                | 1e+03    |
| eval/episode_length_std            | 0        |
| eval/normalized_episode_reward     | 80.5     |
| eval/normalized_episode_reward_std | 2.96     |
| loss/actor                         | -717     |
| loss/alpha                         | -0.0145  |
| loss/critic1                       | 11.8     |
| loss/critic2                       | 11.9     |
| rollout_info/num_transitions       | 2.5e+05  |
| rollout_info/reward_mean           | 5.03     |
| timestep                           | 2000000  |
----------------------------------------------------------------------------------
total time: 128024.75s
